[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Econometrics (TSE)",
    "section": "",
    "text": "Preface\nForeword\n\nThis material is contained from various sources. The material is largely based on the materials of the many courses on time series analysis held by Professor emeritus Pentti Saikkonen at the University of Helsinki. Many thanks to him for giving access to the previous course materials, especially on the course Stationaariset aikasarjat (2015). Thanks also to Professor Mika Meitz and his lecture notes “Lecture Notes on Time Series Econometrics” concerning the English translation.\n\nAikasarja-analyysi / Time Series Analysis at the Univ. of Turku 2016-2025.\n\nOther sources and potential references include the following:\n\nBrockwell, P.J. & R.A. Davis (1996 or 2002). Introduction to Time Series and Forecasting. Springer (2002, 2nd ed.).\nFranses, P.H. & D. van Dijk (2000). Non-linear time series models in empirical finance. Cambridge University Press.\nHamilton, J. (1994). Time Series Analysis. Princeton University Press.\nL\"{u}tkepohl, H. & M. Kr\"{a}tzig (2004). Applied Time Series Econometrics. Cambridge University Press.\nTsay, R.S. (2010). Analysis of Financial Time Series. Wiley\nVerbeek, M. (2008). A Guide To Modern Econometrics. John Wiley & Sons. Third Edition\n\nThese materials focus largely on univariate time series models, but we will also briefly consider the main points of multiple time series, specifically vector autoregressive (VAR) models and cointegration. Moreover, we will also consider some basics of machine learning techniques in time series econometrics. The empirical examples (carried out by RStudio) provide illustration of the theoretical basis. The empirical examples are mostly from economic and financial applications, but the methods are applicable in many other fields as well.\nSome empirical examples are based on well-known journal articles:\n\nHall, R.E. (1978). Stochastic implications of the life cycle-permanent income hypothesis: Theory and evidence. Journal of Political Economy, 86(6), 971–987.\nStock, J.H & M.W. Watson (2001). Vector Autoregressions. Journal of Economic Perspectives, 15(4), 101–115.\n\nDatasets and data sources used in this lecture manuscript:\n\nShiller data: Robert Shiller’s data on Irrational Exuberance\nFRED data\nquantmod R package\n\nThanks also to BSc Juho Pitkäranta for helping with translation and setting up this version. All remaining errors are by the author.\n\n \nNotation\n\nOn this course, we follow the following notational selections:\n\nWe will use the same notation for random quantities and their realized values. The context should make the difference clear.\nIn matrix calculations, vectors are interpreted as column vectors and we might also denote, for example, \\(y=\\left[y_{1}\\text{ }\\cdots\\text{ }y_{T}\\right]^{\\prime}\\), where the superscripted comma denotes transpose of a vector (or elsewhere also of a matrix).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Time Series Econometrics (TSE)</span>"
    ]
  },
  {
    "objectID": "TSE-ch1.html",
    "href": "TSE-ch1.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Time series data\nA time series is a collection of data in which the observations correspond to consecutive time periods. A common notation for time series data is \\(y_{t}\\), \\(t=1,\\ldots,T\\), where \\(t\\) denotes the time period and \\(T\\) the total number of observations. The interval between observations \\(y_{t-1}\\) and \\(y_{t}\\), or \\(y_{t}\\) and \\(y_{t+1}\\), is a some unit of time, for example, an hour, a month, a quarter or one year. In many cases, it may be natural to use more concrete labels for the time periods, for instance \\(t=2000,\\ldots,2024\\) for yearly data, or \\(t=2000:1, 2000:2,\\ldots,2024:12\\) for monthly observations. Therefore, occasionally \\(T\\) also denotes the concrete final time label such as 2024:12 (December 2024).\nIn this course, we assume that the observations recorded are equally spaced in time (or can be approximately treated as such). Irregularly spaced data call for different methods and are not treated here. If the time series observations \\(y_{t}, \\, t=1,\\ldots,T,\\) are scalar valued, we talk about univariate time series. On the other hand, if \\(\\boldsymbol{y}_{t}=(y_{1t},\\ldots,y_{nt})\\) is vector-valued, which is emphasized by the bolded symbol, we talk about multiple time series or vector-valued time series. In these latter cases, relationships between two or more time series is of interest.\nWe restrict ourselves to the common case where \\(y_t\\) is a real-valued time series, or is at least treated as such, meaning that realizations are real numbers. In discrete and limited dependent time series, \\(y_t\\) takes only discrete values, such as binary values (\\(y_t=1\\) or \\(y_t=0\\)) or values where the range is limited (such as the case where \\(y_t\\) is nonnegative). These type of time series variables require specific (nonlinear) models, which we will not consider in this course (except models used for volatility modelling and hence positive-valued variables).\nWhen analyzing time series data, often a useful and natural first step is to plot the data and inspect its main characteristics visually. Below we plot the monthly Consumer Price Index (CPI) in the United States for the time period \\(1990:1\\)–\\(2025:6\\). In this figure, and as almost always when plotting time series data, to point out the nature of time series data (dependency between observations) the depicted line goes through observation points.\nFigure: (Annualized) U.S. quarterly real GDP growth rate (1985:Q1–2007:Q2) (ggplot2 figure).\nThe aim in time series analysis is often to build a statistical model that aims to represent the observed time series and its fluctuation over time. This typically happens by considering the dependence structure of consecutive observations. Autocorrelation (serial correlation) between observations is an essential property of the time series, which needs to be taken into account in statistical/econometric modelling.\nWhen analyzing time series data often a useful and natural first step is to plot the series and inspect its main characteristics visually in addition to possible underlying theory or background knowledge of the phenomenon that the time series presents. This will bring (at least) preliminary idea on dependence structures behind the data generating mechanism of the time series. Some important questions to consider when looking at a time series graph:\nAs an example, in the U.S. CPI time series above, a clear feature is an increasing trend over time. It turns out that instead of the levels, we are mainly interested in the changes of the CPI series as a measure of inflation, as will be discussed below.\nAn increasing trend can also be observed in the (monthly) S&P 500 stock market index. Stock prices rallied substantially between the years 1990 and 2025 (that is, stock prices experienced a substantial surge), while there are also a few substantial market corrections (“bear markets”) throughout the years. What is also important to notice at this stage is that this trending behaviour is clearly different than the one behind the CPI series.\nFigure: S&P 500 stock market index between January 1990 and June 2025 (source: Shiller data).\nNext figure depicts the monthly number of fatalities in traffic accidents in the U.S. between \\(1973:1\\)–\\(1978:12\\). In this series, there is no apparent increasing or decreasing trend. Instead, the series exhibits clear seasonal variation, with the yearly maximum always in July and the minimum in February.\nFigure: Monthly traffic accidents between January 1973 and December 1978. (source: Brockwell and Davis, 1996).\nNext figure shows the yearly number of sunspots for the period 1770–1870. This series has no trend or seasonal variation, although the cyclical pattern of the series could be confused with seasonal variation. However, the pattern is not seasonal because the nature of the cycle changes. In particular, the distance between consecutive peaks is not constant, as would be the case with seasonal variation. An apparent feature is a positive correlation between consecutive observations; a large observation is typically followed by another large one, and a small observation by another small one.\nFigure: The number of sunspots between the years 1770–1870 (source: Brockwell and Davis, 1996).\nOverall, these visual findings might already suggest which type of time series model could be useful. After (at least a preliminary) a statistical model, as we will consider more detail in the coming sections, has been chosen, one can then for example:\nThe end purpose of how the statistical model will be eventually used varies depending on the situation and the fields of application.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "TSE-ch1.html#timeseriesdata",
    "href": "TSE-ch1.html#timeseriesdata",
    "title": "2  Introduction",
    "section": "",
    "text": "Time series data are used and analysed in various fields, such as economics and finance, but also in social, natural, engineering, and medical sciences.\nIn economics and finance, data sets are typically time series, which have led to relatively specific subfields of macroeconometrics and financial econometrics.\n\n\n\n\n\n\n\n\n\nThis violates the no-autocorrelation assumption typically imposed on the linear regression model (for cross-sectional datasets).\nWith cross-sectional data, random sampling (usually) guarantees that the error terms of the linear regression model for different observations are mutually independent, and hence autocorrelation is (typically) not an issue.\n\n\n\nIs there a trend (i.e. the values of the time series tend to increase or decrease over time)?\nAre there seasonal effects (a regularly repeating pattern of highs and lows related to calendar time such as seasons, quarters, months or days of the week)?\nAny major outliers (observations far away from other data points)?\nIs the variability (variance) relatively constant over time?\nAre there any abrupt changes such as breaks or regime switches to either the level or variance of the series?\n\n\n\n\nCharacteristics behind these type of “stochastic” trends, as in the S&P 500 index, will be addressed later in this course when discussing nonstationary time series data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nestimate the unknown parameters of the model,\ncheck whether the estimated model and the observations are compatible,\ntest hypotheses concerning the model parameters,\nand finally, use the model for the intended end purpose.\n\n\n\nOne use is to give a summarized description of the observed data set, which may be useful in understanding the underlying data generation process producing the data.\nAnother central use of a time series model is to forecast the future values of the single or multiple time series. In the multivariate case, one time series of interest may be predicted by making use of information contained in other time series as predictive variables.\nOne is often also interested in exploring potential temporal and contemporaneous dependencies between the time series, such as the relationship between inflation and stock market returns, after relevant transformations as discussed next. One may be interested in investigating whether higher inflation typically occurs simultaneously with increasing stock prices (in such an investigation, it would be appropriate to take also various other time series variables into account).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "TSE-ch1.html#transformations",
    "href": "TSE-ch1.html#transformations",
    "title": "2  Introduction",
    "section": "2.2 Transformations",
    "text": "2.2 Transformations\nAs described above, in time series modelling a good first step is always to plot the time series under investigation. This gives an idea of the main features of the time series such as trends, seasonal patterns and/or apparent sudden breaks or outlying observations. In what follows in the next few Sections, it is assumed that these patterns do not exist. If the observed series does exhibit several such patterns, one should use a transformation is to eliminate these patterns.\nIn time series analysis, a few transformations and “manipulations” are generally used and they are even natural and common parts of the empirical analyses in certain applications.\n\nThe first lag of the time series \\(y_t\\) is denoted by \\(y_{t-1}\\). Generally \\(k\\)th lag is \\(y_{t-k}\\).\nPerhaps the most commonly used transformation for time series data is differencing. The first difference of the series, \\(\\Delta y_t\\), is the change between \\(y_t\\) and \\(y_{t-1}\\). In other words, \\[\\begin{equation*}\n\\Delta y_t = y_t - y_{t-1}.\n\\end{equation*}\\]\nOften differencing is combined with taking natural logarithms (when assuming \\(y_t &gt; 0, \\, \\forall t\\)). Taking logarithms is common especially when the series exhibits exponential growth or when the variation in the series increases as the level increases. The logarithmic change is \\[\\begin{equation*}\n\\Delta \\mathrm{log}(y_t) = \\mathrm{log}(y_t) - \\mathrm{log}(y_{t-1}).\n\\end{equation*}\\] Importantly, the logarithmic changes can be interpreted as relative changes when the changes are small: \\[\\begin{equation*}\n\\Delta \\mathrm{log}(y_t) \\approx (y_t - y_{t-1})/y_{t-1}.\n\\end{equation*}\\]\nThe percentage of change between periods \\(t\\) and \\(t-1\\) is \\(100 \\Delta \\mathrm{log}(y_t)\\). With quarterly data \\(y_t\\), annualized growth rates are obtained by multiplying 400, that is \\(400 \\Delta \\mathrm{log}(y_t)\\), whereas annualized growth rates with monthly data can be approximated as \\(1200 \\Delta \\mathrm{log}(y_t)\\).\n\nAs an example, consider the quarterly U.S. CPI time series. Taking log-differences (first logs and then differences), and multiplying the resulting series with 1200 we obtain yearly inflation rate. Due to an increasing trend in the CPI, after log-differencing, we obtain a time series without clear trend. This detrending via log-differencing means that instead of an increasing trend, we have nonzero positive mean that can be seen here as an average inflation rate.\n\n\n\n\n\n\n\n\n\nFigure: Annualized U.S. inflation (CPI inflation) rate between February 1990 to June 2025 (1990:2–2025:6).\n \nMore on differencing. Differencing may also happen with a longer lag than the first lag: \\(y_{t}-y_{t-s},\\, \\left(s\\geq1\\right)\\).\n\nAs discussed above, the above case \\(s=1\\) is the most common (representing change from one period to another).\nIn the case of seasonal variation, \\(s\\) is often selected to match the length of the seasonal pattern. That is, for example, \\(s=4\\) for the quarterly data and \\(s=12\\) for the monthly series.\n\nIf denoting temporarily \\(y_t\\) as the log CPI, taking seasonal differencing \\(100 (y_{t}-y_{t-12})\\) yields another proxy for the U.S. inflation (cf. the time series above)\n\n\n\n\n\n\n\n\n\nFigure: Annualized U.S. inflation (CPI inflation) obtained as year-to-year changes in the CPI (1990:2–2025:6).\n \nAs another use of differencing, especially in financial econometrics and empirical finance, is to financial time series and returns that different assets yield for investors. Let \\(P_t\\) the price of an asset at time \\(t\\). Holding the asset for one period from \\(t-1\\) to \\(t\\) would result in a simple return \\[\\begin{equation*}\nR_t = \\frac{P_t}{P_{t-1}} - 1 = \\frac{P_t - P_{t-1}}{P_{t-1}}.\n\\end{equation*}\\] Moreover, so called continuously compounded return (or log return) is \\[\\begin{equation*}\nr_t = \\mathrm{log}(1+R_t) = \\mathrm{log} \\Big(\\frac{P_t}{P_{t-1}}\\Big) =p_t - p_{t-1},\n\\end{equation*}\\] where \\(p_t = \\mathrm{log} (P_t)\\) and having some advantages over simple returns. In both cases percentage returns are obtained by multiplying \\(r_t\\) or \\(R_t\\) by 100.\n\nImportantly, in various financial applications, dividend (and equivalent) payments must also be taken into account when computing returns. If an asset pays dividends periodically, we must modify the definitions of asset returns. We will not consider these issues more detail in this course and details are left to more detailed studies in finance.\n\nBelow we depict the annualized 1. log-differences of the monthly S&P 500 index (as percentages when multiplied by 100), as presented above). We therefore use simple log-differences approximating percentage returns in the general U.S. market index. The shape of the time series is now clearly very different than the price level data depicted above.\n\n\n\n\n\n\n\n\n\nFigure: (Annualized) monthly percentage changes in the S&P 500 index.\n \nDetrending. The trend and/or seasonal patterns etc. of the original time series are often modelled via transformation so that this step may itself require estimation of some parameters. For example, in the case of a linear trend, one can fit a trending line using least squares regression, and then consider modelling the resulting residual series using time series methods. In that case, an observed time series \\(y_{t}\\) would be replaced by the time series \\[\\begin{equation*}\n    \\hat{x}_{t}=y_{t}-\\hat{\\alpha}-\\hat{\\boldsymbol{\\beta}}t,\\,\\,t=1,\\ldots,T,\n\\end{equation*}\\] where the estimates \\(\\hat{\\alpha}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) are obtained by regressing \\(y_{t}\\) on a constant and \\(t\\). More generally, instead of the linear function (of time), one can use more general polynomials as well.\n\nTo model seasonal variation, one could use trigonometric functions of time.\nAn often used alternative to remove seasonal variation (if not using already existing seasonally adjusted data) is the so-called seasonal indicators. As an example of this, for quarterly data, one would obtain seasonally-adjusted series by \\[\\begin{equation*}\n  \\hat{x}_{t}=y_{t}-\\hat{\\boldsymbol{\\beta}}_{1}d_{1t}-\\cdots-\\hat{\\boldsymbol{\\beta}}_{4}d_{4t},\\,\\,t=1,\\ldots,T,\n\\end{equation*}\\] where the \\(i\\)th seasonal indicator \\(d_{it}\\) takes value one when the observation in question corresponds to quarter \\(i\\) and zero otherwise (\\(i=1,\\ldots,4)\\).\n\nAs for additional information (in this course), there is also a relatively broad literature on more advanced detrending techniques than differencing and seasonal dummies in econometrics.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "TSE-ch1.html#assumptions-and-limitations-of-this-course",
    "href": "TSE-ch1.html#assumptions-and-limitations-of-this-course",
    "title": "2  Introduction",
    "section": "2.3 Assumptions and limitations of this course",
    "text": "2.3 Assumptions and limitations of this course\nIn summary, we consider time series models/processes assuming that the observed time series do not exhibit clear seasonal patterns. If necessary, those patterns have been eliminated using some transformation or using some seasonal adjustment methods, and one is eventually modelling the transformed series.\n\nThe typical approach especially with macroeconomic data is to use seasonally-adjusted datasets provided by official statistics organizations (like Statistics Finland). Therefore, we can assume that the seasonal effects are, at least at reasonable degree, already taken into account.\n\nConcerning trends, in the first part of this course (Sections 2–9) we consider stationary time series processes and their realizations (i.e. time series). These assume that there are no clear trends and/or trending type behaviour in the time series.\n\nThat is the time series are more or less realizations which resemble more U.S. inflation series and stock market returns above than the CPI level series or the s&P 500 price series.\nAs for now, it suffices to understand that non-stationary time series can usually (though not always) be transformed by differencing such that stationarity assumptions hold reasonably well and thus a stationary such as ARMA(\\(p,q\\)) process is possible.\nAn exception to the stationary time series starts becomes at the later part of the material where trending type of behaviour will be considered more detail. The basics of nonstationary processes will be discussed in Sections 12–13 where modelling the deterministic and/or especially the stochastic trend is of interest in several applications.\n\nFinally, we will start with univariate time series. That is we are analysing one time series at hand. Later on in Sections 10–11 and 13, we will also consider basics of multiple (multivariate) time series.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "TSE-ch1.html#r-lab",
    "href": "TSE-ch1.html#r-lab",
    "title": "2  Introduction",
    "section": "2.4 R Lab",
    "text": "2.4 R Lab\nAll the R codes considered in this section are compiled in the following link (Tab). Notice that you might need to download some datasets from the course website and include them to the same folder as this file.\n\nR Code\n\n\n\n\n# Introduction\n\n# Shiller data\n\n# This script reads monthly data and creates separate plots for CPI and \n# S&P 500 (variable P) using both base R and the ggplot2 package.\n\n# --- Setup ---\n\n# Install and load necessary packages\n# If you don't have these packages, uncomment the following lines to install them\n# install.packages(\"ggplot2\")\n# install.packages(\"zoo\")\n\nlibrary(ggplot2)\nlibrary(zoo) # Used for handling year.month date formats in this Shiller data\n\n# Set locale to English to ensure month names are not localized\nSys.setlocale(\"LC_TIME\", \"C\")\n\n[1] \"C\"\n\n# --- Define/select data and sample period ---\n\n# Set the sample period. The format (for this monthly data) is \"YYYY-MM\".\nstart_period &lt;- \"1990-01\"\nend_period &lt;- \"2025-06\"\n\n# Define the data file name\ndata_file &lt;- \"Shiller_data_031025.txt\"\n\n\n# --- Load and prepare data ---\n\n# Read the data from the text file. This assumes a tab-separated file with a header.\n# If your separator is different (e.g., a comma), use read.csv() instead.\ntryCatch({\n  shiller_data &lt;- read.delim(data_file, header = TRUE)\n}, error = function(e) {\n  stop(paste(\"Error reading the file:\", \n             data_file, \". Make sure it's in the same folder as the script.\"))\n})\n\n# Convert the 'Date' column to a proper time series date object\nshiller_data$Date_ts &lt;- as.yearmon(as.character(shiller_data$Date), \"%Y.%m\")\n\n# Filter the data to the desired sample period\ndata_subset &lt;- subset(shiller_data, Date_ts &gt;= as.yearmon(start_period) & \n                        Date_ts &lt;= as.yearmon(end_period))\n\n\n# --- Create Time Series Objects for Base R plotting ---\n\n# Get the start year and month from the first data point in our subset\nstart_year &lt;- floor(as.numeric(data_subset$Date_ts[1]))\nstart_month &lt;- cycle(data_subset$Date_ts[1])\n\n# Create 'ts' (time series) objects, here monthly data\ncpi_ts &lt;- ts(data_subset$CPI, start = c(start_year, start_month), frequency = 12)\np_ts &lt;- ts(data_subset$P, start = c(start_year, start_month), frequency = 12)\n\n\n# --- Plot (i): CPI Time Series ---\n\n# (i) Base R Implementation for CPI using plot.ts\nplot.ts(cpi_ts,col = \"blue\",lwd = 2,main = \"CPI\",xlab = \"Date\",ylab = \"CPI\")\n\n\n\n\n\n\n\n# (i) ggplot2 Implementation for CPI\ncpi_plot_ggplot &lt;- ggplot(data_subset, aes(x = Date_ts, y = CPI)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  labs(title = \"CPI\",subtitle = paste(\"From\", start_period, \"to\", end_period),\n    x = \"Date\",y = \"Consumer Price Index (CPI)\") +\n  theme_minimal() + theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n# Print the ggplot object to display it\nprint(cpi_plot_ggplot)\n\n\n\n\n\n\n\n# --- Plot (ii): S&P 500 (P) Time Series ---\n\n# (ii) Base R Implementation for S&P 500 (P) using plot.ts\nplot.ts(p_ts,col = \"blue\",lwd = 2,main = \"S&P 500 (variable P)\",\n        xlab = \"Date\", ylab = \"S&P 500 Index\")\n\n\n\n\n\n\n\n# (ii) ggplot2 Implementation for S&P 500 (P)\np_plot_ggplot &lt;- ggplot(data_subset, aes(x = Date_ts, y = P)) +\n  geom_line(color = \"darkseagreen4\", size = 1) +labs(\n    title = \"S&P 500 (P) Time Series (ggplot2)\", subtitle = paste(\"From\", \n    start_period, \"to\", end_period),x = \"Date\",y = \"S&P 500 Index (P)\") +\n  theme_minimal() + theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n# Print the ggplot object\nprint(p_plot_ggplot)\n\n\n\n\n\n\n\n# --- Differencing: Calculate and plot log-differences ---\n\n# Calculate log-differences. The result is shorter, so we add NA to align it.\ndata_subset$cpi_log_diff_1m &lt;- c(NA, 1200*diff(log(data_subset$CPI))) # annualized\ndata_subset$cpi_log_diff_12m &lt;- c(rep(NA, 12), 100*diff(log(data_subset$CPI), 12))\ndata_subset$p_log_diff &lt;- c(NA, 100*diff(log(data_subset$P)))\n\n# Create new 'ts' objects for the differenced series\ncpi_log_diff_1m_ts &lt;- ts(data_subset$cpi_log_diff_1m, start = c(start_year, \n                        start_month), frequency = 12)\ncpi_log_diff_12m_ts &lt;- ts(data_subset$cpi_log_diff_12m, start = c(start_year, \n                        start_month), frequency = 12)\np_log_diff_ts &lt;- ts(data_subset$p_log_diff, start = c(start_year, \n                        start_month), frequency = 12)\n\n\n# --- Plot (iii): 1. log-difference in CPI (Monthly Inflation) ---\n\n# (iii) Base R Implementation\nplot.ts(cpi_log_diff_1m_ts, col = \"blue\", lwd = 2, \n        main = \"Annualized monthly inflation\", xlab = \"Date\",\n        ylab = \"(Annualized) 1. log-difference of U.S. CPI\")\n\n\n\n\n\n\n\n# (iii) ggplot2 Implementation\nggplot(data_subset, aes(x = Date_ts, y = cpi_log_diff_1m)) +\n  geom_line(color = \"purple\", size = 1) +\n  labs(title = \"Annualized monthly inflation (ggplot2)\",\n    subtitle = \"(Annualized) 1. log-difference of CPI\",\n    x = \"Date\", y = \"Log-Difference\"\n  ) +\n  theme_minimal() + theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n# --- Plot (iv): 12-month log-difference in CPI (Year-over-Year Inflation) ---\n\n# (iv) Base R Implementation\nplot.ts(cpi_log_diff_12m_ts,col = \"blue\", lwd = 2,\n        main = \"Year-over-Year U.S. inflation \", xlab = \"Date\", \n        ylab = \"12-month log-difference of CPI\")\n\n\n\n\n\n\n\n# (iv) ggplot2 Implementation\nggplot(data_subset, aes(x = Date_ts, y = cpi_log_diff_12m)) + \n  geom_line(color = \"orange\", size = 1) + labs(\n    title = \"Year-over-Year Inflation (ggplot2)\",\n    subtitle = \"12-month log-difference of CPI\",\n    x = \"Date\", y = \"Log-Difference\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n# --- Plot (v): Log-difference of P (S&P 500 monthly returns) ---\n\n# (v) Base R Implementation\nplot.ts(p_log_diff_ts,col = \"blue\",lwd = 2, main = \"S&P 500 monthly returns\",\n        xlab = \"Date\",\n        ylab = \"log-difference of P\")\n\n\n\n\n\n\n\n# (v) ggplot2 Implementation\nggplot(data_subset, aes(x = Date_ts, y = p_log_diff)) +\n  geom_line(color = \"red\", size = 1) +\n  labs(\n    title = \"S&P 500 monthly returns (ggplot2)\",\n    subtitle = \"log-difference of P\",\n    x = \"Date\", y = \"Log-Difference\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n# --- Plot Built-in Dataset: sunspots ---\nplot(sunspots, main = \"Sunspots data\", xlab = \"Year\",\n     ylab = \"Monthly sunspot numbers\",col = \"blue\")\n\n\n\n\n\n\n\n# --- Plot Built-in Dataset: USAccDeaths ---\nplot.ts(USAccDeaths,main = \"Accidental Deaths in the US 1973-1978\",\n        xlab = \"Year\", ylab = \"Number of Accidental Deaths\", col = \"blue\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "TSE-ch2.html",
    "href": "TSE-ch2.html",
    "title": "3  A primer on time series models",
    "section": "",
    "text": "3.1 Starting point\nRelation to the linear regression for cross-sectional datasets. In standard econometrics or statistics courses, linear regression models are typically introduced using cross-sectional data. A key assumption for these models is the absence of autocorrelation in the error term. With time series data, however, autocorrelation is a fundamental characteristic and is almost always present.\nWe will return to multivariate regression models with time series variables later. First, we will explore a pure time series approach (as characterized by Verbeek, 2008).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A primer on time series models</span>"
    ]
  },
  {
    "objectID": "TSE-ch2.html#starting-point",
    "href": "TSE-ch2.html#starting-point",
    "title": "3  A primer on time series models",
    "section": "",
    "text": "While linear regression can be applied to time series variables, the notation changes to reflect the temporal ordering. Variables are indexed by time, such as \\(y_t\\) and \\(x_t\\) (instead of \\(y_i\\) and \\(x_i\\) for cross-sectional observations).\n\n\n\nIn this approach, the current value of a univariate time series, \\(y_t\\), is modeled as a function of its past values, either directly or indirectly.\nThe emphasis is therefore entirely on using the information contained in \\(y_t\\) and its own history for modeling and forecasting purposes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A primer on time series models</span>"
    ]
  },
  {
    "objectID": "TSE-ch2.html#notes-on-deterministic-components",
    "href": "TSE-ch2.html#notes-on-deterministic-components",
    "title": "3  A primer on time series models",
    "section": "3.2 Notes on deterministic components",
    "text": "3.2 Notes on deterministic components\nIn time series analysis and models for such data, we often decompose a time series process \\(y_t\\) into a deterministic and stochastic parts. This allows us to separate deterministic parts, like the mean and trend, from the random fluctuations that are the primary focus of stochastic models, such as in ARMA or VAR models to be introduced in this course.\nThe general decomposition and starting point in our notation throughout this material is given by: \\[\\begin{equation*}\ny_t = \\mu_t + z_t,\n\\end{equation*}\\] where the components are defined as:\n\n\\(\\mu_t\\): This term represents the deterministic part.\n\\(z_t\\): This is the stochastic part, representing (random) fluctuations around the deterministic component. A crucial assumption is that this process has a zero mean, i.e., \\(\\mathsf{E}(z_t)=0\\). Therefore, \\(z_t\\) represents the demeaned and detrended time series process, which may, and typically, have its own autocorrelation structure and hence importantly \\(z_t\\) is not just pure random noise.\n\nBy this construction, the expected value of \\(y_t\\) is simply its deterministic component: \\[\\begin{equation*}\n\\mathsf{E}(y_t) = \\mathsf{E}(\\mu_t + z_t) = \\mu_t + \\mathsf{E}(z_t) = \\mu_t\n\\end{equation*}\\]\n \nCommon forms of the deterministic component. The functional form of \\(\\mu_t\\) depends on the underlying characteristics of the data and application in general. Two of the most common specifications are:\n\nConstant mean: If the time series fluctuates around a fixed, time-invariant level, the deterministic component is simply a constant \\[\\begin{equation*}  \n  \\mu_t = \\mu.\n\\end{equation*}\\] Here, \\(\\mu\\), or at times \\(\\mu_0\\), represents the unconditional mean of the process \\(\\mathsf{E}(y_t)\\). In general, almost all real-life time series have nonzero mean.\nConstant and linear time trend: If the series exhibits a persistent upward, or downward, movement over time, we also include a linear trend (together with the constant term). The deterministic component then includes both a constant and a time trend \\[\\begin{equation*}\n  \\mu_t = \\mu_0 + \\mu_1 t\n\\end{equation*}\\] where \\(\\mu_0\\) is the constant (the value of \\(\\mu_t\\) at \\(t=0\\)), and \\(\\mu_1\\) is the trend coefficient, representing the average change in \\(y_t\\) from one period to the next.\n\nIn practice, at times we first estimate the parameters of the deterministic part \\(\\mu_t\\), e.g., via Ordinary Least Squares, and then work with \\(z_t\\) as a residual between \\(y_t - \\mu_t\\). This is the tactic as described in Section 1 in connection to simple detrending techniques. Alternatively, and more often, modelling deterministic and random components will be carried out simultaneously.\n\nIn our notation, \\(x_t\\), or \\(\\boldsymbol{x}_t\\) in the case of multiple time series, is used for a general time series process (for different notational purposes) and most often they are explanatory (predictive) variables.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A primer on time series models</span>"
    ]
  },
  {
    "objectID": "TSE-ch3.html",
    "href": "TSE-ch3.html",
    "title": "4  Stationary processes",
    "section": "",
    "text": "4.1 Basic concepts\nThe aim is to build a statistical model for the observed time series \\(y_{1},\\ldots,y_{T}\\). At this stage, and in Sections 3–9 and 12, we assume that we have one, univariate, time series. Multiple time series and multivariate models will be briefly considered later in Sections 10–11 and 13.\nOn a rather general level, the observed time series can be interpreted as the observed value of the random vector \\(\\boldsymbol{y}=\\left(y_{1},\\ldots,y_{T}\\right)\\) or, in other words, the realization of this random vector.\nWhen building a statistical model, we would attempt to specify the probability distribution of the \\(T\\)–dimensional random vector \\(\\boldsymbol{y}=\\left(y_{1},\\ldots,y_{T}\\right)\\) that produced the observed time series.\nA concrete and often used way to specify the joint distribution of \\(\\boldsymbol{y}\\) in time series analysis is to:\nBefore we proceed to such concrete model equations, it is useful to first briefly study some basic concepts of probability theory that form the foundation of time series analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch3.html#basic-concepts",
    "href": "TSE-ch3.html#basic-concepts",
    "title": "4  Stationary processes",
    "section": "",
    "text": "Recall the notation: Here the random vector interpretation emphasizes the fact that \\(y_t\\) will be interpreted as a random variable. Notice that We do not make a distinction between random variables and their realizations; the difference should be clear from the context.\n\n\n\nNote and recall that in a time series, the observations are (typically) not independent as in the conventional linear regression, and therefore it is not enough to specify the marginal distributions of the components \\(y_{t}\\) \\(\\left(t=1,\\ldots,T\\right)\\).\nInstead, we indeed need to specify the \\(T\\)–dimensional joint distribution.\n\n\n\nSpecify a model equation that characterises the dependence structure of consecutive observations, and\ncombine this with a suitable distributional assumption.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch3.html#stochastic-processes",
    "href": "TSE-ch3.html#stochastic-processes",
    "title": "4  Stationary processes",
    "section": "4.2 Stochastic processes",
    "text": "4.2 Stochastic processes\nIn statistical and econometric modelling of time series, one concentrates on the observed time series. That is the realization of a random vector \\(\\boldsymbol{y}=\\left(y_{1},\\ldots,y_{T}\\right)\\).\nFrom a mathematical point of view, however, it is more convenient to consider an “extension” of this vector to all time indices when defining time series models. This leads us to the concept of (a discrete time) stochastic process (or just simply process), which is a collection of random variables \\(\\left\\{y_{t};t=0,\\pm1,\\pm2,\\ldots\\right\\}\\).\n\nOccasionally, the cases \\(t=0,1,2,\\ldots\\) or \\(t=1,2,\\ldots\\) are also considered.\nBecause the set of values the time index takes is usually known, it does not need to be emphasized, and often we can simply denote the stochastic process by \\(y_{t}\\), or \\(\\left\\{y_{t}\\right\\}\\) if one wants to emphasize the distinction to a single component of the process.\nThe terms “process” and “model” are often used interchangeably in time series analysis. In this material, “process” emphasizes to the underlying theoretical mechanism (that is, the stochastic process) we aim to capture with an empirical and estimated “model” for the time series.\n\nFrom a practical point of view, it is important to note that we have only a single realization available, the observations \\(\\left(y_{1},\\ldots,y_{T}\\right)\\), of the process \\(\\left\\{y_{t}\\right\\}\\).\n\nIn other words, we have only one realization from the process and that is only partial, that is observations are available only for the time periods \\(t=1,\\ldots,T\\).\n\nThese above-mentioned points imply that in order to investigate the properties of \\(y_t\\) by making use of the observed data \\(y_{1},\\ldots,y_{T}\\) only, one must make some restrictive assumptions about \\(y_t\\) for this to be possible.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch3.html#weak-stationarity",
    "href": "TSE-ch3.html#weak-stationarity",
    "title": "4  Stationary processes",
    "section": "4.3 Weak stationarity",
    "text": "4.3 Weak stationarity\nTo illustrate the final remarks in the previous section, consider the expected value function of the process \\(y_{t}\\) (assume here \\(\\mathsf{E}\\left(y_{t}^{2}\\right)&lt;\\infty\\) for all \\(t\\)) \\[\\begin{equation*}\n\\mu_{t}=\\mathsf{E}\\left(y_{t}\\right),\\quad t=0,\\pm1,\\pm2,\\ldots\n\\end{equation*}\\] as well as the covariance function of \\(y_{t}\\) \\[\\begin{equation*}\n\\gamma_{s,t}=\\mathsf{Cov}\\left(y_{s},y_{t}\\right)=\\mathsf{E}\\left[\\left(y_{s}-\\mu_{s}\\right)\\left(y_{t}-\\mu_{t}\\right)\\right], \\quad t=0,\\pm1,\\pm2,\\ldots\\text{.}\n\\end{equation*}\\]\nEstimating these functions by making use of one realization \\(y_{1},\\ldots,y_{T}\\) only is of course impossible, unless we somehow restrict/limit their dependence on time. The situation simplifies if it is assumed that these quantities do not depend on time. Indeed, it is common to assume that such a dependence on time does not (in a particular sense) exist.\nA process \\(y_{t}\\) is called weakly stationary, or covariance stationary, if \\[\\begin{equation*}\n\\mathsf{E}\\left(y_{t}\\right)=\\mu &lt; \\infty \\quad \\mathrm{for\\,\\, all} \\quad t=0,\\pm1,\\pm2,\\ldots\n\\end{equation*}\\] and \\[\\begin{equation*}\n\\mathsf{Cov}\\left(y_{t},y_{t+h}\\right)=\\gamma_{t,t+h}=\\gamma_{0,h} &lt; \\infty \\,\\, \\text{for all }\\,\\, h,\\text{ }t=0,\\pm1,\\pm2,\\ldots\\text{ .}\n\\end{equation*}\\] In other words, a form of time invariance holds: The expected value function \\(\\mu_{t}\\) is constant over time, and the covariance function \\(\\gamma_{s,t}\\) does not depend on the time indices \\(s\\) and \\(t\\) but only on their distance \\(t-s\\).\n\nFor brevity, denote \\(\\gamma_{0,h} \\equiv \\gamma_{h}\\).\nWhen \\(h\\) is fixed, \\(\\gamma_{h}\\) is called the autocovariance (coefficient) of \\(y_{t}\\) at lag \\(h\\). As a function of \\(h\\), the \\(\\gamma_{h}\\) is called the autocovariance function of \\(y_{t}\\).\nThe \\(\\mu\\) is called the expected value or the mean.\n\n \nTo summarize, a characteristic feature of weakly stationary process is that the first and second moments are finite and independent of time. Unless otherwise mentioned, in what follows (in this and coming few sections), we assume that weak stationarity holds.\n \nA useful practical aspect of weak stationarity is that the plausibility of this assumption can be easily investigated by looking at a graph of the observed time series: If the observations vary with constant variance around a fixed level, then weak stationarity appears a plausible assumption.\nLet us consider an example related to U.S. real GDP growth during the period 1985–2007, commonly referred to as the Great Moderation time period.\n\nMore on The Great Moderation, see https://www.federalreservehistory.org/essays/great-moderation\nReal Gross Domestic Product (GDP) is the total value of a country’s goods and services produced in a specific period, adjusted for inflation.\n\nThis figure (see below) illustrates that the time series fluctuates around a relatively stable mean and exhibits approximately constant variance over time. As discussed, this behavior is typical for a weakly stationary process.\nIt is important to note that weak stationarity does not imply the absence of variability. Even in a weakly stationary process, individual observations can vary substantially due to random shocks. What matters is that the statistical properties — the mean, variance, and autocovariance structure — remain constant over time. So, occasional high or low values in the series are entirely consistent with weak stationarity, as long as they are the result of random variation and not due to major structural changes in the process.\n\nWe will soon see more examples of weakly stationary time series through simulations.\nIn Section 1, the time series depicting the U.S. inflation rate can also be interpreted as realizations from weakly stationary processes.\n\n\n\n\n\n\n\n\n\n\nFigure: (Annualized) U.S. quarterly real GDP growth rate (1985:Q1-2007:Q2) (ggplot2 figure).\n \nBecause \\(\\gamma_{0}=\\mathsf{Var}\\left(y_{t}\\right)\\), it holds that \\(\\gamma_{0}\\geq0\\). Furthermore, the familiar properties of the covariance function imply that\n\n\\(\\left\\vert \\gamma_{h}\\right\\vert \\leq\\gamma_{0}\\).\nIn this univariate (single time series) case, we have (under weak stationary)\n\\[\\begin{equation*}\n\\mathsf{Cov}\\left(y_{t},y_{t+h}\\right)=\\mathsf{Cov}\\left(  y_{t-h},y_{t}\\right) = \\mathsf{Cov}\\left(y_t, y_{t-h}\\right),\n\\end{equation*}\\] which also implies that \\(\\gamma_{h}=\\gamma_{-h}\\). This expression shows where the term lag (lag length) \\(h\\) is coming from.\n\nPutting the above properties together, the autocovariance function has the properties \\[\\begin{equation*}\n\\gamma_{0}\\geq0, \\,\\, \\left\\vert \\gamma_{h}\\right\\vert \\leq\\gamma_{0} \\quad \\mathrm{and} \\quad \\gamma_{h}=\\gamma_{-h}.\n\\end{equation*}\\] The case \\(\\gamma_{0}=0\\) is obviously not of interest, and in what follows we assume that \\(\\gamma_{0}&gt;0\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch3.html#autocorrelation-function",
    "href": "TSE-ch3.html#autocorrelation-function",
    "title": "4  Stationary processes",
    "section": "4.4 Autocorrelation function",
    "text": "4.4 Autocorrelation function\nIn practice, a more convenient concept than autocovariance function is the autocorrelation function \\[\\begin{equation*}\n\\rho_{h}=\\mathsf{Cor}\\left(y_{t},y_{t+h}\\right)= \\mathsf{Cor}\\left(y_t, y_{t-h}\\right)=\\gamma_{h}/\\gamma_{0}.\n\\end{equation*}\\] The autocorrelation coefficients \\(\\rho_{h}\\) obviously then have the properties \\[\\begin{equation*}\n\\rho_{0}=1, \\,\\, \\left\\vert \\rho_{h}\\right\\vert \\leq1, \\, \\mathrm{and} \\,\\,  \\rho_{h}=\\rho_{-h}.\n\\end{equation*}\\] Therefore, it suffices to consider the autocovariance and autocorrelation functions only for the lag lengths \\(h\\geq 0\\)\n\nThe latter just for the lags \\(h&gt;0\\)\n\nDue to the time invariance guaranteed by weak stationarity, it now seems evident that \\(\\mathsf{E}\\left(y_{t}\\right)=\\mu\\) and \\(\\mathsf{Cor}\\left(y_{t},y_{t+h}\\right)=\\gamma_{h}\\) can be estimated. One more problem remains: The autocorrelation function, in general, has an infinite number of quantities to be estimated. In practice, all of these cannot be estimated (without further restrictions). However, typically we can assume that \\[\\begin{equation*}\n\\gamma_{h}\\rightarrow0, \\,\\, \\mathrm{when} \\,\\, h\\rightarrow\\infty.\n\\end{equation*}\\]  In this case, the random variables \\(y_{t}\\) and \\(y_{t+h}\\) become nearly uncorrelated when the distance \\(h\\) is “large” – that is, when \\(y_{t}\\) and \\(y_{t+h}\\) are “far” from each other in time.\n\nTherefore, in practice it suffices to estimate the autocorrelation function \\(\\rho_{h}\\), when \\(h=1,\\ldots,H\\), and \\(H\\) is so large that \\(\\rho_{h}\\approx0\\) for \\(h&gt;H\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch3.html#sample-autocorrelation-function",
    "href": "TSE-ch3.html#sample-autocorrelation-function",
    "title": "4  Stationary processes",
    "section": "4.5 Sample autocorrelation function",
    "text": "4.5 Sample autocorrelation function\nAssume that the process \\(y_{t}\\) is weakly stationary. Then \\(\\mathsf{E}\\left(y_{1}\\right)=\\cdots=\\mathsf{E}\\left(y_{T}\\right)=\\mu\\). Therefore, a natural estimator of the (population) mean is the sample mean \\[\\begin{equation*}\n\\bar{y}=\\frac{1}{T}\\sum_{t=1}^{T}y_{t}.\n\\end{equation*}\\]\nBecause (under weak stationarity) \\(\\mathsf{Cov}\\left(y_{1},y_{1+h}\\right)=\\cdots=\\mathsf{Cov}\\left(y_{T-h},y_{T}\\right)=\\gamma_{h}\\) \\(\\left(h\\geq0\\right)\\), it also seems natural to estimate the population autocovariance coefficient \\(\\gamma_{h}\\) using the sample covariance of the observations \\(\\left(y_{1},y_{1+h}\\right),\\ldots,\\left(y_{T-h},y_{T}\\right)\\). Because \\(\\mathsf{E}\\left(y_{t}\\right)=\\mathsf{E}\\left(y_{t+h}\\right)\\) (again under weak stationary), a commonly used estimator is \\[\\begin{equation*}\n\\mathsf{c}_{h}=\\frac{1}{T-h}\\sum_{t=1}^{T-h}(y_{t}-\\bar{y})\\left(y_{t+h}-\\bar{y}\\right), \\quad 0\\leq h&lt;T,\n\\end{equation*}\\] which is called the (\\(h\\)th) sample autocovariance coefficient. Sometimes, the denominator \\(T-h\\) is replaced by \\(T\\).\n\nWhen \\(\\mathsf{c}_{h}\\) is interpreted as a function of \\(h\\) (i.e. \\(c_1, c_2,\\ldots\\)), it is called the sample autocovariance function.\n\nA natural estimator of the autocorrelation coefficient \\(\\rho_{h}\\) is hence \\[\\begin{equation*}\n\\mathsf{r}_{h}=\\mathsf{c}_{h}/\\mathsf{c}_{0}, \\qquad 0\\leq h&lt;T,\n\\end{equation*}\\] which is called the sample autocorrelation coefficient and \\(\\mathsf{r}_1, \\mathsf{r}_2, \\ldots\\) define the sample autocorrelation function.\n\n\nNotice that by definition \\(\\mathsf{r}_0=1\\). Some statistical program packages report this uninteresting case.\n\n \nAs an example, consider the quarterly U.S. real GDP growth rate (1985:Q1–2007:Q2). Its sample autocorrelation function is plotted below for the lags \\(h=0,\\ldots,20\\). The two horizontal lines represent certain “confidence bands”.\n\nIf \\(\\rho_{h}=0\\) for all \\(h&gt;0\\), then any individual estimator \\(\\mathsf{r}_{h}\\) has an approximately 95% probability of lying between the confidence bands \\((-1.96/\\sqrt{T}, 1.96/\\sqrt{T})\\) where \\(T\\) is the number of observations. These confidence bands are depicted below and typically in corresponding figures.\nIf the individual sample autocorrelation coefficient \\(\\mathsf{r}_h\\) does not fall between these bands, there is statistically significant autocorrelation at the 5% significance level at the lag \\(h\\).\n\n\n\n\n\n\n\n\n\n\n\nFigure: Sample autocorrelation function (ACF) of the quarterly U.S. real GDP growth rate (\\(T=90\\), and hence \\(1.96/\\sqrt{T} \\approx 0.207\\).)\n \nThis time series exhibit clear autocorrelation, which generally dampens (slowly) as the lag length \\(h\\) increases.\n\nThis dampening effect depends on the time series (process) considered\nNotice that above the first depicted lag (in this R function) is \\(\\mathsf{r}_1\\). At times in different functions and packages \\(\\mathsf{r}_0=1\\) is depicted, but that is of course not of interest.\nIf there is some (remaining) seasonal variation, say in monthly time series after seasonal adjustment (if applicable), it causes rather strong autocorrelations at lags \\(h=12,\\) \\(24\\) and \\(36\\). Similar behaviour can also happen if the time series contain some other cyclical pattern.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch3.html#weakWN",
    "href": "TSE-ch3.html#weakWN",
    "title": "4  Stationary processes",
    "section": "4.6 White noise process",
    "text": "4.6 White noise process\nWeak stationarity is not a strong enough assumption to allow statistical analysis of time series. It alone does not suffice for the construction of a likelihood function, nor is it enough to derive distributions of estimators or test statistics. For this reason, we need to define another concept of stationarity that is stronger than weak stationarity, strict stationarity. We will consider that in the next section.\nBefore that, let us consider first a white noise process, which is the simplest possible example of a weakly stationary process. Let \\(u_{t}\\) be a process for which it holds that\n\n\\(\\mathsf{E}\\left(u_{t}\\right)  =\\mu &lt; \\infty\\), often \\(\\mathsf{E}(u_t)=0\\),\n\\(\\mathsf{Var}\\left(u_{t}\\right)=\\sigma^{2}&lt;\\infty\\), and\n\\(\\mathsf{Cov}\\left(u_{s},u_{t}\\right)=0\\), when \\(s\\neq t\\).\n\nThis process is often called a (weak) white noise process and denoted by \\(u_{t}\\sim\\mathsf{wn}\\left( \\mu,\\sigma^{2}\\right)\\).\n\nBy definition, it is clear that a white noise process is weakly stationary\nDespite their simplicity, as we will see later on, white noise processes play a central role in time series analysis because they can be used to construct more complicated processes and eventually models.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch3.html#strictstationarity",
    "href": "TSE-ch3.html#strictstationarity",
    "title": "4  Stationary processes",
    "section": "4.7 Strictly stationary process",
    "text": "4.7 Strictly stationary process\nLet us now define the above-mentioned more stringent form of stationarity. A process \\(\\left\\{y_{t};t=0,\\pm1,\\pm2,\\ldots\\right\\}\\) is called strictly stationary (sometimes strongly stationary) if the collections of random variables \\(y_{t_{1}},\\ldots,y_{t_{m}}\\) and \\(y_{t_{1}+h},\\ldots,y_{t_{m}+h}\\) have the same \\(m-\\)dimensional joint distribution for all integers \\(t_{1},\\ldots,t_{m}\\), \\(h\\) and \\(m\\) \\(\\left(  m&gt;0\\right)\\).\n\nThat is, the entire probability structure of the process \\(y_{t}\\) is time-invariant, not just the first and second-order moments.\n\n \nProperties. A strictly stationary stochastic process has the following properties (that follow straightforwardly from the definition):\n\nSS1: The random variables \\(y_{t}\\) have the same distribution for all \\(t\\).\nSS2: The random vectors \\((y_{t},y_{t+h})\\) and \\((y_{s},y_{s+h})\\) have the same distribution for all \\(t\\) and \\(s\\) and all (fixed) \\(h\\).\nSS3: \\(y_{t}\\) is weakly stationary if \\(\\mathsf{E}\\left(y_{t}^{2}\\right)&lt;\\infty\\) holds.\n\nIt is clear that weak stationarity does not, in general, imply strict stationarity.\n\nFor instance, weak stationarity says nothing about moments of order 3 and above.\n\n\nExtra: Gaussian processes\n\n\nIn the case of so-called Gaussian processes, weak stationarity implies strict stationarity. A process \\(y_{t}\\) (not necessarily stationary in any sense) is called normal or Gaussian, if the \\(m\\)–dimensional probability distribution of the random variables \\(y_{t_{1}},\\ldots,y_{t_{m}}\\) is multivariate normal (Gaussian) for all integers \\(t_{1},\\ldots,t_{m}\\) and \\(m\\) \\(\\left(m&gt;0\\right)\\) (for simplicity, occasionally we will simply say normal when meaning multivariate normal distribution). Recall that a multivariate normal distribution is fully determined by its first and second moments. Therefore, weak stationarity and normality together do imply strict stationarity.\n\nA very useful property of strict (but not weak) stationarity is that it is conserved in transformations. In other words,\n\nSS4: If \\(x_{t}\\) is a strictly stationary process, then so is also the process \\(y_{t}=g\\left(x_{t+m},\\ldots,x_{t-n}\\right), m,n\\geq0\\), for any “well-behaving” (e.g. continuous or, more generally, measurable) function \\(g\\). Moreover, \\(m\\), \\(n\\), or both can be replaced with \\(\\infty\\).\n\n\n \n“Stationarity” in this material. Unless otherwise stated, stationarity will be interpreted as strict stationarity from now on.\n\nFrom practical time series analysis perspective, as introduced above, visual inspection of the time series is related to the theoretical implications of weakly stationary processes (i.e. mean, variance and autocovariances of the series are not dependent on time) is of interest.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch3.html#iid-and-nid-processes",
    "href": "TSE-ch3.html#iid-and-nid-processes",
    "title": "4  Stationary processes",
    "section": "4.8 IID and NID processes",
    "text": "4.8 IID and NID processes\nLet \\(u_{t}\\) be a sequence of independent and identically distributed (IID) random variables.\n\nThe strict stationarity of this process is easy to check.\n\n\nExtra: Some further results\n\n\nThe cumulative distribution function of the random vector \\(\\left(u_{t_{1}},\\ldots,u_{t_{m}}\\right)\\) evaluated at \\(\\left(x_{1},\\ldots,x_{m}\\right)\\) is \\(F\\left(x_{1}\\right)  \\cdots F\\left(x_{m}\\right)\\). Clearly, this is also the cumulative distribution function of the random vector \\((u_{t_{1}+h},\\ldots,u_{t_{m}+h})\\) evaluated at \\(\\left(x_{1},\\ldots,x_{m}\\right)\\).\nIf \\(\\mathsf{E}\\left(u_{t}^{2}\\right)&lt;\\infty\\), then \\(u_{t}\\) is also weakly stationary, and in this case it has the same moment properties as the (weak) white noise process above.\n\nThis IID process is also called white noise, or sometimes strong white noise. We will use the shorthand notation \\[\\begin{equation*}\nu_{t}\\sim\\mathsf{iid}\\left(\\mu,\\sigma^{2}\\right) \\quad \\mathrm{or} \\quad u_{t}\\sim\\mathsf{IID}\\left(\\mu,\\sigma^{2}\\right)\n\\end{equation*}\\] where \\(\\mathsf{iid}\\) \\(\\leftrightarrow\\) refers to “independently and identically distributed”), \\(\\mu=\\mathsf{E}\\left(u_{t}\\right)\\) and \\(\\sigma^{2}=\\mathsf{Var}\\left(u_{t}\\right)\\) (in what follows, most often \\(\\mu=0\\)).\nIf it is additionally assumed that \\(u_{t}\\sim\\mathsf{N}\\left(\\mu,\\sigma^{2}\\right)\\), one denotes \\[\\begin{equation*}\nu_{t}\\sim\\mathsf{nid}\\left(\\mu,\\sigma^{2}\\right) \\quad \\mathrm{or} \\quad u_{t}\\sim\\mathsf{NID}\\left(\\mu,\\sigma^{2}\\right)$\n\\end{equation*}\\] where \\(\\mathsf{nid}\\) \\(\\leftrightarrow\\) normally and independently distributed.\n\nIn the following, the above standard notation for the \\(\\mathsf{iid}\\), \\(\\mathsf{nid}\\) and \\(\\mathsf{wn}\\) processes are used in these lecture notes from now on.\n\n \n\nExtra: Some (somewhat peculiar) example cases\n\n\nThe first example demonstrates that the mathematical definition of weak stationarity allows for processes that may feel awkward and surprising. Define the process \\[\\begin{equation*}\ny_{t}=A\\cos\\left(\\lambda t\\right)+B\\sin\\left(\\lambda t\\right),\n\\end{equation*}\\] where \\(\\lambda\\in\\lbrack0,\\pi)\\) is a constant and the random variables \\(A\\) and \\(B\\) satisfy \\(\\mathsf{E}\\left(A\\right)=\\mathsf{E}\\left(B\\right)=0\\), \\(\\mathsf{Var}\\left(A\\right)=\\mathsf{Var}\\left(B\\right)=\\sigma^{2}\\) and \\(\\mathsf{Cov}\\left(A,B\\right)=0\\). Obviously, \\(\\mathsf{E}\\left(y_{t}\\right)=0\\) holds. Using properties of trigonometric functions, straightforward calculation can be used to establish that \\[\\begin{equation*}\n\\mathsf{Cov}\\left(y_{t},y_{t+h}\\right)=\\sigma^{2}\\cos\\left(\\lambda h\\right).\n\\end{equation*}\\] Therefore, \\(y_{t}\\) is weakly stationary (details are left as an exercise). This process is peculiar in that its realizations are quite regular functions of time (linear combinations of \\(\\cos\\left(  \\lambda t\\right)\\) and \\(\\sin\\left(\\lambda t\\right)\\)). Note also that for this process, condition @ref(eq:Acfdecay) does not hold. \\(\\square\\)\n \nThe second example: Define the process \\(y_{t}=u_{t}\\sqrt{\\omega+\\alpha u_{t-1}^{2}}\\), where \\(\\omega&gt;0\\), \\(\\alpha\\geq0\\) and \\(u_{t}\\) is (weak) white noise process and satisfies \\(\\mathsf{E}\\left(u_{t}\\right)=0\\) and \\(\\mathsf{E}\\left(u_{t}^{2}\\right)&lt;\\infty\\). It is easy to see that \\(y_{t}\\) is weak white noise (justification as an exercise). Because \\(y_{t-1}=u_{t-1}\\sqrt{\\omega+\\alpha u_{t-2}^{2}}\\), it is clear that \\(y_{t}\\) is not strong white noise (unless \\(\\alpha=0\\)). \\(\\square\\)\n\n \nAs an illustration of nid process, below we depict a simulated realization from the \\(\\mathrm{nid}(0,1)\\) process. The length of the time series is 300 observations (\\(T=300\\)). In other words, we can state that \\(z_t \\sim \\mathsf{nid}(0,1), \\, t=1,\\ldots,300\\). The simulated time series exhibits completely random variation.\n\n\n\n\n\n\n\n\n\nFigure: A simulated realization of nid(0,1) process (\\(T=300\\)).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch3.html#r-lab",
    "href": "TSE-ch3.html#r-lab",
    "title": "4  Stationary processes",
    "section": "4.9 R Lab",
    "text": "4.9 R Lab\n\nR Lab, Section 2\n\n\n\n#  LOAD LIBRARIES\n# -------------------------------------------------------------------\npackages_to_load &lt;- c(\"tseries\", \"forecast\", \"ggplot2\", \"readxl\", \"dplyr\", \n                      \"lubridate\", \"astsa\", \"TSA\")\nfor (pkg in packages_to_load) {\n  if (!require(pkg, character.only = TRUE)) {\n    #install.packages(pkg) # uncomment this\n    library(pkg, character.only = TRUE)\n  }\n}\n\n#  DATA ACQUISITION AND PREPARATION\n# -------------------------------------------------------------------\nstart_date &lt;- \"1984-10-01\"\nend_date   &lt;- \"2007-04-01\"\n\nfull_data &lt;- read_excel(\"GDPC1-qdata.xlsx\")\n\nraw_data &lt;- full_data %&gt;%\n  rename(date = Date, value = GDPC1) %&gt;%\n  filter(date &gt;= as.Date(start_date) & date &lt;= as.Date(end_date))\n\ngdp_ts &lt;- ts(raw_data$value, \n             start = c(year(raw_data$date[1]), quarter(raw_data$date[1])), \n             frequency = 4)\n\ngdp_growth &lt;- diff(log(gdp_ts)) * 400  # Annualized growth rate\ngdp_growth &lt;- na.omit(gdp_growth)\n\n\n#  VISUALIZATION AND MANUAL IDENTIFICATION\n# -------------------------------------------------------------------\nautoplot(gdp_growth) + ggtitle(\"(Annualized) U.S. quarterly real GDP growth rate (1985:Q1-2007:Q2)\")\n\n\n\n\n\n\n\n# Simulate nid(0,1) process\n\nepsilon=rnorm(300); plot(epsilon, type=\"l\", xlab=\"Time\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch4.html",
    "href": "TSE-ch4.html",
    "title": "5  Linear process",
    "section": "",
    "text": "5.1 MA(1) process\nAs was mentioned, white noise processes can be used to define more complicated processes. A simple example and special case is the moving average process of order one, or MA(1) process, \\[\\begin{equation*}\ny_{t} = \\mu + u_{t} + \\theta_1 u_{t-1}, \\quad u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right).\n\\end{equation*}\\] The values of the process are hence assumed to be generated as a weighted average of two independent and unobserved random shocks.\nMA(1) processes are strictly stationary (see above and the property SS4) and also weakly stationary. Simple calculations show that \\[\\begin{eqnarray*}\n\\mathsf{E}\\left(y_{t}\\right) = \\mathsf{E}(\\mu) + \\mathsf{E}\\left(u_{t}\\right)+\\theta_1 \\mathsf{E}\\left(u_{t-1}\\right)=\\mu,\n\\end{eqnarray*}\\] \\[\\begin{eqnarray*}\n\\mathsf{Var}\\left(y_{t}\\right) &=& \\mathsf{E}\\Big(y_t- \\mathsf{E}(y_t)\\Big)^2 \\\\\n&=& \\mathsf{Var}\\left(z_{t}\\right) \\\\\n&=& \\mathsf{Var}\\left(u_{t}\\right)+\\theta_1^{2}\\mathsf{Var}\\left(u_{t-1}\\right) \\\\\n&=& \\sigma^{2}\\left(1+\\theta_1^{2}\\right),\n\\end{eqnarray*}\\] and, for \\(h&gt;0\\), \\[\\begin{eqnarray*}\n\\mathsf{Cov}\\left(y_{t},y_{t+h}\\right) &=& \\mathsf{Cov}\\left(z_{t},z_{t+h}\\right) \\\\\n&=& \\mathsf{E}[\\left(u_{t}+\\theta_1 u_{t-1}\\right)\\left(u_{t+h}+\\theta_1 u_{t+h-1}\\right)] \\\\\n&=& \\mathsf{E}\\left(u_{t}u_{t+h}\\right)+\\theta_1 \\mathsf{E}\\left(u_{t}u_{t+h-1}\\right)+\\theta_1 \\mathsf{E}\\left(u_{t-1}u_{t+h}\\right)+\\theta_1^{2}\\mathsf{E}\\left(u_{t-1}u_{t+h-1}\\right) \\\\\n&=& \\left\\{\n    \\begin{array}\n    [c]{l}\n        \\theta_1 \\sigma^{2},\\,\\, h=1, \\\\\n        0,\\,\\, h&gt;1.\n    \\end{array}\n\\right.\n\\end{eqnarray*}\\] The latter two calculations make use of the independence of process \\(u_{t}\\). These results show the weak stationarity of the MA(1) process.\nBased on these calculations, the autocorrelation function of an MA(1) process takes the form \\[\\begin{equation*}\n\\rho_{h}=\\left\\{\n\\begin{array}\n[c]{l}\n    1,\\,\\, h=0,\\\\\n    \\theta_1 \\Big/\\left(1+\\theta_1^{2}\\right), \\,\\,h=1,\\\\\n    0,\\,\\, h&gt;1.\n\\end{array}\n\\right.\n\\end{equation*}\\] Therefore, a typical feature of an MA(1) process is that the autocorrelation function drops to zero after lag one.\nThe following figure presents two simulated realizations of length 150 of an MA(1) process with \\(u_{t}\\sim\\mathsf{nid}\\left(0,1\\right)\\). Sample autocorrelation function based on the observations as well as the theoretical autocorrelation function are also shown.\nFigure: Two simulated realizations of the MA(1) process.\nIn Figure, on the left, the MA(1) coefficient \\(\\theta_1\\) is 0.9, on the right \\(\\theta_1= -0.9\\). In addition to simulated time series, the figure plots sample autocorrelations (“sample ACF”) and theoretical autocorrelations (“theoretical ACF”) of these two processes. In these figures \\(\\mathsf{r}_0=1\\) are depicted.\nThe figure above shows that in both simulated series, the observations vary around their mean (zero) according to their theoretical standard deviation (\\(\\approx1.345\\)).\nThe estimated sample autocorrelation functions resemble rather closely their theoretical counterparts. In particular, in both cases, the estimated \\(\\mathsf{r}_{1}\\) is well outside the approximate 95% confidence bands implied by the assumption \\(\\rho_{h}=0\\) \\(\\left(  \\forall h&gt;0\\right)\\). The remaining estimated sample autocorrelations fall mostly within these bands.\nAn obvious generalization of the MA(1) is obtained by adding a linear combination of the variables \\(u_{t-2},\\ldots,u_{t-q}\\) \\(\\left(q&lt;\\infty\\right)\\) to the right hand side of the MA(1). This leads to the so called MA(\\(q\\)) process and to be considered more detail in Section 5.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear process</span>"
    ]
  },
  {
    "objectID": "TSE-ch4.html#ma1-process",
    "href": "TSE-ch4.html#ma1-process",
    "title": "5  Linear process",
    "section": "",
    "text": "In this process (model equation), we also include in the mean of the process \\(\\mathsf{E}(y_t)=\\mu\\). Alternatively, we can write the MA(1) process as \\[\\begin{equation*}\ny_{t} - \\mu \\equiv z_t = u_{t}+\\theta_1 u_{t-1}, \\quad u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right),\n\\end{equation*}\\]\nNotice that possible other deterministic components than a nonzero mean can also be readily included in if necessary.\n\n\n\nThe same moment results are also obtained if the assumption \\(u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right)\\) is replaced with the milder assumption \\(u_{t}\\sim\\mathsf{wn}\\left(0,\\sigma^{2}\\right)\\), but in this case one cannot deduce the strict stationarity of \\(y_{t}\\). The same comment holds also to the more general results to be presented in the next section.\n\n\n\nThus, observations more than one period apart are uncorrelated and, when assumption \\(u_{t}\\sim \\mathsf{iid}\\left(0,\\sigma^{2}\\right)\\) holds, even independent.\nThe same conclusion could, of course, be immediately made from the MA(1) model equation above.\n\n\n\n\n\n\n\n\n\n\nIn the left panel, the series has positive autocorrelation, which manifests itself as positive observations, typically followed by another positive observation.\nIn the right panel, due to negative autocorrelation, positive and negative observations typically alternate.\n\n\n\nDue to random variation, some of the remaining 39 estimates may naturally occasionally fall outside these bands.\nFurthermore, as we will discuss later, in the case of an MA(1) process, the confidence bands used above are actually too narrow.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear process</span>"
    ]
  },
  {
    "objectID": "TSE-ch4.html#causal-linear-process",
    "href": "TSE-ch4.html#causal-linear-process",
    "title": "5  Linear process",
    "section": "5.2 Causal linear process",
    "text": "5.2 Causal linear process\nThe MA(1) process introduced in the previous section, and its generalization the MA(\\(q\\)) process (\\(q\\) \\(&lt;\\infty\\)) to be introduced later on, are special cases of the linear process \\[\\begin{equation*}\ny_{t} = \\mu + \\sum_{j=-\\infty}^{\\infty}\\psi_{j}u_{t-j}, \\quad  u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right).\n\\end{equation*}\\] It is clear that the infinite sum on the right hand side of this equation requires further care and can not be well defined without suitable further restrictions on the coefficients \\(\\psi_{j}\\).\n\n\nWe will assume that \\[\\begin{equation*}\n\\sum_{j=-\\infty}^{\\infty}\\psi_{j}^{2}&lt;\\infty.\n\\end{equation*}\\] Under this assumption the infinite sum on the right hand side of the linear process is well defined.\n\n\n\n \nThe mean and autocovariance function of \\(y_{t}\\) (general linear process) can be calculated in a similar fashion as for the MA(1) process regardless of the infinite sum: \\[\\begin{equation*}\n\\mathsf{E}\\left(y_{t}\\right) = \\mu + \\sum_{j=-\\infty}^{\\infty}\\mathsf{E}\\left(\\psi_{j}u_{t-j}\\right)=\\sum_{j=-\\infty}^{\\infty}\\psi_{j}\\mathsf{E}\\left(u_{t-j}\\right)=\\mu,\n\\end{equation*}\\] and denoting (cf. deterministic components section) \\(z_t \\equiv y_t - \\mu\\), we get \\[\\begin{equation*}\n\\mathsf{Var}\\left(y_{t}\\right)= \\mathsf{Var}\\left(z_{t}\\right) =\\sum_{j=-\\infty}^{\\infty}\\mathsf{Var}\\left(\\psi_{j}u_{t-j}\\right)=\\sum_{j=-\\infty}^{\\infty}\\psi_{j}^{2}\\mathsf{Var}\\left(u_{t-j}\\right)=\\sigma^{2}\\sum_{j=-\\infty}^{\\infty}\\psi_{j}^{2}.\n\\end{equation*}\\] Moreover, for \\(h&gt;0\\), \\[\\begin{eqnarray}\n\\mathsf{Cov}\\left(y_{t},y_{t+h}\\right) &=& \\mathsf{Cov}\\left(z_{t},z_{t+h}\\right) \\\\ &=& \\mathsf{E}\\left(\\sum_{j=-\\infty}^{\\infty}\\psi_{j}u_{t-j}\\sum_{i=-\\infty}^{\\infty}\\psi_{i}u_{t+h-i}\\right) \\\\\n&=& \\sum_{j=-\\infty}^{\\infty}\\sum_{i=-\\infty}^{\\infty}\\mathsf{E}\\left(\\psi_{j}\\psi_{i}u_{t-j}u_{t+h-i}\\right) \\\\\n&=& \\sum_{j=-\\infty}^{\\infty}\\sum_{i=-\\infty}^{\\infty}\\psi_{j}\\psi_{i}\\mathsf{E}\\left(  u_{t-j}u_{t+h-i}\\right) \\\\\n&=& \\sigma^{2}\\sum_{j=-\\infty}^{\\infty}\\psi_{j}\\psi_{j+h},\n\\end{eqnarray}\\] where the calculations also make use of the properties of the process \\(u_{t}\\) (compare the results for the MA(1) process). These calculations show that \\(y_{t}\\) is weakly stationary.\n\n\nNotice that the strict stationarity follows from the strict stationarity of \\(u_{t}\\) and the property SS4.\n\n \nCausal linear (MA(\\(\\infty\\))) process. Because the linear process defined above contains an infinite number of unknown parameters (the \\(\\psi_{j}\\)’s), it cannot be used in practice to obtain a useful statistical model unless we place some further restrictions on \\(\\psi_{j}\\).\n\nDespite this, the general linear model is a useful theoretical device because many processes used in practice are special cases of it.\n\nLike in the case of an MA(1) process, it typically holds that \\(\\psi_{j}=0\\) for \\(j&lt;0\\), in which case the general linear process reduces to \\[\\begin{equation*}\ny_{t} = \\mu + \\sum_{j=0}^{\\infty}\\psi_{j}u_{t-j}, \\quad u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right).\n\\end{equation*}\\] Because \\(y_{t}\\) no longer depends on future values of the \\(u_{t}\\) variables, one often speaks of a causal linear process or a causal MA(\\(\\infty\\)) process.\n\nThe values of the process \\(y_{t}\\) are assumed to be generated as a weighted sum of (possibly) infinitely many independent and unobserved random shocks.\n\n\n\nIn the noncausal case the future shocks \\(u_{t+j}\\) \\(\\left(j&gt;0\\right)\\) affect the present value of the process \\(y_{t}\\). In this course, we do not consider noncausal models more detail.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear process</span>"
    ]
  },
  {
    "objectID": "TSE-ch4.html#ar1-process",
    "href": "TSE-ch4.html#ar1-process",
    "title": "5  Linear process",
    "section": "5.3 AR(1) process",
    "text": "5.3 AR(1) process\nA simple special case of a causal linear process containing only one unknown parameter (and constant term) is achieved by assuming that \\(\\psi_{j}=\\phi_1^{j}\\). A process defined like this leads to the autoregressive process of order one, that is an AR(1) process \\[\\begin{equation*}\ny_{t} = \\nu + \\phi_1 y_{t-1}+u_{t}, \\quad u_{t}\\sim \\mathsf{iid}\\left(0,\\sigma^{2}\\right).\n\\end{equation*}\\] Here one interprets the present value of the process to linearly depend on the previous value of the process as well as on an unobseved random shock (or error term) similarly as in the linear process. Furthermore, \\(\\nu\\) denotes the constant term of the process, whose connection to the mean \\(\\mathsf{E}(y_t)=\\mu\\) will be examined below.\n\nReferring to the linear causal process, for the condition \\(\\sum_{j=-\\infty}^{\\infty}\\psi_{j}^{2}&lt;\\infty\\) to be satisfied, it needs to assumed that \\(\\left\\vert \\phi_1\\right\\vert&lt;1\\).\n\n\nTaking the AR(1) model equation as a starting point, the necessity of condition \\(\\left\\vert \\phi_1\\right\\vert &lt;1\\) can be demonstrated by making use of repetitive substitutions.\n\nFirst, substitute \\(y_{t-1} = \\nu + \\phi_1 y_{t-2}+u_{t-1}\\) on the right hand side of the AR(1) equation.\nThen substitute \\(y_{t-2} = \\nu + \\phi_1 y_{t-3}+u_{t-2}\\) to the resulting expression, and so on.\n\nContinuing in this fashion, we obtain the equation \\[\\begin{equation*}\n    y_{t}=\\phi_1^{k}y_{t-k} + \\nu \\sum_{j=0}^{k-1}\\phi_1^{j} + \\sum_{j=0}^{k-1}\\phi_1^{j}u_{t-j}.\n\\end{equation*}\\] When \\(\\left\\vert \\phi_1\\right\\vert&lt;1\\), this leads us to the limiting solution \\[\\begin{equation*}\ny_{t} = \\nu \\sum_{j=0}^{\\infty}\\phi_1^{j} + \\sum_{j=0}^{\\infty}\\phi_1^{j}u_{t-j}.\n\\end{equation*}\\] Because the AR(1) process (with \\(|\\phi_1| &lt; 1\\)) is clearly a special case of the linear process, it is strictly and weakly stationary when \\(|\\phi_1| &lt; 1\\).\nThe first and second moments can be deduced from the general formulae derived in the previous section. The expected value, variance and autocovariance functions take the form \\[\\begin{equation*}\n\\mathsf{E}\\left(y_{t}\\right) \\equiv \\mu = \\nu \\sum_{j=0}^{\\infty}\\phi_1^{j} + \\mathsf{E}\\Big(\\sum_{j=0}^{\\infty}\\phi_1^{j}u_{t-j}\\Big) =  \\nu \\sum_{j=0}^{\\infty}\\phi_1^{j} = \\nu / (1-\\phi_1),\n\\end{equation*}\\] \\[\\begin{equation*}\n\\mathsf{Var}\\left(y_{t}\\right)=\\sigma^{2}\\sum_{j=0}^{\\infty}\\phi_1^{2j}=\\sigma^{2}/\\left(1-\\phi_1^{2}\\right),\n\\end{equation*}\\] and, for \\(h&gt;0\\), \\[\\begin{eqnarray*}\n   \\gamma_h &=& \\mathsf{Cov}\\left(y_{t},y_{t+h}\\right) \\\\\n   &=& \\sigma^{2}\\sum_{j=0}^{\\infty}\\phi_1^{j}\\phi_1^{j+h} \\\\\n   &=& \\phi_1 \\gamma _{h-1} \\\\\n   &=& \\sigma^{2}\\phi_1^{h}/\\left(1-\\phi_1^{2}\\right).\n\\end{eqnarray*}\\]\nThe autocorrelation function of the AR(1) process hence becomes \\[\\begin{equation*}\n\\rho_{h}=\\left\\{\n\\begin{array}\n[c]{l}\n1, \\, h=0, \\\\\n\\phi_1^{h}, \\, h&gt;0.\n\\end{array}\n\\right.\n\\end{equation*}\\] Unlike in the case of an MA(1) process, the autocorrelation function differs from zero for all lags (unless \\(\\phi_1=0\\)). Note, however, that condition \\(\\gamma_{h}\\rightarrow0, \\,\\, \\mathrm{when} \\,\\, h\\rightarrow\\infty\\) is satisfied.\n\nLater, we will study a generalization of the AR(1) process, called an AR(\\(p\\)) process, which is obtained by adding a linear combination of the variables \\(y_{t-2},\\ldots,y_{t-p}, \\left(p&lt;\\infty\\right)\\) on the right hand side of the AR(1) process.\n\n \nIn the AR(1) process, the autoregressive parameter \\(\\phi_1\\) clearly measures the persistence of a random shock to the time series.\n\nIf \\(\\phi_1\\) is close to unity in absolute value, autocorrelation is high and the series (process) is strongly “persistent”.\nIf \\(\\phi_1\\) is close to zero, there is no persistence and the effect of the shock is temporary.\n\nThe sign of \\(\\phi_1\\) determines whether the time series is positively or negatively autocorrelated.\n\nIf \\(\\phi_1\\) is positive, then the positive values of \\(y_t\\) are tending to follow positive values, and similarly with negative values.\nIf \\(\\phi_1\\) is negative, then positive values tend to follow negative values, and vice versa.\n\n\n\n\n\n\n\n\n\n\nFigure: Two simulated realizations of the AR(1) process.\n \nFigure presents two simulated realizations of length 150 of an AR(1) process with \\(u_{t}\\sim\\mathsf{nid}\\left(0,1\\right)\\). On the left, the AR coefficient \\(\\phi_1\\) is 0.7, on the right -0.7. For simplicity, the constant term (and hence the mean) is set to 0. In addition to simulated time series, the figure plots sample autocorrelations and theoretical autocorrelations of these two processes.\n\nAs they should be stationary series, the time series vary around their mean (zero) according to their theoretical standard deviation (\\(\\approx1.4\\)).\nWhen \\(\\phi_1 = 0.7\\) (left), the observations have relatively strong positive autocorrelation and, as a consequence, several consecutive observations occur above the mean, as well as below the mean. This gives the time series a “smooth” flavour.\nWhen \\(\\phi_1 = -0.7\\) (right), the sign of autocorrelation between consecutive observations changes depending on the distance between them. These changes from positive to negative of the autocorrelation coefficients give the observed time series a jagged/zigzag pattern. Moreover, clusters of consecutive observations with small absolute values are also observed (the same for large absolute values).\nIn both cases, the estimated sample autocorrelation functions are rather close to their theoretical counterparts. Moreover, several estimated autocorrelation coefficients are outside of the 95% confidence bands based on the assumption of \\(\\rho_{h}=0\\) \\(\\left(\\forall h&gt;0\\right)\\).\n\n \n\nExtra: Noncausal AR process\n\n\nThe definition of an AR(1) process is usually based on its model equation as described above. In connection with this, the condition \\(\\left\\vert \\phi_1\\right\\vert&lt;1\\) is often called the stationarity condition of an AR(1) process. This terminology is somewhat misleading because AR(1) equation has a stationary solution also in the case \\(\\left\\vert \\phi_1\\right\\vert &gt;1\\), although this solution cannot be represented in the form of linear process. When \\(\\left\\vert \\phi_1\\right\\vert&gt;1\\), AR(1) equation can be rewritten as \\[\\begin{equation*}\n    y_{t}=\\phi_1^{-1}y_{t+1}-\\phi_1^{-1}u_{t+1}\n\\end{equation*}\\] by increasing the time index \\(t\\) by one step. Using repetitive substitutions forward in time in a fashion similar as above, this leads to \\[\\begin{equation*}\n    y_{t}=\\phi_1^{-k-1}y_{t+1+k}-\\sum_{j=0}^{k}\\phi_1^{-j-1}u_{t+1+j}.\n\\end{equation*}\\] This leads to the limiting solution \\[\\begin{equation*}\n    y_{t}=-\\sum_{j=1}^{\\infty}\\phi_1^{-j}u_{t+j}.\n\\end{equation*}\\] Note that we can arrive to the (more or less) same (except for an unimportant minus sign) solution by starting from the linear process and by assuming \\(\\psi_{j}=\\phi_1^{j}\\), when \\(j&lt;0,\\) and \\(\\psi_{j}=0,\\) when \\(j\\geq0\\) \\(\\left(\\left\\vert \\phi_1\\right\\vert &gt;1\\right)\\). This solution of the AR(1) equation is called noncausal. This kind of noncausal AR processes have received some attention in the recent literature. However, in this course we restrict our attention to causal AR processes (as do most textbooks).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear process</span>"
    ]
  },
  {
    "objectID": "TSE-ch4.html#random-walk",
    "href": "TSE-ch4.html#random-walk",
    "title": "5  Linear process",
    "section": "5.4 Random walk",
    "text": "5.4 Random walk\nFor the AR(1) model, and for any initial value \\(y_{0}\\), we obtain representation (after recursive substitutions) \\[\\begin{equation*}\ny_{t}=\\phi_1^{t}y_{0} + \\nu \\sum_{j=0}^{t-1}\\phi_1^{j} + \\sum_{j=0}^{t-1}\\phi_1^{j}u_{t-j}, \\quad t=1,2,\\ldots\\text{ }.\n\\end{equation*}\\] If we assume the initial value \\(y_{0}\\) to be independent of the variables \\(u_{t}\\), \\(t\\geq1\\), one can use the assumption \\(u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right)\\) to deduce \\[\\begin{equation*}\n\\mathsf{E}\\left(y_{t}\\right)=\\phi_1^{t}\\mathsf{E}\\left(y_{0}\\right) + \\nu \\sum_{j=0}^{t-1}\\phi_1^{j}\n\\end{equation*}\\] and \\[\\begin{eqnarray*}\n\\mathsf{Var}\\left(y_{t}\\right) &=& \\mathsf{E} \\Big(y_t- \\mathsf{E}(y_t) \\Big)^2 \\\\\n&=& \\mathsf{Var}\\left(\\phi_1^{t}y_{0}\\right)+\\mathsf{Var}\\left(\\sum_{j=0}^{t-1}\\phi_1^{j}u_{t-j}\\right) \\\\\n&=& \\phi_1^{2t}\\mathsf{Var}\\left(y_{0}\\right)+\\sigma^{2}\\sum_{j=0}^{t-1}\\phi_1^{2j}.\n\\end{eqnarray*}\\] When \\(\\left\\vert \\phi_1\\right\\vert =1\\), the expected value of \\(y_{t}\\), or at least its variance, clearly depends on \\(t\\) regardless of how \\(y_{0}\\) (or its distribution) is chosen.\n\nIn this case, the AR(1) process therefore has no stationary solution.\n\n\nWhen \\(\\phi=1\\) and \\(t\\geq1\\), the AR(1) process reduces to \\[\\begin{equation*}\ny_{t} = \\nu + y_{t-1}+u_{t},\\,\\, u_{t}\\sim \\mathsf{iid}\\left(0,\\sigma^{2}\\right).\n\\end{equation*}\\] This is called a random walk. This name is due to the “wandering” nature of the realizations of the process.\n\nThe left panel of figure below illustrates this. For simplicity, we assume here \\(\\nu=0\\) (that is the random walk without drift) and \\(u_t \\thicksim \\mathsf{nid}(0,1)\\).\nThe right panel illustrates the obvious fact that the differences \\(y_{t}-y_{t-1}=u_{t}\\) of a random walk \\(y_{t}=y_{0}+\\sum_{j=0}^{t-1}u_{t-j}\\) are stationary.\n\n\n\n\n\n\n\n\n\n\nFigure: A simulated realizations of the random walk process (assuming \\(\\mathsf{nid}(0,1)\\)).\n \nThe random walk and its generalizations play a central role in the analysis of nonstationary time series. We will come back to this in Sections 12–13.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear process</span>"
    ]
  },
  {
    "objectID": "TSE-ch4.html#arma11-process",
    "href": "TSE-ch4.html#arma11-process",
    "title": "5  Linear process",
    "section": "5.5 ARMA(1,1) process",
    "text": "5.5 ARMA(1,1) process\nA concept easing the algebraic manipulations of time series processes is the so-called backshift operator or lag operator. For any process (or simply a sequence of numbers) \\(x_{t}\\), define the operation with the equation \\(Bx_{t}=x_{t-1}\\). More generally, \\(B^{2}x_{t}=B\\left(Bx_{t}\\right)=Bx_{t-1}=x_{t-2}\\), and inductively define \\[\\begin{equation*}\nB^{k}x_{t}=B\\left(B^{k-1}x_{t}\\right)=x_{t-k}, \\,\\, B^{0}x_{t}=x_{t}.\n\\end{equation*}\\]\n\nHere \\(k\\) can also be negative, and when \\(k&lt;0\\), the operator becomes a “forward shift” operator, for example \\(B^{-1}x_{t}=x_{t+1}\\), \\(B^{-2}x_{t}=x_{t+2}\\) etc.\nAt times backshift or lag operator is denoted by \\(L^k\\) instead of \\(B^k\\).\n\nUsing the lag operator, one can also define polynomials.\n\nFor example, \\(\\theta\\left(B\\right)=1+\\theta_1 B\\) and, e.g., \\(\\psi\\left(B\\right)=\\sum_{j=-\\infty}^{\\infty}\\psi_{j}B^{j}\\)).\n\nOne can algebraically operate with the lag operator exactly as if \\(B\\) were a real or a complex number.\n\nFor instance, the MA(1) process can be written as \\[\\begin{equation*}\ny_{t}= \\mu + u_{t}+\\theta_1 u_{t-1} = \\theta\\left(B\\right)u_{t}.\n\\end{equation*}\\]\nWhen differencing (some) process \\(y_{t}\\) twice, we obtain \\[\\begin{eqnarray*}\n\\left(1-B\\right)^{2}y_{t} &=& \\left(1-B\\right)\\left[\\left(1-B\\right)  y_{t}\\right] \\\\\n&=&\\left(  1-B\\right)  \\left(  y_{t}-y_{t-1}\\right) \\\\\n&=& y_{t}-y_{t-1}-y_{t-1}+y_{t-2} \\\\\n&=& \\left(1-2B+B^{2}\\right)y_{t}.\n\\end{eqnarray*}\\]\n\nUsing the lag operator, the general linear process can be defined by the equation \\[\\begin{equation*}\ny_{t} - \\mu = \\psi\\left(B\\right)u_{t}, \\quad u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right),\n\\end{equation*}\\] where \\(\\psi\\left(B\\right)=\\sum_{j=-\\infty}^{\\infty}\\psi_{j}B^{j}\\). The operator \\(\\psi\\left(B\\right)\\) is sometimes thought as a linear filter, which transforms the white noise sequence \\(\\left\\{  u_{t}\\right\\}\\) to the process \\(\\left\\{y_{t}\\right\\}\\).\n\nIn this context, and especially in the causal case \\(\\psi_{j}=0\\), \\(j&lt;0\\), the white noise \\(u_{t}\\) is often called the innovation sequence of the process \\(y_{t}\\).\n\n \nIn what follows, we will often consider the special case of the (causal) linear process in which the filter \\(\\psi\\left(B\\right)\\) is rational, that is, \\[\\begin{equation*}\n\\psi\\left(B\\right)=\\sum_{j=0}^{\\infty}\\psi_{j}B^{j}=\\theta\\left(B\\right)\\phi\\left(B\\right)^{-1},\n\\end{equation*}\\] where \\(\\phi\\left(B\\right)\\) and \\(\\theta\\left(B\\right)\\) are polynomials of finite order. In the simplest case, these polynomials are of order one so that \\[\\begin{equation*}\n\\phi\\left(B\\right)=1-\\phi_1 B \\quad \\mathrm{and} \\quad \\theta\\left(B\\right)=1+\\theta_1 B.\n\\end{equation*}\\]\n\nIt is clear that to obtain stationarity, some restrictions have to be placed on the coefficients of the polynomial \\(\\phi\\left(B\\right)\\).\n\n \nBased on our discussion on the AR(1) process above, it is clear that in the first-order case a sufficient condition is that \\(\\left\\vert \\phi_1\\right\\vert &lt;1\\). Then the condition \\(\\sum_{j=-\\infty}^{\\infty}\\psi_{j}^{2}&lt;\\infty\\) attached to the linear process is satisfied and the process \\[\\begin{equation*}\ny_{t}-\\mu =\\left[\\theta\\left(B\\right)\\phi\\left(B\\right)^{-1}\\right] u_{t}\n\\end{equation*}\\] is well defined. Multiplying both sides of this equation with the polynomial \\(\\phi\\left(B\\right)\\), we obtain the representation \\[\\begin{equation*}\n\\phi\\left(B\\right) (y_{t}-\\mu) = \\theta\\left(B\\right)u_{t}\n\\end{equation*}\\] or \\[\\begin{equation*}\ny_{t} = \\nu + \\phi_1 y_{t-1}+u_{t}+\\theta_1 u_{t-1}, \\quad u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right),\n\\end{equation*}\\] where \\(\\nu = \\phi\\left(1\\right) = 1- \\phi_1\\) (see the properties of the backshift operator). A process defined like this is called the autoregressive moving average process of order one, or the ARMA(1,1) process.\n\nThis combines the AR(1) and MA(1) processes introduced earlier, and these processes can still be obtained as special cases.\nLater we will study a generalization of this process called the ARMA(\\(p,q\\)) process, which can be obtained by generalizing ARMA(1,1) in a similar fashion as discussed around AR(1) and MA(1) processes.\nThe coefficients \\(\\psi_j\\) of the filter \\(\\psi\\left(B\\right)=\\sum_{j=0}^{\\infty}\\psi_{j}B^{j}\\) can be solved fairly straightforwardly from equation \\(\\psi\\left(B\\right)=\\theta\\left(B\\right)\\phi\\left(B\\right)^{-1}\\).\n\nThe above shows that the autocorrelation function of an ARMA(1,1) process can then be derived by applying the general formulas obtained for the general linear process. Details of these calculations are left as exercises.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear process</span>"
    ]
  },
  {
    "objectID": "TSE-ch4.html#wold-decomposition",
    "href": "TSE-ch4.html#wold-decomposition",
    "title": "5  Linear process",
    "section": "5.6 Wold decomposition",
    "text": "5.6 Wold decomposition\nThe following famous result (named after Herman Wold) shows that every weakly stationary non-deterministic process can be expressed as a sum of a deterministic process and a causal MA\\(\\left(\\infty\\right)\\) process (the proof of this result is beyond the scope of this course and omitted). In other words, every weakly stationary non-deterministic process \\(y_{t}\\) (\\(t=0,\\pm1,\\pm2,\\ldots\\)) has a representation \\[\\begin{equation*}\ny_{t} = \\sum_{j=0}^{\\infty}\\psi_{j}u_{t-j}+\\upsilon_{t},\n\\end{equation*}\\] where\n\n\\(\\mathrm{(i)}\\) \\(\\psi_{0}=1\\), \\(\\sum_{j=0}^{\\infty}\\psi_{j}^{2}&lt;\\infty\\),\n\\(\\mathrm{(ii)}\\) \\(u_{t}\\sim\\mathsf{wn}\\left(0,\\sigma^{2}\\right)\\), \n\\(\\mathrm{(iii)}\\) \\(\\upsilon_{t}\\) is deterministic, and\n\\(\\mathrm{(iv)}\\) \\(\\mathsf{Cov}\\left(  u_{t},\\upsilon_{s}\\right)=0\\) for all \\(t\\) and \\(s\\). \\(\\square\\)\n\nPart (iii) means that the process \\(\\upsilon_{t}\\) can be predicted linearly using the variables \\(y_{t-1}, y_{t-2},\\ldots\\), with no error. This and part (ii) together imply that \\(u_{t}\\) can be interpreted as a forecast error when forecasting \\(y_{t}\\) linearly using the lags of \\(y_t\\).\n\nIn the case \\(\\upsilon_{t}=0\\) the process \\(y_{t}\\) is called purely non-deterministic.\n\nWhen, as in practice, only one realization of the process \\(y_{t}\\) is observed, the process \\(\\upsilon_{t}\\) can therefore be treated as a non-random function of time (it can be perfectly predicted using the realized values of \\(y_{t-1},y_{t-2},\\ldots\\), so only remaining variation should be non-random). Modelling \\(\\upsilon_{t}\\) can thus be thought as modelling a trend as discussed in the introduction.\n\nA simple example of a process \\(\\upsilon_{t}\\) is that it is constant. Such a realization of \\(\\upsilon_{t}\\) can in practice be interpreted as the expected value of process \\(y_{t}\\), \\(\\mu\\), or at least be included in it.\n\nThe task of modelling a weakly stationary process reduces to the task of modelling the linear filter \\(\\psi\\left(B\\right)=\\sum_{j=0}^{\\infty}\\psi_{j}B^{j}\\).\n\nFurthermore, in the case of the ARMA(\\(p,q\\)) processes to be investigated shortly, this means assuming that the filter \\(\\psi\\left(B\\right)\\) is rational, or in other words that \\(\\psi\\left(B\\right)=\\theta\\left(B\\right)\\phi\\left(B\\right)^{-1}\\), where \\(\\phi\\left(B\\right)\\) is a polynomial of order \\(p\\) and \\(\\theta\\left(B\\right)\\) is a polynomial of order \\(q\\).\n\n\nAs a remark, we note that the significance of the Wold decomposition should be evaluated keeping in mind that it concerns weakly stationary processes and linear forecasting. Although every weakly stationary process can be represented by the Wold decomposition, this does not mean that the decomposition is the best way to describe the process. There exist (strictly) stationary processes for which linear prediction is not optimal (in the sense of minimising mean-square forecast error).\n\n \n\nExtra: Deterministic and non-deterministic processes/parts\n\n\nConsider the weakly stationary process \\[\\begin{equation*}\ny_{t}=A\\cos\\left(\\lambda t\\right)+B\\sin\\left(\\lambda t\\right), \\qquad t=0,\\pm1,\\pm2,\\ldots,\n\\end{equation*}\\] where \\(\\lambda\\in\\lbrack0,\\pi)\\) is a constant and the random variables \\(A\\) and \\(B\\) satisfy the conditions \\(\\mathsf{E}\\left(A\\right)=\\mathsf{E}\\left(B\\right)=0\\), \\(\\mathsf{Var}\\left(A\\right)=\\mathsf{Var}\\left(B\\right)=\\sigma^{2}\\) and \\(\\mathsf{Cov}\\left(A,B\\right)=0\\). Because \\[\\begin{equation*}\ny_{t}+y_{t-2}=A\\left[\\cos\\left(\\lambda t\\right)+\\cos\\left(\\lambda\\left(  t-2\\right)\\right)\\right]+B\\left[\\sin\\left(\\lambda t\\right)+\\sin\\left(  \\lambda\\left(t-2\\right)\\right)\\right],\n\\end{equation*}\\] we can use the trigonometric identities \\[\\begin{equation*}\n\\sin\\left(x_{1}\\right)+\\sin\\left(x_{2}\\right)=\\sin\\left(\\left(x_{1}+x_{2}\\right)2\\right)\\cos\\left(\\left(x_{1}-x_{2}\\right)2\\right)\n\\end{equation*}\\] and \\[\\begin{equation*}\n\\cos\\left(x_{1}\\right)+\\cos\\left(x_{2}\\right)=2\\cos\\left(\\left(x_{1}+x_{2}\\right)2\\right)\\cos\\left(\\left(x_{1}-x_{2}\\right)2\\right)\n\\end{equation*}\\] to derive the result \\(y_{t}+y_{t-2}=2\\cos\\left(\\lambda\\right)y_{t-1}\\) or, in other words, \\[\\begin{equation*}\ny_{t}=2\\cos\\left(\\lambda\\right)y_{t-1}-y_{t-2}, \\quad t=0,\\pm1,\\pm2,\\ldots.\n\\end{equation*}\\] The process \\(y_{t}\\) is somewhat peculiar in that when \\(y_{t-1}\\) and \\(y_{t-2}\\) (and the value of the constant \\(\\lambda\\)) are known, the value for the current period \\(y_{t}\\) can be predicted using a simple linear formula with perfect precision without any forecast error. A process with such a property is called a deterministic process. More generally, the forecast of \\(y_{t}\\) is allowed to be any linear function of the past values of the process \\(y_{t-1},y_{t-2},\\ldots\\) so that the forecast is a linear combination of the variables \\(y_{t-1},\\ldots,y_{t-n}\\) or a mean square limit of such linear combinations as \\(n\\rightarrow\\infty\\). If a process is not deterministic, then it is called non-deterministic.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear process</span>"
    ]
  },
  {
    "objectID": "TSE-ch4.html#meanautocorprop",
    "href": "TSE-ch4.html#meanautocorprop",
    "title": "5  Linear process",
    "section": "5.7 Properties of sample mean and autocorrelations",
    "text": "5.7 Properties of sample mean and autocorrelations\nBecause the sample mean and the sample autocorrelation function are central tools in the analysis of time series, we next briefly discuss some of the statistical properties.\nWe can show that (see below) that the sample mean \\[\\begin{equation*}\n\\bar{y}=\\frac{1}{T}\\left(y_{1}+\\cdots+y_{T}\\right)\n\\end{equation*}\\] is\n\nunbiased and consistent estimator of the population mean \\(\\mu=\\mathsf{E}\\left(y_{t}\\right)\\), and\nasymptotically normally distributed\n\n \n\nExtra: Proof of unbiasedness and consistency of the sample mean\n\n\nIn the following discussion, we assume both weak and strict stationarity of the process.\nFor the sample mean \\(\\bar{y}=T^{-1}\\left(y_{1}+\\cdots+y_{T}\\right)\\), it holds that \\[\\begin{equation*}\n\\mathsf{E}\\left(\\bar{y}\\right)=\\frac{1}{T}\\left(\\mathsf{E}\\left(y_{1}\\right)+\\cdots+\\mathsf{E}\\left(y_{T}\\right)\\right)=\\mu\n\\end{equation*}\\] so that it is an unbiased estimator of the population mean \\(\\mu=\\mathsf{E}\\left(y_{t}\\right)\\) (recall that in general, an estimator \\(\\widehat{\\theta}\\) of a population parameter \\(\\theta\\) is called unbiased if \\(\\mathsf{E}(\\widehat{\\theta})=\\theta\\)).\nFor the variance of \\(\\bar{y}\\), we can construct \\[\\begin{eqnarray*}\n\\mathsf{Var}\\left(\\bar{y}\\right) &=& \\frac{1}{T^{2}}\\sum_{t=1}^{T}\\sum_{s=1}^{T}\\mathsf{Cov}\\left(y_{t},y_{s}\\right) \\\\\n&=& \\frac{1}{T^{2}}\\sum_{t-s=-T}^{T}\\left(T-\\left\\vert t-s\\right\\vert\\right)\\gamma_{t-s}\\\\\n&=& \\frac{1}{T}\\sum_{h=-T}^{T}\\left(1-\\frac{\\left\\vert h\\right\\vert}{T}\\right)\\gamma_{h}.\n\\end{eqnarray*}\\] The second equality above can be justified by noting that the preceding double sum is the sum of the elements of the matrix \\(\\left[\\gamma_{t-s}\\right]_{t,s=1,..,T}\\). Now assume that\n\\[\\begin{equation*}\n\\sum_{h=-\\infty}^{\\infty}\\left\\vert\\gamma_{h}\\right\\vert&lt;\\infty,\n\\end{equation*}\\] a requirement more stringent than condition \\(\\gamma_{h}\\rightarrow0, \\,\\, \\mathrm{when} \\,\\, h\\rightarrow\\infty\\). This condition is satisfied by many processes used in practice (for instance, by the AR(1) and MA(1) processes). Making use of the triangle inequality, we now obtain the result \\[\\begin{equation*}\n\\mathsf{Var}\\left(\\bar{y}\\right)=\\mathsf{E}\\left(\\bar{y}-\\mu\\right)^{2}\\leq\\frac{1}{T}\\sum_{h=-T}^{T}\\left(1-\\frac{\\left\\vert h\\right\\vert}{T}\\right)\\left\\vert\\gamma_{h}\\right\\vert\\rightarrow0, \\quad \\mathrm{as} \\quad T\\rightarrow\\infty.\n\\end{equation*}\\] In other words, the sample mean is a consistent estimator for the expected value.\nIf we strengthen the assumptions made above and additionally assume that the process \\(y_{t}\\) is Gaussian, the sample mean is also normally distributed with the mean and variance as indicated above. Furthermore, the following asymptotic result can be established (we omit the details) \\[\\begin{equation*}\n\\sqrt{T}\\left(  \\bar{y}-\\mu\\right)  \\overset{d}{\\rightarrow}\\mathsf{N}\\left(0,\\sum\\nolimits_{h=-\\infty}^{\\infty}\\gamma_{h}\\right)\n\\quad \\mathrm{or} \\quad\n\\bar{y}\\underset{as}{\\sim}\\mathsf{N}\\left(  \\mu,\\frac{1}{T}\\sum\\nolimits_{h=-\\infty}^{\\infty}\\gamma_{h}\\right).\n\\end{equation*}\\] This result can be derived without assuming \\(y_t\\) to be Gaussian.\nThe purpose here is to show the law of large numbers and the central limit theorem to apply to stationary processes with “reasonable assumptions”. To use this result to construct tests and confidence intervals for \\(\\mu\\), we also need to estimate the infinite sum \\(\\sum\\nolimits_{h=-\\infty}^{\\infty}\\gamma_{h}\\). A suitable estimator is (compare the expression of \\(\\mathsf{Var}\\left(\\bar{y}\\right)\\) above) \\(\\sum_{h=-K}^{K}\\left(1-\\left\\vert h\\right\\vert /T\\right)\\mathsf{c}_{h}\\), where \\(\\mathsf{c}_{h}\\) is the sample autocovariance coefficient defined earlier, and \\(K\\) is a “suitably” chosen number smaller than \\(T\\) (for example, \\(K\\approx\\sqrt{T}\\)).For large values of \\(h\\), the estimator \\(\\textsf{c}_{h}\\) becomes unprecise, and for this reason \\(K\\) should not be too large compared to \\(T\\).\n\n \nThe statistical properties of the sample autocorrelation coefficients \\(\\mathsf{r}_{h}=\\mathsf{c}_{h}/\\mathsf{c}_{0}\\) are more complicated to derive than those of the sample mean, so we will not attempt to provide any detailed justifications. Under “reasonably general assumptions”, consistency and asymptotic normality of them can be established.\n\nImportant special case: For the important special case of \\(y_{t}\\sim\\mathsf{iid}\\left(\\mu,\\sigma^{2}\\right)\\), it holds that \\[\\begin{equation*}\n\\left(\\mathsf{r}_{1},\\ldots,\\mathsf{r}_{H}\\right)\\underset{as}{\\sim}\\mathsf{N}\\left(\\boldsymbol{0},T^{-1}\\boldsymbol{I}_{H}\\right),\n\\end{equation*}\\] with a \\(H\\)-dimensional zero mean vector and \\(\\boldsymbol{I}_{H}\\) \\(\\left(H\\times H\\right)\\) denotes an identity matrix. This result can be used to test whether it is realistic to consider an observed time series as an uncorrelated time series process. Under the hypothesis to be tested (uncorrelatedness), the estimators \\(\\mathsf{r}_{1},\\ldots.,\\mathsf{r}_{H}\\) are approximately independent with distribution \\(\\mathsf{N}\\left(0,T^{-1}\\right)\\). Based on this, we get \\[\\begin{equation*}\n\\mathsf{P}(\\left\\vert \\mathsf{r}_{h}\\right\\vert \\geq1.96/\\sqrt{T})\\approx 0.05,\n\\end{equation*}\\] a result that can be used to evaluate the significance of individual sample autocorrelation coefficients.\nTo obtain a joint test for several autocorrelation coefficients, one can use the test statistic \\[\\begin{equation*}\nQ=T\\sum_{h=1}^{H}\\mathsf{r}_{h}^{2}\\underset{as}{\\sim}\\chi_{H}^{2},\n\\end{equation*}\\] whose large values would lead to rejection. Note that the asymptotic \\(\\chi_{H}^{2}\\)–distribution follows from the distribution result above for \\(\\left(\\mathsf{r}_{1},\\ldots,\\mathsf{r}_{H}\\right)\\) and the definition of the chi-squared distribution. In practice, an alternative and slightly different test statistic \\[\\begin{equation*}\nQ_{{\\tiny LB}}=T\\left(T+2\\right)\\sum_{h=1}^{H}\\mathsf{r}_{h}^{2}/\\left(T-h\\right)\\underset{as}{\\sim}\\chi_{H}^{2},\n\\end{equation*}\\] called the Ljung-Box test statistic is preferred because in small samples its distribution has been found to be closer to the \\(\\chi_{H}^{2}\\)–distribution than that of the test statistic \\(Q\\). It should be clear that both tests need \\(H\\) not to be too large compared to \\(T\\) to work well.\nThe autocorrelation function can be used to reveal linear dependences between the observations, but not nonlinear ones (with the exception of Gaussian processes).  To investigate the presence of potential nonlinear dependence over time, one (somewhat limited) approach is to test for autocorrelation in the squared observations.\n\nAssuming \\(y_{t}\\sim\\mathsf{iid}\\left(\\mu,\\sigma^{2}\\right)\\) and \\(\\mathsf{E}\\left(  y_{t}^{4}\\right)&lt;\\infty\\), what was said above about the sample autocorrelations also holds for sample autocorrelations computed from the squared observations \\(y_{t}^{2}\\). In particular, the asymptotic result(s) remain valid when the autocorrelations are computed from squared observations, and also the Ljung-Box test has its indicated asymptotic distribution. In this context, however, the test is usually called the McLeod-Li test.\nInvestigating the autocorrelations between squared observations is of great interest, especially in the case of financial time series, which themselves often are uncorrelated. We will return to this point later.\n\nAs an empirical illustration, let us consider the quarterly U.S. real GDP growth rate. It turns out that we obtain the following results from the Ljung-Box test for different lag lengths \\(H\\):\n| Lag | Q_{LB}  | p.value |\n|-----|---------|---------|\n| 4   | 16.487  | 0.002   |\n| 8   | 19.494  | 0.012   |\n| 12  | 27.942  | 0.006   |\n| 16  | 32.897  | 0.008   |\n| 20  | 34.679  | 0.022   |\nThese results clearly point out statistically significant autocorrelation (at the 5 % significance level) in the real GDP growth rate, which was apparent already based on the estimated autocorrelation coefficients. Furthermore, for the McLeod-Li tests, the resulting p-values are high (around 0.5 or higher) for all the lag length selections.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear process</span>"
    ]
  },
  {
    "objectID": "TSE-ch4.html#r-lab",
    "href": "TSE-ch4.html#r-lab",
    "title": "5  Linear process",
    "section": "5.8 R Lab",
    "text": "5.8 R Lab\n \n\nR code: Simulate MA(1) process and autocorrelations\n\n\n\nlibrary(tseries)\n\npar(mfrow = c(3, 2))  # Set up the plotting area to have 3 rows and 2 columns\n\n# Generate the first set of figures\ny1 &lt;- arima.sim(list(order=c(0,0,1), ma=c(0.9)), n=150, sd=1)\ny2 &lt;- arima.sim(list(order=c(0,0,1), ma=c(-0.9)), n=150, sd=1)\n\nplot(y1, type=\"l\", main=\"MA(1) process: theta_1 = 0.9\")\nplot(y2, type=\"l\", main=\"MA(1) process: theta_1 = -0.9\")\n\nacor1 &lt;- acf(ts(y1), lag=40, type=\"correlation\", main=\"Sample ACF\", xlab=\"\", ylab=\"\")\nacor2 &lt;- acf(ts(y2), lag=40, type=\"correlation\", main=\"Sample ACF\", xlab=\"\", ylab=\"\")\n\nacf1u &lt;- ARMAacf(ma=c(0.9), lag.max=40, pacf=FALSE)\nplot(acf1u, type=\"h\", xlab=\"Lag\", ylab=\" \", main=\"Theoretical ACF\")\n\nacf2u &lt;- ARMAacf(ma=c(-0.9), lag.max=40, pacf=FALSE)\nplot(acf2u, type=\"h\", xlab=\"Lag\", ylab=\" \", main=\"Theoretical ACF\")\n\n\n\n\n\n\n\n\n\n\nR code: Simulate AR(1) process and autocorrelations\n\n\n\nlibrary(tseries)\n\npar(mfrow = c(3, 2))  # Set up the plotting area to have 3 rows and 2 columns\n\n# Generate the first set of figures\ny1 &lt;- arima.sim(list(order=c(1,0,0), ar=c(0.7)), n=150, sd=1)\ny2 &lt;- arima.sim(list(order=c(1,0,0), ar=c(-0.7)), n=150, sd=1)\n\nplot(y1, type=\"l\", main=\"AR(1) process: phi_1 = 0.7\")\nplot(y2, type=\"l\", main=\"AR(1) process: phi_1 = -0.7\")\n\nacor1 &lt;- acf(ts(y1), lag=40, type=\"correlation\", main=\"Sample ACF\", xlab=\"\", ylab=\"\")\nacor2 &lt;- acf(ts(y2), lag=40, type=\"correlation\", main=\"Sample ACF\", xlab=\"\", ylab=\"\")\n\nacf1u &lt;- ARMAacf(ar=c(0.9), lag.max=40, pacf=FALSE)\nplot(acf1u, type=\"h\", xlab=\"Lag\", ylab=\" \", main=\"Theoretical ACF\")\n\nacf2u &lt;- ARMAacf(ar=c(-0.9), lag.max=40, pacf=FALSE)\nplot(acf2u, type=\"h\", xlab=\"Lag\", ylab=\" \", main=\"Theoretical ACF\")\n\n\n\n\n\n\n\n\n\n\nR code: Simulate a random walk process\n\n\n\nlibrary(tseries)\n\npar(mfrow = c(1, 2)) \n\n# AR(1) process / random walk (when selecting phi_1)\nT = 150\nphi_1=1\nepsilon=rnorm(T)\ny=epsilon    # initialize the vector y\ny[1]=epsilon[1]  # implicitly assuming initial value y[0]=0\nfor(i in 2:T) y[i] = phi_1*y[i-1]+epsilon[i]\nplot(y,type=\"l\", main=\"\",xlab=\"Time\", ylab=\"y_t\")\n\nplot(diff(y),type=\"l\", main=\"\",xlab=\"Time\", ylab=\"y_t - y_{t-1}\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear process</span>"
    ]
  },
  {
    "objectID": "TSE-ch5.html",
    "href": "TSE-ch5.html",
    "title": "6  ARMA processes",
    "section": "",
    "text": "6.1 AR(\\(p\\)) process\nEarlier, we considered the AR(1) process. This can be generalized to the AR(\\(p\\)) process \\[\\begin{equation*}\n    y_{t} = \\nu + \\phi_{1}y_{t-1}+\\cdots+\\phi_{p}y_{t-p}+u_{t}, \\quad u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right),\n\\end{equation*}\\] which can be rewritten \\[\\begin{equation*}\n    \\phi\\left(B\\right) (y_{t}- \\mu) = u_{t},\n\\end{equation*}\\] with the lag-polynomial \\(\\phi\\left(B\\right)=1-\\phi_{1}B-\\cdots-\\phi_{p}B^{p}\\). The current value of the process hence depends linearly on \\(p\\) past values of the process and the constant term, as well as on an unobserved random shock (or error term or innovation).\nA sufficient condition for the stationarity of an AR(\\(p\\)) process. A sufficient condition for the stationarity of the AR(\\(p\\)) process is that the roots of the polynomial \\(\\phi\\left(z\\right)\\) \\(\\left(z\\in \\mathbb{C}\\right)\\) lie outside the unit circle/disk in the complex plane, or equivalently that \\[\\begin{equation*}\n\\phi\\left(z\\right)\\neq 0 \\,\\, \\mathrm{for}\\,\\, \\left\\vert z\\right\\vert \\leq1.\n\\end{equation*}\\]\nBased on the above-mentioned, a stationary AR(\\(p\\)) process has an MA\\(\\left(\\infty\\right)\\) representation \\[\\begin{equation*}\n    (y_{t}- \\mu)=\\phi \\left(B\\right)^{-1}u_{t}=\\psi \\left(B\\right)u_{t}=\\sum_{j=0}^{\\infty}\\psi_{j}u_{t-j},\n\\end{equation*}\\] where \\(\\psi\\left(B\\right) =\\sum_{j=0}^{\\infty}\\psi_{j}B^{j}=\\phi\\left(B\\right)^{-1}\\). The coefficients \\(\\psi_{j}\\) can be solved as a function of the parameters \\(\\phi_{1},\\ldots,\\phi_{p}\\) from equation \\[\\begin{equation*}\n    \\left(1-\\phi_{1}B-\\cdots-\\phi_{p}B^{p}\\right) \\left(\\psi_{0}+\\psi_{1}B+\\psi_{2}B^{2}+\\cdots \\right)=1\n\\end{equation*}\\] by interpreting the right hand side as a power series in \\(B\\), and setting the coefficients of \\(B^{j}\\) equal to each other on both sides of the equation.\nAutocorrelation function. The autocorrelation function of an AR(\\(p\\)) process could be derived by making use of its MA\\(\\left(\\infty\\right)\\) representation. Instead, we present an often used and rather practical alternative approach based on the AR(\\(p\\)) model equation.\nMultiplying both sides of the demeaned AR(\\(p\\)) process presentation with \\(y_{t-h}-\\mu\\) \\(\\left(h\\geq0\\right)\\) and taking expectations, we obtain \\[\\begin{equation*}\n    \\mathsf{E}\\Big((y_{t}-\\mu)(y_{t-h}-\\mu)\\Big)  =\\phi_{1}\\mathsf{E}\\Big((y_{t-1}-\\mu) (y_{t-h}-\\mu)\\Big)+\\cdots+\\phi_{p}\\mathsf{E}\\Big((y_{t-p}-\\mu)(y_{t-h}-\\mu)\\Big)  +\\mathsf{E}\\Big(u_{t}(y_{t-h}-\\mu)\\Big).\n\\end{equation*}\\] Because \\(y_{t-h}\\) (and likewise \\(y_{t-h}-\\mu\\)) is a linear function of the innovation terms \\(u_{t-h},\\) \\(u_{t-h-1},\\ldots\\), the variables \\(y_{t-h}\\) and \\(u_{t}\\) are independent when \\(h&gt;0\\). Therefore, we get\nAs \\(\\gamma_{h}=\\gamma_{-h}\\), we get \\[\\begin{equation*}\n    \\gamma_{h}=\\left\\{\n    \\begin{array}\n        [c]{l}\n        \\phi_{1}\\gamma_{1}+\\cdots+\\phi_{p}\\gamma_{p}+\\sigma^{2},\\,\\, h=0\\\\\n        \\phi_{1}\\gamma_{h-1}+\\cdots+\\phi_{p}\\gamma_{h-p},\\,\\, h &gt; 0.\n    \\end{array}\n    \\right.\n\\end{equation*}\\] Dividing this (in the case \\(h&gt;0\\)) with the variance \\(\\gamma_{0}\\) leads to the autocorrelation function of an AR(\\(p\\)) process \\[\\begin{equation*}\n    \\rho_{h}=\\phi_{1}\\rho_{h-1}+\\cdots+\\phi_{p}\\rho_{h-p}, \\quad h&gt;0,\n\\end{equation*}\\] so that it satisfies a difference equation similar to the one the AR(\\(p\\)) process itself satisfies.\nPartial autocorrelation function. We next define the partial autocorrelation function, which is, among other things, a useful tool for model selection. In general, \\(\\alpha_h\\) equals the conventional partial correlation coefficient that measures the correlation between the random variables \\(y_t\\) and \\(y_{t-h}\\) when the linear effect of the random variables \\(y_{t-1}, \\ldots, y_{t-h+1}\\) has been first eliminated.\nIn the case of an AR(\\(p\\)) process, the partial autocorrelation function \\(\\alpha_h\\) has a special useful feature. For \\(m&gt;p\\), an AR(\\(p\\)) process can be interpreted as an AR(\\(m\\)) process with \\(\\phi_{p+1}=\\cdots=\\phi_{m}=0\\), which makes it clear that the partial autocorrelation function of an AR(\\(p\\)) process satisfies (also \\(\\alpha_0=1\\)): \\[\\begin{equation*}\ny_t \\thicksim \\mathrm{AR}(p) \\Rightarrow \\alpha_p = \\phi_p \\quad\n\\mathrm{and} \\quad \\alpha_h=0 \\,\\, \\mathrm{for}\\,\\, h &gt; p.\n\\end{equation*}\\] In other words, the partial autocorrelation function of an AR(\\(p\\)) process drops to zero after lag \\(p\\) (assuming \\(\\phi_p \\neq 0\\)).\nAn important detail for model selection is that if an observed time series has been generated by an AR(\\(p\\)) process, its estimated autocorrelation function should decay to zero as the lag length increases with no apparent breaks, whereas the estimated partial autocorrelation function should have a visible break after lag \\(p\\).\nTo identify the location of the break point in the estimated partial autocorrelation function, one can make use of the following result:\nEmpirical example. As an example, below we present the estimated sample partial autocorrelation functions (with lag lengths \\(h=0,\\ldots,20\\)) of the quarterly U.S. real GDP growth series, together with the above-mentioned critical/confidence bands.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ARMA processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch5.html#ARprocess",
    "href": "TSE-ch5.html#ARprocess",
    "title": "6  ARMA processes",
    "section": "",
    "text": "Notice that \\(\\phi\\left(B\\right) \\mu \\equiv \\phi\\left(1\\right) \\mu \\equiv \\nu\\).\n\n\n\n\nAs an example, in the stationary AR(1) case (\\(|\\phi_1| &lt; 1)\\), the characteristic equation yields \\(\\phi(z)=0 \\Leftrightarrow 1-\\phi _{1}z = 0\\) when \\(|z| &gt; 1\\). This shows why in the case \\(p=1\\), the condition \\(\\left\\vert \\phi_{1}\\right\\vert&lt;1\\) was found sufficient to ensure the existence of a (causal) stationary MA(\\(\\infty\\)) representation.\n\n\n\nNotice that the resulting root (roots) can also be complex numbers. Related to the above stationarity condition, the absolute value or norm of a complex number \\(z=x+iy\\) \\(\\left(i=\\sqrt{-1}\\right)\\) is defined as \\(\\left\\vert z\\right\\vert=\\sqrt{x^{2}+y^{2}}\\). The latter form of the stationarity condition can be further expressed as \\(\\phi\\left(z\\right)  =0\\Rightarrow\\left\\vert z\\right\\vert&gt;1\\).\nStill about possible complex roots: Because the coefficients of the polynomial \\(\\phi\\left(z\\right)\\) are real numbers, the potential complex roots always appear as conjugate roots, that is, if \\(\\zeta=x+iy\\) is a root, then also \\(\\bar{\\zeta}=x-iy\\) is a root.\n\n\n\nExtra: More details on stationarity condition\n\n\nStill more about the possibility of getting complex roots. The absolute value or norm of a complex number \\(z=x+iy\\) \\(\\left(i=\\sqrt{-1}\\right)\\) can be identified with the norm of the vector \\(\\left(x,y\\right)\\).\nOne way to illustrate this stationarity condition makes use of a well-known result in mathematics called the fundamental theorem of algebra. As a consequence of this result, the polynomial \\(\\phi\\left(z\\right)\\) can be written as (assuming that \\(\\phi_{p}\\neq0\\)) \\[\\begin{equation*}\n    \\phi \\left(z\\right)=\\left(1-\\zeta _{1}^{-1}z\\right) \\cdots \\left( 1-\\zeta_{p}^{-1}z\\right),\n\\end{equation*}\\] where for the roots \\(\\zeta_{i}\\) it therefore holds that \\(\\phi\\left(\\zeta_{i}\\right)=0\\) and \\(\\left\\vert \\zeta_{i}\\right\\vert&gt;1\\) (\\(i=1,\\ldots,p\\)). Therefore, an AR(\\(p\\)) process can be expressed as \\(\\left(1-\\zeta_{1}^{-1}B\\right)\\cdots\\left(1-\\zeta_{p}^{-1}B\\right)  y_{t}=u_{t}\\). If all the roots of \\(\\phi\\left(z\\right)\\) are real, this equation can be divided with the polynomials \\(\\left(  1-\\zeta_{i}^{-1}B\\right)\\), \\(i=1,\\ldots,p\\), one at a time, and in this way it can be seen (similarly as in the case \\(p=1\\)) that the resulting expression is a well-defined linear process. This procedure can also be generalized to the case of complex roots.\n\n\n\n\nRecall that two polynomials are the same if their coefficients are the same.\nFor instance, the coefficient of \\(B^{0}\\) is 1, so that \\(\\psi_{0}=1\\). Next, the coefficient of \\(B^{1}\\) is zero, so that \\(\\psi_{1}-\\phi_{1}\\psi_{0}=0\\) from which \\(\\psi_{1}=\\phi_{1}\\) follows. The general solution is left as an exercise.\n\n\n\n\n\n\\(\\mathsf{E}\\Big(u_t (y_{t-h}-\\mu)\\Big)=0, \\quad h&gt;0\\),\nFor \\(h=0\\), it can be seen that \\(\\mathsf{E}\\Big(u_{t}(y_{t}-\\mu)\\Big)=\\mathsf{E}\\left(u_{t}^{2}\\right)  =\\sigma^{2}\\).\n\n\n\nWhen the roots of \\(\\phi\\left(z\\right)\\) lie outside the unit disk on the complex plane, the solution \\(\\rho_{h}\\) to this difference equation decays exponentially to zero as the lag length \\(h\\) increases. \nThe solution \\(\\phi_{1}^{h}\\) for the AR(1) is an example of this (this solution can also be easily obtained by solving the difference equation above in the case \\(p=1\\) using the initial value \\(\\rho_{0}=1\\)).\n\n\n\n\nTherefore, a particular consequence is that \\(\\left\\vert \\alpha_{h}\\right\\vert\\leq1\\).\n\n\n\nThe sample counterpart of the (population) partial autocorrelation function is obtained by using sample autocovariances \\(c_h\\).\nThe (sample) partial autocorrelation function can generally be based on the Yule-Walker equations (see details below in the Extra section).\n\n\n\n\nIn the case of an AR(\\(p\\)) process, the estimators \\(\\widehat{\\alpha}_{h}\\), \\(h&gt;p\\), are approximately independent with \\(\\mathsf{N}\\left(0,T^{-1}\\right)\\)–distribution. Therefore, \\[\\begin{equation*}\n\\mathsf{P}(\\left\\vert \\widehat{\\alpha}_{h}\\right\\vert \\geq1.96/\\sqrt{T})\\approx0.05\n\\end{equation*}\\] when \\(h&gt;p\\).\nOn the other hand, because the estimators \\(\\mathsf{c}_{1},\\ldots,\\mathsf{c}_{p}\\) are consistent, the estimator \\(\\widehat{\\alpha}_{p}\\) converges in probability to the value of the theoretical partial autocorrelation coefficient \\(\\alpha_{p}\\) which for an AR(\\(p\\)) process is nonzero. Therefore, for \\(h=p\\), \\(\\mathsf{P}(\\left\\vert \\widehat{\\alpha}_{h}\\right\\vert \\geq1.96/\\sqrt{T})\\) approaches one as the sample size increases.\nThese remarks justify depicting the sample partial autocorrelation coefficients similar to that of the usual autocorrelation coefficients, and adding to it horizontal critical/confidence bands to make its interpretation easier.\n\n\n\nExtra: Yule-Walker equations to obtain autocorrelation and partial autocorrelation coefficients\n\n\nChoosing \\(h=1,\\ldots,p\\) in the AR(\\(p\\)) autocorrelation function leads to the equations (\\(\\rho_{0}=1\\) and \\(\\rho_{h}=\\rho_{-h}\\)) \\[\\begin{align*}\n    \\rho_{1}& =\\phi_{1}+\\phi_{2}\\rho_{1}+\\cdots+\\phi_{p}\\rho_{p-1}\\\\\n    \\rho_{2}& =\\phi_{1}\\rho_{1}+\\phi_{2}+\\cdots+\\phi_{p}\\rho_{p-2}\\\\\n    & \\vdots\\\\\n    \\rho_{p}& =\\phi_{1}\\rho_{p-1}+\\phi_{2}\\rho_{p-2}+\\cdots+\\phi_{p},\n\\end{align*}\\] which collectively are called the . In matrix notation, these can be expressed as \\[\\begin{equation*}\n    \\boldsymbol{\\rho}=\\boldsymbol{P\\phi,}\n\\end{equation*}\\] where \\(\\boldsymbol{\\rho}=\\left[\\rho_{1}\\text{ }\\cdots\\text{ }\\rho_{p}\\right]^{\\prime}\\), \\(\\boldsymbol{\\phi=}\\left[\\phi_{1}\\text{ }\\cdots\\text{ }\\phi_{p}\\right]^{\\prime}\\) and \\(\\boldsymbol{P=}\\left[\\rho_{i-j}\\right]_{i,j=1,\\ldots,p}\\) is a \\(p\\times p\\) matrix, whose row \\(i\\) and column \\(j\\) element equals \\(\\rho_{i-j}\\). This makes it clear that the parameter vector \\(\\boldsymbol{\\phi}\\) can be expressed as a function of the autocovariance or autocorrelation coefficients. The result is Yule-Walker equation \\[\\begin{equation}\n    \\boldsymbol{\\phi}=\\boldsymbol{P}^{-1}\\boldsymbol{\\rho=\\Gamma}^{-1}\\boldsymbol{\\gamma},\\label{Yule-Walker}\n\\end{equation}\\] where \\(\\boldsymbol{\\gamma}=\\left[\\gamma_{1}\\text{ }\\cdots\\text{ }\\gamma_{p}\\right]^{\\prime}=\\gamma_{0}\\boldsymbol{\\rho}\\) and \\(\\boldsymbol{\\Gamma}=\\left[\\gamma_{i-j}\\right]  _{i,j=1,\\ldots,p}=\\gamma_{0}\\boldsymbol{P}\\). From the equations derived above it follows that also the parameter \\(\\sigma^{2}\\) can be expressed as a function of the autocovariance coefficients and the parameters \\(\\phi_{1},\\ldots,\\phi_{p}\\) as \\[\\begin{equation}\n    \\sigma^{2}=\\gamma_{0}-\\phi_{1}\\gamma_{1}-\\cdots-\\phi_{p}\\gamma_{p}.\\label{Yule-Walker_sigma^2}\n\\end{equation}\\]\nA note on \\(\\boldsymbol{P}^{-1}\\): It is rather clear that this inverse matrix exists here. If not, there would need to be an exact linear relationship between the variables \\(y_{t-1},\\ldots,y_{t-p}\\), which (except for the case \\(\\sigma^{2}=0\\)) is not possible. This will be clarified further in Section 6 in connection with forecasting %(Technical note (that can be ignored): The sufficient condition for stationarity of an AR(\\(p\\)) process guarantees that \\(P\\) is invertible, although seeing this is somewhat technical (see, e.g., Hannan (1970), p. 408, ex. 7).\n \nFor further preciseness, let us denote \\(\\boldsymbol{\\gamma}=\\boldsymbol{\\gamma}_{p}\\) and \\(\\boldsymbol{\\Gamma=\\Gamma}_{p}\\) in Yule-Walker equation. Now, the partial autocorrelation function with lag \\(h\\) is defined as \\[\\begin{equation*}\n    \\alpha_{h}=\\left\\{\n    \\begin{array}\n        [c]{l}\n        1, \\,\\, \\mathrm{for} \\,\\, h=0\\\\\n        \\text{the last component of the vector }\\boldsymbol{\\Gamma}_{h}^{-1}%\n        \\boldsymbol{\\gamma}_{h}\\text{, when }h\\geq1.\n    \\end{array}\n    \\right.\n\\end{equation*}\\] Because the second and third expressions of Yule-Walker equation can be defined for any weakly stationary (non-deterministic) process, so can also the partial autocorrelation function.\nThe sample counterpart of the (population) partial autocorrelation function is straightforward to define, simply estimating the vector \\(\\boldsymbol{\\gamma}_{h}\\) and matrix \\(\\boldsymbol{\\Gamma}_{h}\\) using the obvious estimators \\(\\boldsymbol{c}_{h}=\\left[\\mathsf{c}_{1}\\text{ }\\cdots\\text{ }\\mathsf{c}_{h}\\right]^{\\prime}\\) and \\(\\boldsymbol{C}_{h}=\\left[  \\mathsf{c}_{i-j}\\right]_{i,j=1,\\ldots,h}\\). Therefore, the sample partial autocorrelation coefficient \\(\\widehat{\\alpha}_{h}\\) equals \\(1\\) for \\(h=1\\), and the last component of the Yule-Walker estimate \\(\\boldsymbol{\\widehat{\\phi}}_{YW}=\\boldsymbol{C}_{h}^{-1}\\boldsymbol{c}_{h}\\) of the parameter \\(\\boldsymbol{\\phi}\\) for \\(h\\geq1\\). As a remark, we note that there exist recursive formulas that can be used to compute both theoretical and sample partial autocorrelation coefficients that avoid computing the inverse of (the potentially large) matrix \\(\\boldsymbol{C}_{h}\\), but we omit the details of this.\n\n\n\n\nThe corresponding sample autocorrelation functions are shown already above.\nThe first two sample partial autocorrelation coefficients \\(\\widehat{\\alpha}_{1}\\) and \\(\\widehat{\\alpha}_{2}\\) are clearly outside the confidence bands, whereas the rest lie within these bands (except 11th lag), suggesting an AR(2) process. The sample autocorrelation function partly supports this conclusion, while also MA(2) or some ARMA model might clearly be potential candidate for the GDP growth.\nWe are coming back to empirical model selection later in this material.\n\n\n\nFigure: Sample partial autocorrelation function (PACF) of the U.S. quarterly real GDP growth rate.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ARMA processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch5.html#maq-process-and-invertibility",
    "href": "TSE-ch5.html#maq-process-and-invertibility",
    "title": "6  ARMA processes",
    "section": "6.2 MA(\\(q\\)) process and invertibility",
    "text": "6.2 MA(\\(q\\)) process and invertibility\nThe MA(\\(q\\)) process is defined as \\[\\begin{equation*}\n    y_{t} = \\mu + u_{t}+\\theta_{1}u_{t-1}+\\cdots+\\theta_{q}u_{t-q}, \\quad u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right),\n\\end{equation*}\\] or alternatively using the lag operator as \\[\\begin{equation*}\n    (y_{t}-\\mu)=\\theta\\left(B\\right)u_{t},\\,\\, u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right)\n\\end{equation*}\\] with the lag-polynomial \\(\\theta\\left(B\\right)=1+\\theta_{1}B+\\cdots+\\theta_{q}B^{q}\\). Therefore, the current value of the process is assumed to depend linearly on the present and last \\(q\\) unobservable random shocks (error terms or innovations). Because an MA(\\(q\\)) process is a special case of the (causal) linear process, it is always both weakly and strictly stationary. Moreover, we get\n\n\\(\\mathsf{E}\\left( y_{t}\\right) = \\mu\\),\n\\(\\mathsf{Var}\\left( y_{t}\\right) =\\gamma _{0}=\\left( 1+\\theta _{1}^{2}+\\cdots +\\theta _{q}^{2}\\right) \\sigma^{2}\\).\n\n \nAutocorrelation function. By making use of the general results of linear process, the MA(\\(q\\)) process has the autocovariance function given by \\[\\begin{equation*}\n    \\gamma_{h}=\n        \\begin{cases}\n        \\sigma^{2}\\sum_{j=0}^{q-h}\\theta_{j}\\theta_{j+h}, & \\text{for } \\, 0\\leq h\\leq q,\\\\\n        0, & \\text{for } \\, h&gt;q,\n    \\end{cases}\n\\end{equation*}\\] where \\(\\theta_0=1\\). The autocorrelation function is then obtained via the formula \\(\\rho_{h}=\\gamma_{h}\\gamma_{0}\\).\n\nTherefore, the autocorrelation function of an MA(\\(q\\)) process drops to zero after lag \\(q\\) (assuming that \\(\\theta_{q}\\neq0\\)).\nIf one observes a similar feature in a sample autocorrelation function, one can consider an MA(\\(q\\)) process as a good candidate to model the time series.\n\nWhen considering the suitability of an MA(\\(q\\)) process, one can make use of the following result: In the case of an MA(\\(q\\)) process, the estimators \\(\\mathsf{r}_{h}\\), \\(h&gt;q\\), are approximately normally distributed with mean zero and variance \\(\\left(1+2\\rho_{1}^{2}+\\cdots+2\\rho_{q}^{2}\\right)T\\). Therefore, if one wants to test whether an individual autocorrelation coefficient is statistically different from zero at the 5% significance level, the estimates \\(\\mathsf{r}_{h}\\), \\(h&gt;q\\), should be compared with the critical bounds \\[\\begin{equation*}\n\\pm1.96\\sqrt{\\mathsf{\\widehat{w}}_{q}/T}, \\quad \\mathsf{\\widehat{w}}_{q}=\\left(1+2\\mathsf{r}_{1}^{2}+\\cdots+2\\mathsf{r}_{q}^{2}\\right).\n\\end{equation*}\\] In this case, \\(\\mathsf{P}(\\left\\vert \\mathsf{r}_{h}\\right\\vert \\geq1.96\\sqrt{\\mathsf{\\widehat{w}}_{q}/T})\\approx0.05\\). Note also that the bounds above are wider than in the case \\(y_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right)\\) when \\(\\mathsf{\\widehat{w}}_{q}\\) is replaced by \\(1\\).\n \nInvertibility. It can be shown that in a (causal) AR(\\(p\\)) process there exists a one to one correspondence between the parameters \\(\\boldsymbol{\\phi}=(\\phi_1,\\ldots,\\phi_p)\\) and \\(\\sigma^{2}\\), and the autocovariance function. For an MA(\\(q\\)) process, a similar result does not hold.\n\nTo see this, consider as an example the MA(1) process for which it holds that (assume that \\(\\theta_1 \\neq0\\)) \\[\\begin{equation*}\n  \\gamma_{0}=\\sigma^{2}\\left(1+\\theta_1^{2}\\right)=\\sigma^{2}\\theta_1^{2}\\left(1+1/\\theta_1^{2}\\right) \\,\\, \\mathrm{and} \\,\\, \\gamma_{1}=\\sigma^{2}\\theta_1=\\theta_1^{2}\\sigma^{2}\\left(1/\\theta_1\\right)\\text{.}\n\\end{equation*}\\] Define \\(\\theta_1^{\\ast}=1/\\theta_1\\) and \\(\\sigma_{\\ast}^{2}=\\theta_1^{2}\\sigma^{2}\\), so that \\(u_{t}^{\\ast}=\\theta_1u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma_{\\ast}^{2}\\right)\\). Now, it can be seen that the two MA(1) processes \\(y_{t}=u_{t}+\\theta_1 u_{t-1}\\) and \\(y_{t}^{\\ast}=u_{t}^{\\ast}+\\theta_1^{\\ast}u_{t-1}^{\\ast}\\) have exactly the same autocovariance function, so that they cannot be distinguished from each other based on autocovariances.\nIf in addition \\(u_{t}\\sim\\mathsf{nid}\\left(  0,\\sigma^{2}\\right)\\), the entire probability structures of these two processes are indistinguishable.\nA consequence is that estimating the parameters based on an observed time series, unless one “tells” the method which of the parameter combinations \\(\\left(\\theta_1,\\sigma^{2}\\right)\\) and \\(\\left(  \\theta_1^{\\ast},\\sigma_{\\ast}^{2}\\right)\\) (that fit the data equally well) should be chosen.\n\n\n\nThe typical solution to the above problem is to set the condition \\(\\left\\vert \\theta_1\\right\\vert &lt;1\\). When one assumes \\(\\left\\vert \\theta_1\\right\\vert&lt;1\\) in an MA(1) process, one can use the equation \\(u_{t}=y_{t}-\\mu - \\theta_1 u_{t-1}\\) and repetitive substitutions to first obtain \\(u_{t}=y_{t}-\\mu -\\theta_1 (y_{t-1}-\\mu) +\\theta_1^{2}u_{t-2}\\), and eventually \\[\\begin{equation*}\n    (y_{t}-\\mu)=-\\sum_{j=1}^{k}\\left(-\\theta_1\\right)^{j}(y_{t-j}-\\mu)-\\left(-\\theta_1\\right)^{k+1}u_{t-k-1}+u_{t}.\n\\end{equation*}\\] Similarly as when considering the stationarity of an AR(1) process, it can be concluded, given that the condition \\(\\left\\vert \\theta_1\\right\\vert &lt;1\\), \\[\\begin{equation*}\n    y_{t} -\\mu =-\\sum_{j=1}^{\\infty}\\left(-\\theta_1\\right)^{j} (y_{t-j}-\\mu) + u_{t}.\n\\end{equation*}\\] That is, when \\(\\left\\vert \\theta_1 \\right\\vert&lt;1\\) holds, an MA(1) process can be “inverted” to an AR\\(\\left(\\infty\\right)\\) process, which explains why in the case \\(\\left\\vert \\theta_1\\right\\vert&lt;1\\) an MA(1) process is called invertible.\nAn alternative way to express the invertibility condition of an MA(1) process is to require that the root of the polynomial \\(\\theta\\left(z\\right)\\) (that is, the solution to the equation \\(\\theta\\left(  z\\right)  =0\\)) is greater than one in absolute value, or in other words, that \\[\\begin{equation*}\n\\theta\\left(z\\right)  \\neq0 \\,\\, \\mathrm{for} \\,\\, \\left\\vert z\\right\\vert \\leq1.\n\\end{equation*}\\] Similarly as in the case of the stationarity condition of an AR(\\(p\\)) process, this condition ensures invertibility (that is, the existence of an AR\\(\\left(\\infty\\right)\\) representation also for MA(\\(q\\)) processes).\n \nInvertibility condition of an MA(\\(q\\)) process . An MA(\\(q\\)) process is invertible, if the roots of the polynomial \\(\\theta\\left(z\\right)\\) \\(\\left(z\\in\\mathbb{C}\\right)\\) lie outside the unit circle/disk on the complex plane or, equivalently, if \\[\\begin{equation*}\n\\theta\\left(z\\right) \\neq0 \\,\\,\\, \\mathrm{for} \\,\\,\\, \\left\\vert z\\right\\vert\\leq1.\n\\end{equation*}\\]\n \nWhen the invertibility condition holds, one can formally solve \\(u_{t}\\) from equation \\((y_{t}-\\mu)=\\theta\\left(B\\right)u_{t}\\) by dividing both sides by the polynomial \\(\\theta\\left(B\\right)^{-1}\\). The solution is \\[\\begin{equation*}\n    \\pi\\left(B\\right)(y_{t}-\\mu) = u_{t} \\,\\,\\, \\mathrm{or} \\,\\,\\, \\sum_{j=0}^{\\infty}\\pi_{j}(y_{t-j}-\\mu)=u_{t},\n\\end{equation*}\\] where \\(\\pi\\left(B\\right)=\\sum_{j=0}^{\\infty}\\pi_{j}B^{j}=\\theta\\left(B\\right)^{-1}\\) and the coefficients \\(\\pi_{j}\\) can be solved as a function of the parameters \\(\\theta_{1},\\ldots,\\theta_{q}\\) from the equation \\[\\begin{equation*}\n    \\left(1+\\theta_{1}B+\\cdots+\\theta_{q}B^{q}\\right) \\left(\\pi_{0}+\\pi_{1}B+\\pi_{2}B^{2}+\\cdots\\right)=1\n\\end{equation*}\\] by interpreting the right hand side as a power series in \\(B\\), setting the coefficients of \\(B^{j}\\) equal on both sides of the equation (and \\(\\pi_{0}=1\\)).\nAs in the MA(1) case, invertibility ensures that there exists a one to one correspondence between the autocovariance function of an MA(\\(q\\)) process and the parameters \\(\\boldsymbol{\\theta}=\\left(\\theta_{1},\\ldots,\\theta_{q}\\right)\\) and \\(\\sigma^{2}\\).\n\nExcept for the first-order case, this correspondence is rather complicated, and we omit the details.\nIn what follows (unless otherwise mentioned), MA processes are always assumed to be invertible.\n\n \nPartial autocorrelation function. The general definition of a partial autocorrelation function presented in the previous subsection can also be applied in the case of an MA(\\(q\\)) process, but the calculations involved would become rather complicated. However, because an MA(\\(q\\)) process has (due to invertibility and assuming here \\(\\theta_{q}\\neq0\\)) an AR(\\(\\infty\\)) representation, and based on what was said above about the partial autocorrelation function of an AR(\\(p\\)) process, it is intuitively clear that the partial autocorrelation function of an MA(\\(q\\)) process does not drop to zero at any point, but rather smoothly decays towards zero.\n\nIt can be shown that the the partial autocorrelation function of an MA(1) process is \\[\\begin{equation*}\n  \\alpha_{h}=-\\left(-\\theta_{1}\\right)^{h}/\\left(1+\\theta_{1}^{2}+\\cdots+\\theta_{1}^{2h}\\right).\n\\end{equation*}\\] Because \\(\\left\\vert \\theta_{1}\\right\\vert&lt;1\\), the partial autocorrelation function of an MA(1) process decays to zero at an exponential rate as the lag length \\(h\\) increases.\nFor a general MA(\\(q\\)) process, it can be shown that the partial autocorrelation function decays to zero at an exponential rate, potentially following the shape of a dampening sine wave.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ARMA processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch5.html#armapq-process",
    "href": "TSE-ch5.html#armapq-process",
    "title": "6  ARMA processes",
    "section": "6.3 ARMA(\\(p,q\\)) process",
    "text": "6.3 ARMA(\\(p,q\\)) process\nAn ARMA(\\(p\\),\\(q\\)) process can be characterized as a combination of AR(\\(p\\)) and MA(\\(q\\)) processes. It is defined by the equation \\[\\begin{equation*}\n    y_{t} = \\nu + \\phi_{1}y_{t-1}+\\cdots+\\phi_{p}y_{t-p}+u_{t}+\\theta_{1}u_{t-1}+\\cdots+\\theta_{q}u_{t-q},\n\\end{equation*}\\] where \\(u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right)\\). Similarly as in the case of an AR(\\(p\\)) process, the current value \\(y_{t}\\) is assumed to depend linearly on \\(p\\) past values of the process. Unlike the AR(\\(p\\)) process, the error term is now (generally) not an independent random shock, but instead an autocorrelated MA(\\(q\\)) process.\nUsing the lag operator and the lag-polynomials \\(\\phi\\left(B\\right)=1-\\phi_{1}B-\\cdots-\\phi_{p}B^{p}\\) and \\(\\theta\\left(B\\right)=1+\\theta_{1}B+\\cdots+\\theta_{q}B^{q}\\), the ARMA(\\(p,q\\)) process can be expressed as \\[\\begin{equation*}\n    \\phi\\left(B\\right)(y_{t}-\\mu) = \\theta\\left(B\\right)u_{t}.\n\\end{equation*}\\]\n\nIf \\(\\theta\\left(B\\right)=1\\) (that is, \\(q=0\\)), one obtains the AR(\\(p\\)) process as a special case.\nIf \\(\\phi\\left(B\\right)=1\\) (that is, \\(p=0\\)), one obtains the MA(\\(q\\)) process.\n\n\n \nStationarity and invertibility. Based on what was said for linear processes, it is clear than an ARMA(\\(p\\),\\(q\\)) process has a well-defined (causal) MA(\\(\\infty\\)) representation if the polynomial \\(\\phi\\left(z\\right)\\) satisfies the sufficient stationarity condition of an AR(\\(p\\)) process (what was said in the case of an AR(\\(p\\)) process holds, but now with MA(\\(q\\)) error terms).\n \n\nA sufficient condition for the stationarity of the ARMA(\\(p\\),\\(q\\)) process is that the roots of the polynomial \\(\\phi\\left(z\\right)\\) \\(\\left(z\\in\\mathbb{C}\\right)\\) lie outside the unit circle/disk on the complex plane, or equivalently that \\[\\begin{equation*}\n\\phi\\left(  z\\right)  \\neq0 \\,\\,\\, \\mathrm{for} \\,\\,\\, \\left\\vert z\\right\\vert \\leq1.\n\\end{equation*}\\]\n\n\n \n\nAn ARMA(\\(p\\),\\(q\\)) process is invertible, if the roots of the polynomial \\(\\theta\\left(z\\right)\\) \\(\\left(z\\in\\mathbb{C}\\right)\\) lie outside the unit circle/disk on the complex plane, or equivalently if \\[\\begin{equation*}\n\\theta\\left(z\\right)\\neq0 \\,\\,\\, \\mathrm{for} \\,\\,\\, \\left\\vert z\\right\\vert \\leq1.\n\\end{equation*}\\] The general discussion concerning invertibility in the previous subsection in connection with the MA(1) process also holds here and generalizes to the case of an ARMA(\\(p\\),\\(q\\)) process.\n\n \nAs can be deduced from what was discussed above, when the stationarity condition holds, an ARMA(\\(p\\),\\(q\\)) process has an MA\\(\\left(\\infty\\right)\\) representation (cf. the AR(\\(p\\)) case) \\[\\begin{equation*}\n    (y_{t}-\\mu)=\\frac{\\theta\\left(B\\right)}{\\phi\\left(B\\right)}u_{t}=\\psi\\left(B\\right)u_{t},\n\\end{equation*}\\] where \\(\\psi\\left(B\\right)=\\sum_{j=0}^{\\infty}\\psi_{j}B^{j}=\\theta\\left(B\\right)\\phi\\left(B\\right)^{-1}\\) and the coefficients \\(\\psi_{j}\\) can be solved as a function of the parameters \\(\\phi_{1},\\ldots,\\phi_{p}\\) and \\(\\theta_{1},\\ldots,\\theta_{q}\\) by equating the coefficients of \\(B^{j}\\) on both sides of the equation \\[\\begin{equation*}\n\\left(1-\\phi_{1}B-\\cdots-\\phi_{p}B^{p}\\right)\\left(\\psi_{0}+\\psi_{1}B+\\psi_{2}B^{2}+\\cdots\\right)=1+\\theta_{1}B+\\cdots+\\theta_{p}B^{q}.\n\\end{equation*}\\]\n\nFrom this equation, it can be solved that \\(\\psi_{0}=1\\), \\(\\theta_{1}=\\psi_{1}-\\psi_{0}\\phi_{1}\\), and in general, \\[\\begin{equation*}\n  \\psi_{j}=\\sum_{i=1}^{p}\\phi_{i}\\psi_{j-i}+\\theta_{j}, \\,\\, j=0,1,2,\\ldots,\n\\end{equation*}\\] where \\(\\theta_{0}=1,\\) \\(\\theta_{j}=0\\), \\(j&gt;q\\), and \\(\\psi_{j}=0\\), \\(j&lt;0\\).\n\n\nOn the other hand, when the invertibility condition holds, an ARMA(\\(p\\),\\(q\\)) process has an AR\\(\\left(\\infty\\right)\\) representation \\[\\begin{equation*}\n    \\frac{\\phi\\left(B\\right)}{\\theta\\left(B\\right)}(y_{t}-\\mu)=\\pi\\left(B\\right)(y_{t}-\\mu)=u_{t},\n\\end{equation*}\\] where \\(\\pi\\left(B\\right)=\\sum_{j=0}^{\\infty}\\pi_{j}B^{j}=\\phi\\left(B\\right)\\theta\\left(B\\right)^{-1}\\). The coefficients \\(\\pi_{j}\\) can be solved similarly as the \\(\\psi_{j}\\)’s above (just exchange the roles of the polynomials \\(\\phi\\left(B\\right)\\) and \\(\\theta\\left(B\\right)\\)). The end result can be expressed as \\[\\begin{equation*}\n    \\pi_{j}=-\\sum_{i=1}^{p}\\theta_{i}\\pi_{j-i}-\\phi_{j},\\,\\, j=0,1,2,\\ldots,\n\\end{equation*}\\] where \\(\\phi_{0}=-1,\\) \\(\\phi_{j}=0\\), \\(j&gt;p\\), and \\(\\pi_{j}=0\\), \\(j&lt;0\\). In particular, it holds that \\(\\pi_{0}=1\\) and, as in the case of the coefficients \\(\\psi_{j}\\), \\(\\pi_{j}\\rightarrow 0\\) at an exponential rate as \\(j\\rightarrow \\infty\\).\n \nIdentification condition. Earlier in connection to MA processes, the invertibility condition was motivated by the fact that it guarantees the existence of a one-to-one correspondence between the autocovariance function of a process and its model parameters. For general ARMA(\\(p\\),\\(q\\)) processes, invertibility alone is not sufficient to guarantee the existence of such a correspondence, which is also required for maximum likelihood estimation of the parameters.\n\nConsider as an example the simple case of an ARMA(1,1) process with the linear representation \\[\\begin{equation*}\n  (y_{t}-\\mu)=\\frac{1+\\theta_{1}B}{1-\\phi_{1}B}u_{t}.\n\\end{equation*}\\] In the special case \\(\\phi_{1}=-\\theta_{1}\\), it is clear that the polynomials on the right hand side can be cancelled out, resulting in the equation \\(y_{t}-\\mu=u_{t}\\) so that the process is not a “real” ARMA(1,1) process, but instead just white noise. This implies that the parameters \\(\\phi_{1}\\) and \\(\\theta_{1}\\) are not identified in the sense that maximum likelihood estimation will not work because this method cannot distinguish different pairs of parameter values that satisfy the restriction \\(\\phi_{1}=-\\theta_{1}\\) (and \\(\\left\\vert \\phi_{1}\\right\\vert &lt;1\\), \\(\\left\\vert \\theta_{1}\\right\\vert&lt;1\\)).\n\n\nIdentification (or uniqueness) condition of an ARMA(\\(p\\),\\(q\\)) process. The polynomials \\(\\phi\\left(z\\right)=1-\\phi_{1}z-\\cdots-\\phi_{p}z^{p}\\) and \\(\\theta\\left(z\\right)=1+\\theta_{1}z+\\cdots+\\theta_{q}z^{q}\\) of a stationary and invertible ARMA(\\(p\\),\\(q\\)) process are assumed to have no common roots and \\(\\phi_{p}\\neq0\\) or \\(\\theta_{q}\\neq0\\).\n\nQuite often, the requirement that \\(\\phi_{p}\\neq0\\) or \\(\\theta_{q}\\neq0\\) is not explicitly mentioned but is understood to be contained in the condition ruling out common roots.\nUnless otherwise mentioned, in what follows, we assume that this identification condition holds.\n\n \nAutocorrelation function of an ARMA(\\(p\\),\\(q\\)) process can be derived in a manner similar to the AR(\\(p\\)) case.\n\nIn what follows, we only present the general principle of the solution.\n\nMultiplying both sides of the ARMA(\\(p,q\\)) model equation with \\(y_{t-h}\\) \\(\\left(h\\geq0\\right)\\) and taking expectations yields \\[\\begin{align*}\n    \\mathsf{E}\\Big((y_{t}-\\mu)(y_{t-h}-\\mu)\\Big) & =\\phi_{1}\\mathsf{E}\\Big(y_{t-1}(y_{t-h}-\\mu)\\Big)+\\cdots+\\phi_{p}\\mathsf{E}\\Big((y_{t-p}-\\mu)(y_{t-h}-\\mu)\\Big)+\\mathsf{E}\\Big(u_{t}(y_{t-h}-\\mu)\\Big) \\\\\n    & +\\theta_{1}\\mathsf{E}\\Big(u_{t-1}(y_{t-h}-\\mu)\\Big)+\\cdots+\\theta_{q}\\mathsf{E}\\Big(u_{t-q}(y_{t-h}-\\mu)\\Big).\n\\end{align*}\\] Making use of the MA\\(\\left(\\infty\\right)\\) representation and the definition of autocovariance this further leads to \\[\\begin{equation*}\n    \\gamma_{h} =\n    \\begin{cases}\n        \\phi_{1} \\gamma_{h-1} + \\cdots + \\phi_{p}\\gamma_{h-p} + \\sigma^{2}\\sum_{j=0}^{\\infty}\\theta_{h+j}\\psi_{j}, & 0 \\leq h &lt; \\max\\left\\{p, q+1\\right\\}, \\\\\n        \\phi_{1} \\gamma_{h-1} + \\cdots + \\phi_{p}\\gamma_{h-p}, & h \\geq \\max\\left\\{p, q+1\\right\\}\n    \\end{cases}\n\\end{equation*}\\] where \\(\\theta_{0}=1\\) and \\(\\theta_{j}=0\\) for \\(j\\notin\\left\\{0,\\ldots,q\\right\\}\\). The autocorrelation function is then obtained using the formula \\(\\rho_{h}=\\gamma_{h}/\\gamma_{0}\\). Moreover, the coefficients \\(\\psi_{j}\\) can be expressed as function of the parameters \\(\\phi_{1},\\ldots,\\phi_{p}\\) and \\(\\theta_{1},\\ldots,\\theta_{q}\\).\n\n\nBecause this recursive solution is obtained from a difference equation similar to the one the autocovariances of an AR(\\(p\\)) process satisfy, it is rather clear that as the lag length \\(h\\) increases, the autocorrelation function of an ARMA(\\(p,q\\)) process decays exponentially to zero, potentially following the shape of a dampening sine wave (in the case \\(p=1\\) this is rather easy to verify).\nBecause an ARMA(\\(p,q\\)) process has an AR(\\(\\infty\\)) representation when the invertibility condition holds, one can use the general definition of the partial autocorrelation function to conclude that the general shape of the partial autocorrelation function of an ARMA(\\(p,q\\)) process is comparable to that of the autocorrelation function.\n\nTo conclude and importantly, neither the autocorrelation nor the partial autocorrelation function of an ARMA(\\(p,q\\)) process ever drop to zero but instead both steadily decay towards zero.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ARMA processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch6.html",
    "href": "TSE-ch6.html",
    "title": "7  Parameter estimation",
    "section": "",
    "text": "7.1 Estimation of AR models with conditional OLS\nEstimation of the AR(\\(p\\)) model can be carried out with ordinary least squares (OLS), conditioning on the first \\(p\\) observations. The AR(\\(p\\)) model (including constant term) can be rewritten as \\[\\begin{equation*}\ny_t= \\boldsymbol{x}^{\\prime}_t \\boldsymbol{\\beta} + u_t,\n\\end{equation*}\\] where \\(\\boldsymbol{x}_t=[1 \\quad y_{t-1} \\, \\cdots \\, y_{t-p}]^{\\prime}\\) and \\(\\boldsymbol{\\beta}=[\\nu \\quad \\phi_1 \\, \\cdots \\, \\phi_p]^{\\prime}\\). The OLS estimators of \\(\\boldsymbol{\\beta}\\) (and \\(\\sigma^2\\)) can be constructed in the usual way familiar from past studies related to linear regression models. That is, the conditional OLS estimator of \\(\\boldsymbol{\\beta}\\) is \\[\\begin{equation*}\n\\widehat{\\boldsymbol{\\beta}} = \\Big(\\sum_{t=1}^T \\boldsymbol{x}_t \\boldsymbol{x}^{\\prime}_t \\Big)^{-1} \\sum_{t=1}^T \\boldsymbol{x}_t y_t.\n\\end{equation*}\\] In this case (notation), it is assumed that the required \\(p\\) initial values \\(y_{-p+1},\\ldots, y_0\\) are available.\nThe usual (asymptotic) statistical properties of the resulting OLS estimator hold. In particular, it can be shown that:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameter estimation</span>"
    ]
  },
  {
    "objectID": "TSE-ch6.html#estimation-of-ar-models-with-conditional-ols",
    "href": "TSE-ch6.html#estimation-of-ar-models-with-conditional-ols",
    "title": "7  Parameter estimation",
    "section": "",
    "text": "Notice that the summation in the OLS estimator starts from \\(t=1,2,\\ldots\\) due to these available initial values. Depending on, mainly notational choices, alternatively the OLS estimator is at times written as \\[\\begin{equation*}\n\\widehat{\\boldsymbol{\\beta}} = \\Big(\\sum_{t=p+1}^T \\boldsymbol{x}_t \\boldsymbol{x}^{\\prime}_t \\Big)^{-1} \\sum_{t=p+1}^T \\boldsymbol{x}_t y_t,\n\\end{equation*}\\] if treating observations \\(y_1,\\ldots, y_p\\) as initial values.\n\n\n\nAs the error term \\(u_t\\) is uncorrelated with the lags of \\(y_t\\), we have \\(\\mathsf{E}(\\boldsymbol{x}_t u_t)=\\mathbf{0}\\) (that is, \\(\\mathsf{E}(u_t (y_{t-j}-\\mu)) = 0\\) for \\(j=1,2,\\ldots,p\\)), and the OLS estimator \\(\\widehat{\\boldsymbol{\\beta}}\\) is a consistent estimator of \\(\\boldsymbol{\\beta}\\).\nHeteroskedasticity-autocorrelation consistent (HAC) standard errors, such as Newey-West standard errors, can, and often should, be automatically used to take the potential remaining conditional heteroskedasticity and autocorrelation in the residuals into account for detailed statistical inference (if this is the main objective in empirical analysis).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameter estimation</span>"
    ]
  },
  {
    "objectID": "TSE-ch6.html#maximum-likelihood-estimation",
    "href": "TSE-ch6.html#maximum-likelihood-estimation",
    "title": "7  Parameter estimation",
    "section": "7.2 Maximum likelihood estimation",
    "text": "7.2 Maximum likelihood estimation\nEstimation of ARMA (ARIMA) models with a moving average component is somewhat more complicated than the OLS based estimation of autoregressive models. The essential problem in estimating these models stems from the fact that \\(u_t\\) is not observable.\nThe estimation of an ARMA(\\(p,q\\)) model, including also the AR(\\(p\\)), can be based on (conditional) method of maximum likelihood by making an assumption of the distribution of the error \\(u_t\\). A typical case would be to assume the error term is Gaussian \\(u_t\\sim \\mathsf{nid}(0,\\sigma^2)\\).\n\nIt is worth noting that regardless of the errors being exactly Gaussian or not, the maximum likelihood estimator (MLE) can be interpreted as a quasi-MLE (QMLE) where the possible error in the model specification can be taken into consideration by using a robust parameter covariance matrix in formulation of the standard errors.\nWe will come back to this QMLE interpretation within the context of AR-GARCH model. Similar argumentation as there can be used also in this context (without the GARCH errors).\n\n \nWe will next consider details on so called conditional maximum likelihood technique. The exact method (exact method of maximum likelihood) will be introduced at the end of next section in Extra material.\n \nConditional maximum likelihood method. The (exact) ML estimation of ARMA models requires the necessary selections for the initial values of \\(y_t\\) and \\(u_t\\). By conditioning to the initial values, we get the conditional maximum likelihood technique. If the sample size (length of the time series) is large, the conditional and exact MLEs are close to each other (both lead to the same asymptotic distributions). Moreover, in practice, numerical methods can be used to maximize the log-likelihood function in both cases.\n \nLet us consider a concrete example case of an ARMA(1,1) model \\[\\begin{equation*}\ny_t = \\nu + \\phi_1 y_{t-1} + u_t + \\theta_1 u_{t-1},\n\\end{equation*}\\] where \\(u_t \\thicksim \\mathsf{nid}(0,\\sigma^2)\\). That is, the normality assumption of the error term is assumed. The goal is to estimate the vector of parameters \\(\\boldsymbol{\\vartheta}=(\\nu, \\phi_1, \\theta_1, \\sigma^2)\\).\n\nWe use the notation \\(\\boldsymbol{\\vartheta}\\) (“vartheta”) to denote a general parameter vector containing all the parameters of the model.\nEven though we concentrate on the ARMA(1,1), the following argumentation generalizes readily to more general ARMA processes, which are of the form \\(y_t = \\mathsf{E}_{t-1}(y_t) + u_t\\) where \\(\\mathsf{E}_{t-1}(y_t)\\) corresponds to the employed model structure (systematic part of the model). Here \\(\\mathsf{E}_{t-1}(\\cdot)\\) denotes the conditional expectation and it will be introduced more detail in Forecasting section. In this ARMA(1,1) case, we get \\(\\mathsf{E}_{t-1}(y_t) = \\nu + \\phi_1 y_{t-1} + \\theta_1 u_{t-1}\\), whereas, e.g., in the AR(\\(p\\)) case we get \\(\\mathsf{E}_{t-1}(y_t) = \\nu + \\phi_1 y_{t-1} + \\cdots + \\phi_p y_{t-p}\\).\n\nIn the ARMA(1,1) model, taking the initial values for \\(y_0\\) and \\(u_0\\) as given, the sequence of \\(u_1, \\ldots, u_T\\) can be constructed from \\(y_1, \\ldots, y_T\\) by iterating \\[\\begin{equation*}\nu_t = y_t  - \\nu - \\phi_1 y_{t-1} - \\theta_1 u_{t-1}, \\quad t=1,\\ldots,T.\n\\end{equation*}\\] When conditioning on \\(y_{t-1}\\) and \\(u_{t-1}\\), which are included in the information available at time \\(t-1\\), we end up that the conditional distribution of \\(y_t\\) that is (conditionally) normally distributed due to the normality assumption of \\(u_t\\): \\[\\begin{equation*}\ny_t|(y_{t-1}, u_{t-1}) \\thicksim \\mathsf{N}(\\nu + \\phi_1 y_{t-1} + \\theta_1 u_{t-1}, \\sigma^2).\n\\end{equation*}\\] The specific form of the conditional density function of \\(y_t\\), conditional on \\(y_{t-1}\\) and \\(u_{t-1}\\), is hence \\[\\begin{equation*}\nf_{y_t|y_{t-1}, u_{t-1}}(y_t|y_{t-1}, u_{t-1}; \\boldsymbol{\\vartheta}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\mathrm{exp} \\Big(- \\frac{(y_t - \\nu - \\phi_1 y_{t-1} - \\theta_1 u_{t-1})^2}{2 \\sigma^2}  \\Big).\n\\end{equation*}\\]\n \nConstruction of the log-likelihood function. The (conditional) log-likelihood function can based on the conditional density functions for single observations. Here we are not just restricting ourselves to the ARMA(1,1) and/or Gaussian case. That is the following steps can be generalized also to other modelling choices such as ARMA models.\nDenote \\(\\ubar{\\mathbf{Y}}_{t}= (y_1 \\ldots y_t)\\) and, therefore, the sample of observations is denoted by \\(\\ubar{\\mathbf{Y}}_T=(y_1, \\ldots y_T)\\). The joint conditional density function (without arguments) \\(f_{\\ubar{\\mathbf{Y}}_{T}}(\\ubar{\\mathbf{Y}}_{T})\\) is obtained as the product of conditional density functions \\[\\begin{equation*}\nf_{\\ubar{\\mathbf{Y}}_T}=f_{y_T|\\ubar{\\mathbf{Y}}_{T-1}}\\cdot f_{\\ubar{\\mathbf{Y}}_{T-1}}=f_{y_T|\\ubar{\\mathbf{Y}}_{T-1}} f_{y_{T-1}|\\ubar{\\mathbf{Y}}_{T-2}}\\cdot f_{\\ubar{\\mathbf{Y}}_{T-2}}=\\cdots=\\prod_{t=1}^{T} f_{y_t|\\ubar{\\mathbf{Y}}_{t-1}}\\cdot f_{\\ubar{\\mathbf{Y}}_0}.\n\\end{equation*}\\] By conditioning on \\(\\ubar{\\mathbf{Y}}_0\\), which contains the necessary initial values (depending on, e.g., the lag lengths \\(p\\) and \\(q\\) of the ARMA(\\(p,q\\)) model), the conditional density function gets the form \\[\\begin{equation*}\n    \\prod_{t=1}^{T} f_{y_t|\\ubar{\\mathbf{Y}}_{t-1}},\n\\end{equation*}\\] which then leads to the the (conditional) log-likelihood function \\[\\begin{eqnarray*}\nl(\\boldsymbol{\\vartheta}) = \\sum_{t=1}^{T} \\mathrm{log} \\, f_{y_t|\\ubar{\\mathbf{Y}}_{t-1}}(y_t|\\ubar{\\mathbf{Y}}_{t-1}; \\boldsymbol{\\vartheta}).\n\\end{eqnarray*}\\]\n\nGoing back to the specific example of Gaussian ARMA(1,1) model, the conditional log-likelihood function is hence \\[\\begin{eqnarray*}\nl(\\boldsymbol{\\vartheta}) = -\\frac{T}{2} \\mathrm{log}(2 \\pi) - \\frac{T}{2} \\mathrm{log}(\\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{t=1}^{T}\nu^2_t,\n\\end{eqnarray*}\\] where \\(u_t = y_t - \\nu - \\phi_1 y_{t-1} - \\theta_1 u_{t-1}\\) in this ARMA(1,1) case.\n\nThe maximum likelihood estimate (MLE) of \\(\\boldsymbol{\\vartheta}\\) is obtained by maximizing the conditional log-likelihood function. This requires numerical optimization methods. These numerical (iterative) algorithms require initial values and/or possibly also preliminary estimation of the parameters, but econometric/statistical programs and packages are carrying out these steps straightforwardly.\n \nAR(\\(p\\)) case. We will next show that, in the case of the AR(\\(p\\)) model, the conditional maximum likelihood estimation actually leads to the (conditional) least squares estimates when the error term is Gaussian (\\(u_t \\thicksim \\mathrm{nid}(0, \\sigma^2)\\)).\nSince \\(y_t\\) is a linear combination of \\(u_t, u_{t-1}, \\ldots\\) (given stationarity and corresponding MA(\\(\\infty\\)) representation), it follows that \\(u_s\\) and \\(\\boldsymbol{x}_t\\) are independent \\(\\forall s \\ge t\\), where, as above, \\(\\boldsymbol{x}_t=[1 \\quad y_{t-1} \\, \\cdots \\, y_{t-p}]^{\\prime}\\). By assuming the normality of the error term, we get \\[\\begin{equation*}\n    y_t|\\ubar{\\mathbf{Y}}_{t-1} \\thicksim \\mathsf{N}(\\boldsymbol{x}^{'}_t \\boldsymbol{\\beta}, \\sigma^2),\n\\end{equation*}\\] and the conditional density function of \\(y_t\\) is \\[\\begin{equation*}\n    f_{y_t|\\ubar{\\mathbf{Y}}_{t-1}}(y_t|\\ubar{\\mathbf{Y}}_{t-1}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\mathrm{exp}\\Big(-\\frac{1}{2\\sigma^2}(y_t -\\boldsymbol{x}^{'}_t \\boldsymbol{\\beta})^2 \\Big).\n\\end{equation*}\\] From this we get the (sample) log-likelihood function \\[\\begin{equation*}\nl(\\boldsymbol{\\vartheta}) \\equiv l(\\boldsymbol{\\beta}, \\sigma^2)=-\\frac{T-p}{2} \\mathrm{log}(2\\pi)-\\frac{T-p}{2} \\mathrm{log}(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{t=1}^{T}(y_t-\\boldsymbol{x}^{'}_t \\boldsymbol{\\beta})^2.\n\\end{equation*}\\] where the first term can also be removed because it does not depend on the parameters \\(\\boldsymbol{\\vartheta}\\). This corresponds the (conditional) log-likelihood function of a Gaussian linear regression model, but now for time series observation \\((y_1,\\ldots, y_T)\\). Maximizing this gives us the same conditional least squares estimator \\(\\widehat{\\boldsymbol{\\beta}}\\) (and \\(\\widehat{\\sigma}^2\\)) as obtained above.\n \nExact method of maximum likelihood. The exact log-likelihood function contains also the impact of initial values when constructing the log-likelihood function. Some details on this technique is provided below.\n \n\nExtra: Exact maximum likelihood estimation of ARMA models\n\n\nLet us take a closer look at exact maximum likelihood (ML) based estimation.\nThe exact likelihood function. For the purpose of estimation based on the exact method of maximum likelihood, consider the ARMA(\\(p,q\\)) process \\[\\begin{equation*}\n    y_{t} = \\nu + \\phi_{1}y_{t-1}+\\cdots+\\phi_{p}y_{t-p}+u_{t}+\\theta_{1}u_{t-1}+\\cdots+\\theta_{q}u_{t-q},\\,\\, u_{t}\\sim\\mathsf{nid}\\left(  0,\\sigma^{2}\\right),\n\\end{equation*}\\] and assume the stationarity, invertibility and the identification condition of the ARMA process hold. Suppose \\(\\boldsymbol{y}=\\left(  y_{1},\\ldots,y_{T}\\right)\\) is a vector of observations generated by this process. As before, denote \\(\\boldsymbol{\\phi}=(\\phi_{1},\\ldots,\\phi_{p})\\), \\(\\boldsymbol{\\theta}=\\left(  \\theta_{1},\\ldots,\\theta_{q}\\right)\\) and \\(\\boldsymbol{\\boldsymbol{\\beta}}=(\\boldsymbol{\\phi},\\boldsymbol{\\theta})\\). Making use of the general formulas for the mean and autocovariance function of a linear process, it can be seen that \\(\\mathsf{E}\\left(\\boldsymbol{y}\\right) = \\boldsymbol{0}\\) and \\(\\mathsf{Cov}\\left(  \\boldsymbol{y}\\right)  =\\sigma^{2} \\boldsymbol{\\Sigma}\\), where the (positive definite) matrix \\(\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}\\left(\\boldsymbol{\\boldsymbol{\\beta}}\\right)\\) \\(\\left(  T\\times T\\right)\\) is a function of the parameter \\(\\boldsymbol{\\boldsymbol{\\beta}}\\) (but not of the parameter \\(\\sigma^{2}\\)).\nSince linear transformations of Gaussian distributions are also Gaussian (this holds also for infinite dimensional transformations), \\(\\boldsymbol{y}\\sim \\mathsf{N}\\left(\\boldsymbol{0},\\sigma ^{2} \\boldsymbol{\\Sigma} \\right)\\).\n\n\nIn addition to the constant term, a trend component can be inserted in without affecting the asymptotic properties of the ML-estimators for\\(\\boldsymbol{\\boldsymbol{\\beta}}\\) and \\(\\sigma ^{2}\\).\n\nThe density function of the random vector \\(\\boldsymbol{y}\\) is \\[\\begin{equation*}\n    f_{\\boldsymbol{y}}\\left( \\boldsymbol{y}\\right) =\\left( 2\\pi \\right) ^{-T/2}\\det\\left( \\sigma ^{2}\\boldsymbol{\\Sigma }\\right) ^{-1/2}\\exp\\left\\{ -\\frac{1}{2\\sigma ^{2}}\\boldsymbol{y}^{\\prime}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{y}\\right\\} .\n\\end{equation*}\\] Making use of properties of the determinant, \\(\\det\\left(  \\sigma^{2}\\boldsymbol{\\Sigma}\\right)=\\sigma^{2T}\\det\\left(\\boldsymbol{\\Sigma}\\right)\\), so that the log likelihood function becomes \\[\\begin{equation*}\n    l\\left( \\boldsymbol{\\beta},\\sigma ^{2}\\right) =-\\frac{T}{2}\\log \\sigma ^{2}-\\frac{1}{2}\\log \\det \\left(\\boldsymbol{\\Sigma }\\left( \\boldsymbol{\\beta} \\right)\\right) -\\frac{1}{2\\sigma ^{2}}\\boldsymbol{y}^{\\prime }\\boldsymbol{\\Sigma}\\left(\\boldsymbol{\\beta} \\right) ^{-1}\\boldsymbol{y}.\n\\end{equation*}\\] Because the matrix \\(\\boldsymbol{\\Sigma}\\left(  \\boldsymbol{\\boldsymbol{\\beta}}\\right)\\) is a rather complicated function of the parameter \\(\\boldsymbol{\\boldsymbol{\\beta}}\\), the log-likelihood function cannot be expressed in a simple form and, consequently, the above expression is difficult to be used directly to maximize the likelihood function. In practice, one has to resort to numerical methods to maximize the likelihood function. This involves computing the inverse matrix \\(\\boldsymbol{\\Sigma}\\left(  \\boldsymbol{\\boldsymbol{\\beta}}\\right)  ^{-1}\\) and the determinant \\(\\det\\left(  \\boldsymbol{\\Sigma}\\left(  \\boldsymbol{\\boldsymbol{\\beta}}\\right)\\right)\\) for a range of given values of \\(\\boldsymbol{\\boldsymbol{\\beta}}\\). For a given value of \\(\\boldsymbol{\\boldsymbol{\\beta}}\\), the matrix \\(\\boldsymbol{\\Sigma}\\left(  \\boldsymbol{\\boldsymbol{\\beta}}\\right)\\) can be computed as described in Section @ref(ARMAprocesses).\nMany different algorithms have been proposed to solve the maximization problem described above. \n\n \nStatistical inference. It can be shown that when the stationarity, invertibility, and identification conditions hold, the “usual” asymptototic properties of an maximum likelihood estimator (exact or conditional) hold. In particular, the estimators \\(\\boldsymbol{\\widehat{\\boldsymbol{\\beta}}}\\) and \\(\\widehat{\\sigma}^{2}\\) are both consistent, asymptotically normally distributed and asymptotically independent. In other words, it holds \\[\\begin{equation*}\n    \\boldsymbol{\\widehat{\\boldsymbol{\\beta}}}\\underset{as}{\\sim}\\mathsf{N}\\left(  \\boldsymbol{\\boldsymbol{\\beta}},\\boldsymbol{V}\\left(\\boldsymbol{\\boldsymbol{\\beta}}\\right)  ^{-1}\\right),\n\\end{equation*}\\] where the matrix \\(\\boldsymbol{V}\\left(\\boldsymbol{\\boldsymbol{\\beta}}\\right)=\\mathsf{E}\\left[-\\partial^{2}l(\\boldsymbol{\\boldsymbol{\\beta}},\\sigma^{2})/\\partial\\boldsymbol{\\boldsymbol{\\beta}}\\partial\\boldsymbol{\\boldsymbol{\\beta}}^{\\prime})\\right]\\) is the so-called Fisher information matrix of the parameter \\(\\boldsymbol{\\boldsymbol{\\beta}}\\) (the parameter \\(\\sigma^{2}\\) does not appear in the final expression of \\(\\boldsymbol{V}\\left(\\boldsymbol{\\boldsymbol{\\beta}}\\right)\\) as a result of taking the expectation).\n\nThese asymptotic results hold also without the normality assumption, although in that case the estimators are no longer (asymptotically) efficient. It should be noted that the stationarity, invertibility, and identification conditions are required for the above asymptotic distribution result, which can give a poor approximation also when these conditions are “nearly violated”.\n\n \nHypothesis testing. Using the empirical counterpart of \\(\\boldsymbol{V}\\left(\\boldsymbol{\\boldsymbol{\\beta}}\\right)\\), namely \\(\\widehat{\\boldsymbol{V}}(\\boldsymbol{\\widehat{\\boldsymbol{\\beta}},\\widehat{\\sigma}^{2}})=-\\partial^{2}l(\\boldsymbol{\\widehat{\\boldsymbol{\\beta}}},\\widehat{\\sigma}^{2})/\\partial\\boldsymbol{\\boldsymbol{\\beta}}\\partial\\boldsymbol{\\boldsymbol{\\beta}}^{\\prime}\\) (the so-called empirical or observed information matrix of the parameter \\(\\boldsymbol{\\boldsymbol{\\beta}}\\)), the asymptotic distribution result above can be used to construct statistical Wald tests and confidence intervals concerning the parameter \\(\\boldsymbol{\\boldsymbol{\\beta}}\\)\n\nThe partial derivatives appearing in this expression can in practice be approximated with their numerical counterparts.\n\nIn particular, the square roots of the diagonal elements of the matrix \\(\\widehat{\\boldsymbol{V}}(\\boldsymbol{\\widehat{\\boldsymbol{\\beta}}},\\widehat{\\sigma}^{2}\\boldsymbol{)}^{-1}\\) can be used as approximate standard errors of the components of the estimator \\(\\boldsymbol{\\widehat{\\boldsymbol{\\beta}}}\\). For instance, an individual estimated component of \\(\\boldsymbol{\\widehat{\\boldsymbol{\\beta}}}\\) would be interpreted as “significantly” different from zero if its absolute value is at least 2 (or 1.96) times the size of its approximate standard error. Formally, \\(\\boldsymbol{V}(\\boldsymbol{\\beta}, \\sigma^2)^{-1} = [v^{ij}]\\), its empirical counterpart is \\(\\widehat{\\boldsymbol{V}}(\\widehat{\\boldsymbol{\\beta}}, \\widehat{\\sigma}^2)^{-1} = [\\widehat {v}^{ij}]\\), and \\[\\begin{equation*}\n    \\widehat{\\boldsymbol{\\beta}}_i \\underset{as.}{\\thicksim} \\mathsf{N}(\\boldsymbol{\\beta}_i, v^{ii}).\n\\end{equation*}\\] Thus null hypothesis \\(\\beta_i=0\\) gets rejected at the 5% significance level if the \\(t\\)-ratio (cf. \\(t\\)-test) \\[\\begin{equation*}\n    \\Big|\\frac{\\widehat{\\boldsymbol{\\beta}}_i}{\\sqrt{\\widehat {v}^{ii}}}\\Big|\\ge 1.96 \\Leftrightarrow|\\widehat{\\boldsymbol{\\beta}}_i| \\ge 1.96 \\sqrt{\\widehat {v}^{ii}}\n\\end{equation*}\\] and the 95% confidence intervals for \\(\\beta_i\\) can be obtained by \\[\\begin{equation*}\n    \\widehat{\\beta}_i \\pm 1.96 \\cdot \\mathrm{s.e.}(\\widehat{\\beta}_i) =1.96\\sqrt{\\widehat {v}^{ii}}.\n\\end{equation*}\\] Tests constructed using likelihood ratio principles can also be used in a conventional manner.\n \nEmpirical examples. In the previous section, we concluded an AR(2) model to be one possible candidate for the quarterly U.S real GDP growth (1985:Q1–2007:Q2). The conditional maximum likelihood estimation of this model gives us \\[\\begin{equation*}\n    y_{t} = \\underset{\\left(0.197\\right) }{1.693} + \\underset{\\left(0.101\\right) }{0.160} y_{t-1} + \\underset{\\left(0.101\\right) }{0.287} y_{t-1} + \\widehat{u}_{t}, \\quad \\widehat{\\sigma}^{2}=3.536,\n\\end{equation*}\\] where under the estimates in brackets are the approximate standard errors. Especially the estimate for the AR(2) coefficient is statistically significant at the 5% significance level and hence the model cannot be shrinked. Alternatively, if reporting the estimation result for the demeaned process, we then obtain \\[\\begin{equation*}\n    \\bar{y}_{t} = \\underset{\\left(0.101\\right) }{0.160}\\bar{y}_{t-1} + \\underset{\\left(0.102\\right) }{0.287}\\bar{y}_{t-2} +\\widehat{u}_{t},\n\\end{equation*}\\] where \\(\\bar{y}_{t}=y_{t}-3.061\\) is the demeaned GDP growth series.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameter estimation</span>"
    ]
  },
  {
    "objectID": "TSE-ch7.html",
    "href": "TSE-ch7.html",
    "title": "8  Model selection of ARMA model",
    "section": "",
    "text": "8.1 Starting point\nConsider an observed time series \\(y_{1},\\ldots,y_{T}\\), which is assumed (e.g., judging by its graph) to be generated by a stationary process, and whose modelling as some ARMA(\\(p,q\\)) process appears reasonable.\nFinding a suitable ARMA(\\(p,q\\)) process or, using terminology often used in connection with statistical modelling, an ARMA(\\(p,q\\)) model, traditionally consists of the following stages (that are partially related with each other):\nIf the estimated model is found insufficient in Step 3, one has to select a new model, that is new orders \\(p\\) and \\(q\\), re-estimate the parameters and evaluate sufficiency of the model. As mentioned above, Steps 1–3 are related with each other and are not always performed separately or in this order.\nIn practice, one can never know with certainty whether the true (correct) orders of the ARMA(\\(p,q\\)) model have been found.\nFor these reasons, it is often advisable to follow the principle of parsimony and choose a model that is as simple as possible while being sufficiently large and adequate for the data.\nIn what follows in this section, the error term is assumed to satisfy \\(u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right)\\), and in connection with maximum likelihood estimation, the even stronger assumption \\(u_{t}\\sim\\mathsf{nid}\\left(0,\\sigma^{2}\\right)\\).\nConstant term or demeaning?. As discussed, the mean \\(\\mathsf{E}(y_t)=\\mu\\) is typically nonzero, which means that we need to include a constant term to the model, or alternatively considered demeaned time series. Some clarifying points from empirical modelling perspective:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model selection of ARMA model</span>"
    ]
  },
  {
    "objectID": "TSE-ch7.html#starting-point",
    "href": "TSE-ch7.html#starting-point",
    "title": "8  Model selection of ARMA model",
    "section": "",
    "text": "As has been mentioned above, to achieve this some preliminary transformations may be required, such as via differencing related to ARIMA(\\(p,d,q\\)) and/or nonstationary (unit root) processes (to be introduced later on this material). The most common examples of such transformations are taking differences (potentially combined with taking logarithms) and eliminating a deterministic trend by relevant detrending method.\nIn the case of a process following a linear trend, the model would then be \\(y_{t}= \\nu +  \\delta t + z_{t}\\), \\(t\\) \\(=1,\\ldots,T\\), where \\(z_{t}\\) is an ARMA(\\(p,q\\)) process (cf. the decomposition \\(y_t = \\mu_t + z_t\\)). The idea is to estimate the parameters \\(\\nu\\) and \\(\\delta\\) using least squares, and then move on to consider the obtained series of residuals. From the point of view of parameter estimation, this amounts to modelling the deterministic part via parameters \\(\\nu\\) and \\(\\delta\\) (related to \\(\\mu_t\\))), and the parameters of the ARMA(\\(p,q\\)) process \\(z_{t}\\) separately. An obvious alternative would be to estimate all these parameters jointly.\n\n\n\n\n\nSpecifying (selecting) the model orders \\(p\\) and \\(q\\)\nEstimation of model parameters (including initial values and possibly preliminary estimation of parameters)\nEvaluating the sufficiency of the estimated model by model diagnostic checks and at times forecasting performance\n\n\n\n\nIf the orders are chosen too small, then some aspects of the autocorrelation structure of the time series remain unmodelled, which is obviously not optimal, especially from the strict modelling and perhaps also forecasting perspective. This may lead one to think, to be on the safe side, that it would be best to choose the orders large that there is no possibility of having chosen them too small. However, using orders that are too large leads to inefficiency in parameter estimation. As especially forecasting is based on an estimated model this may often lead to decreasing forecast accuracy.\nIt is particularly harmful if both orders \\(p\\) and \\(q\\) are chosen too large, because this leads to a model in which \\(\\phi_{p}=0=\\theta_{q}\\) so that the model parameters are not identified (see the identification condition of the ARMA processes) and meaningful estimation of them is not possible.\n\n\n\n\n\nUnless otherwise mentioned, it is also assumed that the stationarity and invertibility conditions hold.\n\n\n\n\nThe mean is typically estimated using the sample mean \\(\\bar{y}\\), after which one may consider the centered time series \\(y_{t}-\\bar{y}\\), \\(t=1,\\ldots,T\\), and acts as if \\(\\bar{y}\\) would exactly equal the unknown \\(\\mu\\). This leads to a slight error, but it can be shown that in large samples this error is negligible.\nIf not centering the time series process, you should always include in the constant term to the model equation, as followed in our notation above when introducing different ARMA processes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model selection of ARMA model</span>"
    ]
  },
  {
    "objectID": "TSE-ch7.html#sample-autocorrelations-and-partial-autocorrelations",
    "href": "TSE-ch7.html#sample-autocorrelations-and-partial-autocorrelations",
    "title": "8  Model selection of ARMA model",
    "section": "8.2 Sample autocorrelations and partial autocorrelations",
    "text": "8.2 Sample autocorrelations and partial autocorrelations\nA good second step after visualizing the series is to investigate the autocorrelation structure of the time series using the estimated (sample) autocorrelation function (ACF) and partial autocorrelation function (PACF). In many cases, these functions may contain clues regarding the suitable lag lengths \\(p\\) and \\(q\\) concerning an adequate ARMA(\\(p,q\\)) model.\nIn particular, recall what we have learned about the properties of AR, MA and ARMA processes! That is, one should look for potential breaks (“sudden drops to zero”) in the sample autocorrelation and sample partial autocorrelation functions.\n\nA break in the sample autocorrelation function, but not in the sample partial autocorrelations, suggests an MA model may be a suitable.\nWith an AR case, the properties are completely other way round. That is, we should see a break in the sample partial autocorrelations but not in the sample autocorrelations.\nIf there are no clear breaks (and behavior as described above for MA or AR processes), an ARMA process appears as a likely candidate model.\n\nIn other words, in summary, an AR(\\(p\\)) process is described by an ACF that is infinite in extent (it tails off), but a PACF that is (close to) zero for lags larger than \\(p\\). Moreover, for an MA(\\(q\\)) process we have and ACF that is (close to) zero for lags larger than \\(q\\) and a PACF that is infinite in extent (it tails off). In the absence of either of these two situations, an ARMA model might be an adequate model.\nThis is so called Box-Jenkins model selection procedure. Although quite often this procedure does not point to a single particular candidate model, it may help by ruling out some model alternatives and may be helpful as a tool to be used jointly with other model selection criteria to be introduced below.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model selection of ARMA model</span>"
    ]
  },
  {
    "objectID": "TSE-ch7.html#information-criteria-and-sequential-tests",
    "href": "TSE-ch7.html#information-criteria-and-sequential-tests",
    "title": "8  Model selection of ARMA model",
    "section": "8.3 Information criteria and sequential tests",
    "text": "8.3 Information criteria and sequential tests\nLikewise using sample ACF and PACF, the purpose here is to select the orders \\(p\\) and \\(q\\) of the ARMA(\\(p,q\\)) model after one has first set “sufficiently large” maximum lag lengths \\(p^{\\ast}\\) and \\(q^{\\ast}\\). This preliminary selection can be made, for instance, by making use of the sample autocorrelation and partial autocorrelation functions.\n\nLet \\(\\tilde{\\sigma}_{p,q}^{2}\\) (\\(0\\leq p\\leq p^{\\ast}\\), \\(0\\leq q\\leq q^{\\ast}\\)) be the estimator of the innovation variance \\(\\sigma^{2}\\) of an ARMA(\\(p,q\\)) process.\nSuppose also that the quantity \\(m&gt;\\max\\left(p^{\\ast},q^{\\ast}\\right)\\) used in this estimation method is held constant for all attempted values of \\(p\\) and \\(q\\).\n\nAs discussed, it is preferable to avoid choosing \\(p\\) and \\(q\\) too large.\n\nOne possible way to choose \\(p\\) and \\(q\\) would be to minimize \\(\\tilde{\\sigma}_{p,q}^{2}\\) over the possible values \\(0\\leq p\\leq p^{\\ast}\\), \\(0\\leq q\\leq q^{\\ast}\\). However, this approach does not work, because due to the nature of the least squares or maximum likelihood method, this would lead one to choose orders too large.\n\nTo fix this obvious problem, one approach to select the orders \\(p\\) and \\(q\\) is to minimize the function \\[\\begin{equation}\n    C\\left(p,q\\right)  =\\log\\tilde{\\sigma}_{p,q}^{2}+ \\frac{\\left(p+q+1\\right)  g\\left(  T\\right)}{T}, \\quad 0\\leq p\\leq p^{\\ast},0\\leq q\\leq q^{\\ast},\n\\end{equation}\\] where the so-called penalty function \\(g\\left(\\cdot\\right)\\) is positive valued and satisfies \\(g\\left(  T\\right)  /T\\rightarrow0\\) as \\(T\\rightarrow \\infty\\) (“+1” due to the inclusion of the constant term \\(\\nu\\)). The idea behind the penalty function is to penalize for using an unnecessarily large model. If increasing the order \\(p\\) or \\(q\\) does not make \\(\\tilde{\\sigma}_{p,q}^{2}\\) sufficiently much smaller, then one does not choose the larger model.\nTypically used penalty functions (which have been derived based on different principles) are:\n\nAIC: \\(g\\left( T\\right) =2\\) (Akaike information criterion)\nHQ: \\(g\\left( T\\right) =2\\log (\\log T)\\) (Hannan and Quinn information criterion)\nBIC: \\(g\\left( T\\right) =\\log T\\) (Schwarz information criterion/Bayesian information criterion)\n\nThe first of these (AIC) penalizes the least (favors larger models) and the last (BIC) the most (favors smaller models).\n\nWe note that of the HQ penalty function, there exist also other versions in which the constant 2 has been replaced by some other constants.\n\n \nIn practice, it is often advisable to use the criteria described above only as one tool to help in model selection, and not base the selection only on mechanical minimization of the criterion \\(C\\left(  p,q\\right)\\). In principle, the final selection of the model should always be done after successful parameter estimation (i.e. when there are clearly no suspicious behavior in some estimates) and inspection of diagnostic check to evaluate the sufficiency of the model (see the next subsection).\nHowever, a modern and partly also machine learning perspective, information criteria or equivalent tools are often used quite mechanically to select lag lengths of the ARMA model. Even though not completely advisable, in the era of large datasets, it is not always possible (or reasonable) to perform diagnostic checking in each step.\n \nNeighbouring models. In case of time series that appear difficult to model and select a suitable ARMA(\\(p,q\\)) model, it may be useful to consider as alternatives also models in which lag lengths \\(p\\) and \\(q\\) are one larger than in the model that minimizes the criterion function.\n\nKeeping, however, in mind that selecting both orders too large can lead to identification problems as was mentioned in connection to the identification condition of the ARMA processes\n\nOne may also choose a few models that correspond to the smallest values of the information criterion function (see above), and continue with these models to more detailed investigation. Finally, it is common to use several of the above-mentioned criterion functions simultaneously and see whether they indicate the same model or not.\n \nSequential tests. One way to choose the AR(\\(p\\)) or ARMA(\\(p,q\\)) model orders is based on sequential tests. In what follows, we will consider an AR(\\(p\\)) model for simplicity, but the following procedure can be generalized into ARMA models too.\n\nStart by choosing a relatively large model order \\(p*\\)\nEstimate an AR(\\(p*\\)) model \\[\\begin{equation*}\n       y_t = \\nu + \\phi_1 y_{t-1} + \\cdots + \\phi_{p^*-1} y_{t- p^* +1} + \\phi_{p^*} y_{t-p^*} + u_t.\n  \\end{equation*}\\]\nTest (evaluate) constraint \\(H_0:\\phi_{p*}=0\\)\nIf the null hypothesis cannot be rejected, estimate an AR(\\(p*-1\\)) model, and test for \\(H_0:\\phi_{p*-1}=0\\)\nThis sequential testing procedure is carried out until we can reject the null hypothesis (get the first rejection).\n\nThe sequential testing above can be used mechanically (notice the obvious multiple testing problem and its impact on p-values, but this is often ignored in this context) using \\(t\\)-test or the Wald test statistics or using information criteria.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model selection of ARMA model</span>"
    ]
  },
  {
    "objectID": "TSE-ch7.html#evaluating-the-adequacy-of-the-estimated-model",
    "href": "TSE-ch7.html#evaluating-the-adequacy-of-the-estimated-model",
    "title": "8  Model selection of ARMA model",
    "section": "8.4 Evaluating the adequacy of the estimated model",
    "text": "8.4 Evaluating the adequacy of the estimated model\nIn the previous sections, it is assumed that the lag lengths (orders) \\(p\\) and \\(q\\) of the ARMA(\\(p,q\\)) model have been chosen, in one way or another, that the model is adequate/sufficient. This means that the error term \\(u_{t}\\) satisfies at least the assumption \\(u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right)\\), but at the same time \\(p\\) and \\(q\\) are not, at least simultaneously, unnecessarily large.\n\nIf also assuming the normality of the error term, then satisfying preferably the assumption \\(u_{t}\\sim\\mathsf{nid}\\left(0,\\sigma^{2}\\right)\\)\n\nAs discussed above, one often follows the principle of parsimony when choosing the model, which can sometimes leads to a model in which at least one of the orders has been chosen too small. In such a case, the error term of that selected model would remain autocorrelated, and moving to a larger model to incorporate this remaining correlation may improve, for instance, forecasting performance of the model.\n\n\n \nResiduals. A natural way to investigate the adequacy of an estimated model is to use residuals, whose properties should resemble those of the theoretical error terms \\(u_{t}\\). As with linear regression models, residuals are acquired as the difference of the observed time series (\\(y_t\\)) and its fitted values \\(\\widehat{y}_t\\) of the selected and estimated (ARMA) adequate model \\[\\begin{equation*}\n\\widehat{u}_t = y_t - \\widehat{y}_t.\n\\end{equation*}\\] In practice, one can also perform a further scaling and divide the residuals \\(\\widehat{u}_{t}\\) with their estimated standard error \\(\\widehat{\\sigma}\\). The standardized residuals obtained in this way, \\(\\widehat{u}_{t}/\\widehat{\\sigma}\\), should in the case of a correctly specified model be approximately independent with mean zero and variance one.\n \nGraphical and formal residual analysis. The first step in the investigation of the properties of the residuals is to plot their time series graph. If the estimated model is correct (or more realistically “adequate/sufficient”), the time series of the residuals should not exhibit trends, cyclical components, systematic variation in their level over time, variation of the variance over time (that is, heteroskedasticity), too many outlying observations, etc.\n\nThat is due to the fact that residuals \\(\\widehat{u}_t\\) should resemble assumptions made on the error term \\(u_t\\).\n\nIn the next step, one can investigate whether the residuals are not autocorrelated. This is essentially performed in the same way as testing and investigating whether there is autocorrelation in the observed time series. In this case, the observed time series is the residual series.\n\nIt should be noted, though, that for the autocorrelation coefficients computed from residuals, the critical bounds \\(\\pm1.96/\\sqrt{T}\\) are not valid, not even asymptotically.\nExpressions for the asymptotically correct critical bounds do exist, but are complicated, although some computer programs plot them automatically. If such correct bounds are not (easily) available, the somewhat incorrect bounds \\(\\pm1.96/\\sqrt{T}\\) are sometimes used to give at least a rough measure of the significance of the autocorrelation coefficients computed from the residuals.\n\nIn testing remaining autocorrelation in the residuals, the Ljung-Box test statistic (introduced earlier) can also be used.\n\nHowever, when considering an ARMA(\\(p,q\\)) model with a constant term, instead of the (asymptotic) \\(\\chi_{H}^{2}\\)–distribution used earlier, one should now use the \\(\\chi_{H-p-q-1}^{2}\\)–distribution where one has to choose \\(H\\) “large enough” for this (asymptotic) distribution to be valid.\nIn practice, \\(H\\) is chosen somewhere around 10–20 depending on the sample size \\(T\\). Moreover, typically a couple of different selections of \\(H\\) are considered to see whether the testing results are similar for different selections.\n\n \nPossible (remaining) nonlinearities. Although the errors of the chosen model could be considered to have no autocorrelation, they are not necessarily independent. In line with earlier discussion on possible nonlinearities, one but clearly restricted way of investigating potential nonlinear dependencies in the errors is to use autocorrelations of the squared residuals.\nIt can be shown that when errors satisfy the assumption \\(u_{t}\\sim\\mathsf{nid}\\left(  0,\\sigma^{2}\\right)\\), the sample autocorrelation coefficients of the squared residuals \\(\\widehat{u}_{t}^{2}\\) (\\(t=1,\\ldots.,T\\)) are approximately independent and \\(\\mathsf{N}\\left(  0,1/T\\right)\\)–distributed. Therefore, to sample autocorrelation coefficients of squared residuals \\(\\widehat{u}_{t}^{2}\\), one can apply the critical bounds \\(\\pm1.96/\\sqrt{T}\\) as well as the McLeod-Li test presented (that is, Ljung-Box test applied to squared residuals). \n\n\nIf one notices that the errors are not autocorrelated, but that their squares are autocorrelated, one can also conclude that the errors can not be Gaussian (for Gaussian processes, being not autocorrelated is equivalent with being independent). Although the asymptotic results of the estimators presented in the previous section do hold even without the normality of the errors, it is still useful to investigate how realistic the normality assumption is by checking the histogram of the residuals or using quantile-quantile plot.\nIf the errors are only serially uncorrelated, but not independent, the covariance matrix of the maximum likelihood estimator \\(\\boldsymbol{\\widehat{\\boldsymbol{\\beta}}}\\) is not necessarily the one given in parameter estimation section. The consistency and asymptotic normality of \\(\\boldsymbol{\\widehat{\\boldsymbol{\\beta}}}\\) can still hold under more general assumptions, but the estimated standard errors or test statistics are not necessarily valid.\n\n \nEmpirical example. Let us take a look at the sufficiency of the AR(2) model estimated for the U.S. real GDP growth series. Below we depict the residual time series from the fitted model, as well as their sample autocorrelations and histogram.\n\nIt turns out that there is no (substantial) statistically significant residual autocorrelation left in the residuals. In addition to the sample autocorrelations, the Ljung-Box test statistics for different lag lengths \\(H\\) generally also confirm this conclusion (at least at the 5% or higher significance level).\nAs an example, the Ljung-Box test statistic gets the value \\(17.546\\) from the first 16 lags. That is, to say, \\(H=16\\) and the corresponding \\(p\\)-value from the \\(\\chi_{13}^{2}\\)–distribution is \\(0.176\\) (degrees of freedom: \\(H-p-q-1=13\\).\n\nThe histogram of the residuals suggest that the normality assumption is reasonable (see bottom right). Residual histograms typically strictly matching the density function of the normal distribution (as depicted also in the figure below), but in this case the normality assumption seems relatively adequate.\n\n\n\n\n\n\n\n\n\n\nFigure: The residual time series of the AR(2) model estimated for the US real GDP growth series (upper figure), their estimated autocorrelation function (\\(h=0,\\ldots,20\\), bottom left) and their histogram (bottom right).\n\n \nThe main conclusions from residual diagnostics do not substantially change if using the AIC to select the lag length of the AR model. In the resulting AR(3) model the third lag (\\(y_{t-3}\\)) is not statistically significant predictor in terms of the estimated t-value. Moreover, when considering ARMA models, the AIC selects MA(2) model, with essentially the same conclusions from residual diagnostics as in the AR(2) model.\nWhen considering squared residuals and the McLeod-Li test statistics for different lag lengths, it turns out that basically with any lag length selection the resulting p-values are very high (typically higher than 0.50). Therefore, for the Great Moderation time period in the U.S. economy, there is no initial evidence for strong nonlinearities in the real GDP growth.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model selection of ARMA model</span>"
    ]
  },
  {
    "objectID": "TSE-ch7.html#r-lab",
    "href": "TSE-ch7.html#r-lab",
    "title": "8  Model selection of ARMA model",
    "section": "8.5 R Lab",
    "text": "8.5 R Lab\n\nR Lab, Sections 5-7: ARMA modelling\n\n\n\n# ARMA MODELLING\n\n# U.S. GDP GROWTH ANALYSIS: DETAILED SELECTION & EXTENDED DIAGNOSTICS\n# Final version with restored comparative model selection (AR vs. ARMA)\n# and a dedicated step for the user to choose the final model.\n\n\n#  LOAD LIBRARIES\n# -------------------------------------------------------------------\npackages_to_load &lt;- c(\"tseries\", \"forecast\", \"ggplot2\", \"readxl\", \"dplyr\", \n                      \"lubridate\", \"astsa\", \"TSA\")\nfor (pkg in packages_to_load) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg, character.only = TRUE)\n  }\n}\n\n\n#  DATA ACQUISITION AND PREPARATION\n# -------------------------------------------------------------------\nstart_date &lt;- \"1984-10-01\"\nend_date   &lt;- \"2007-04-01\"\n\nfull_data &lt;- read_excel(\"GDPC1-qdata.xlsx\")\n\nraw_data &lt;- full_data %&gt;%\n  rename(date = Date, value = GDPC1) %&gt;%\n  filter(date &gt;= as.Date(start_date) & date &lt;= as.Date(end_date))\n\ngdp_ts &lt;- ts(raw_data$value, \n             start = c(year(raw_data$date[1]), quarter(raw_data$date[1])), \n             frequency = 4)\n\ngdp_growth &lt;- diff(log(gdp_ts)) * 400  # Annualized growth rate\ngdp_growth &lt;- na.omit(gdp_growth)\n\n\n#  VISUALIZATION AND MANUAL IDENTIFICATION\n# -------------------------------------------------------------------\nautoplot(gdp_growth) + ggtitle(\"(Annualized) U.S. quarterly real GDP growth rate (1985:Q1-2007:Q2)\")\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nAcf(gdp_growth, main=\"ACF of GDP Growth\")  # forecast package\nPacf(gdp_growth, main=\"PACF of GDP Growth\") # forecast package\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n# --- Extended Ljung-Box Test ---\ncat(\"\\nExtended Ljung-Box Test for multiple lags:\\n\")\n\n\nExtended Ljung-Box Test for multiple lags:\n\nlags_to_test &lt;- c(4, 8, 12, 16, 20)\nlb_results &lt;- data.frame(Lag=integer(), \"Q-statistic\"=double(), \"p-value\"=double())\nfor (l in lags_to_test) {\n  test &lt;- Box.test(gdp_growth, type = \"Ljung-Box\", lag = l, fitdf = 0)\n  lb_results[nrow(lb_results) + 1,] &lt;- c(l, round(test$statistic, 3), round(test$p.value, 3))\n}\nprint(lb_results)\n\n  Lag Q.statistic p.value\n1   4      16.487   0.002\n2   8      19.494   0.012\n3  12      27.942   0.006\n4  16      32.897   0.008\n5  20      34.679   0.022\n\n# --- McLeod-Li Test ---\ncat(\"\\nExtended McLeod-Li Test for multiple lags:\\n\")\n\n\nExtended McLeod-Li Test for multiple lags:\n\nml_results &lt;- data.frame(\n  Lag = integer(),\n  p_value = numeric()\n)\n\nfor (l in lags_to_test) {\n  test &lt;- TSA::McLeod.Li.test(y = gdp_growth, gof.lag = l)\n  pval &lt;- test$p.values[l] # Extract p-value for the current lag\n  ml_results[nrow(ml_results) + 1, ] &lt;- c(l, round(pval, 3)) # Append row: lag and p-value\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(ml_results) # This package does not print out the values of the test \n\n  Lag p_value\n1   4   0.342\n2   8   0.564\n3  12   0.533\n4  16   0.688\n5  20   0.717\n\n# statistic (only p-values)\n\n\n\n#  DETAILED MODEL SELECTION (using Conditional MLE)\n# -------------------------------------------------------------------\ncat(\"\\n--- Running Detailed Model Selection ---\\n\")\n\n\n--- Running Detailed Model Selection ---\n\n# --- (i) - Restricting to AR Models Only ---\ncat(\"\\n--- SCENARIO 1: AR-ONLY MODELS ---\\n\")\n\n\n--- SCENARIO 1: AR-ONLY MODELS ---\n\nar_aic &lt;- auto.arima(gdp_growth, seasonal = FALSE, max.q =0, method = \"CSS\", ic = \"aic\")\ncat(\"\\nBest AR model selected by AIC:\\n\")\n\n\nBest AR model selected by AIC:\n\nprint(ar_aic)\n\nSeries: gdp_growth \nARIMA(3,0,0) with non-zero mean \n\nCoefficients:\n         ar1     ar2      ar3    mean\n      0.1855  0.2976  -0.0966  3.0185\ns.e.  0.1037  0.1012   0.1042  0.3184\n\nsigma^2 = 3.458:  log likelihood = -183.02\n\nar_bic &lt;- auto.arima(gdp_growth, seasonal = FALSE, max.q = 0, method = \"CSS\", ic = \"bic\")\ncat(\"\\nBest AR model selected by BIC:\\n\")\n\n\nBest AR model selected by BIC:\n\nprint(ar_bic)\n\nSeries: gdp_growth \nARIMA(2,0,0) with non-zero mean \n\nCoefficients:\n         ar1     ar2    mean\n      0.1599  0.2872  3.0614\ns.e.  0.1010  0.1015  0.3567\n\nsigma^2 = 3.536:  log likelihood = -184.02\n\n# --- (ii) - Allowing ARMA Models ---\ncat(\"\\n--- SCENARIO 2: ARMA MODELS ---\\n\")\n\n\n--- SCENARIO 2: ARMA MODELS ---\n\narma_aic &lt;- auto.arima(gdp_growth, seasonal = FALSE, method = \"CSS\", ic = \"aic\")\ncat(\"\\nBest ARMA model selected by AIC:\\n\")\n\n\nBest ARMA model selected by AIC:\n\nprint(arma_aic)\n\nSeries: gdp_growth \nARIMA(0,0,2) with non-zero mean \n\nCoefficients:\n         ma1     ma2    mean\n      0.2047  0.2998  3.0893\ns.e.  0.1031  0.0964  0.2919\n\nsigma^2 = 3.556:  log likelihood = -183.27\n\narma_bic &lt;- auto.arima(gdp_growth, seasonal = FALSE, method = \"CSS\", ic = \"bic\")\ncat(\"\\nBest ARMA model selected by BIC:\\n\")\n\n\nBest ARMA model selected by BIC:\n\nprint(arma_bic)\n\nSeries: gdp_growth \nARIMA(0,0,0) with non-zero mean \n\nCoefficients:\n        mean\n      3.1001\ns.e.  0.2089\n\nsigma^2 = 3.97:  log likelihood = -189.25\n\n#  CHOOSE YOUR FINAL MODEL\n# -------------------------------------------------------------------\n# After reviewing the 4 models above, assign your choice to 'final_model'.\n# The default is the AR model selected by AIC.\ncat(\"\\n\\n--- Selecting Final Model for Detailed Analysis ---\\n\")\n\n\n\n--- Selecting Final Model for Detailed Analysis ---\n\n#final_model &lt;- ar_aic   # &lt;--- SET YOUR CHOICE HERE\nfinal_model &lt;- ar_bic\n# final_model &lt;- arma_aic\n# final_model &lt;- arma_bic  \n\n\n#  FINAL MODEL ESTIMATION AND DUAL OUTPUT\n# -------------------------------------------------------------------\ncat(\"\\n\\n--- Final Model Estimation Results ---\\n\")\n\n\n\n--- Final Model Estimation Results ---\n\n# --- Output with Mean (µ) ---\ncat(\"\\nVERSION 1: Model Output Reporting the MEAN (µ)\\n\")\n\n\nVERSION 1: Model Output Reporting the MEAN (µ)\n\nprint(summary(final_model))\n\nSeries: gdp_growth \nARIMA(2,0,0) with non-zero mean \n\nCoefficients:\n         ar1     ar2    mean\n      0.1599  0.2872  3.0614\ns.e.  0.1010  0.1015  0.3567\n\nsigma^2 = 3.536:  log likelihood = -184.02\n\nTraining set error measures:\n                       ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 2.690307e-06 1.848811 1.457363 -36.02403 83.33033 0.7123962\n                  ACF1\nTraining set 0.0254192\n\n# --- Output with Constant (\\nu) ---\ncat(\"\\n\\nVERSION 2: Model Output Reporting the CONSTANT (c)\\n\")\n\n\n\nVERSION 2: Model Output Reporting the CONSTANT (c)\n\n# Build the results table from scratch\nestimates &lt;- final_model$coef\nstd_errors &lt;- sqrt(diag(final_model$var.coef))\nt_values &lt;- estimates / std_errors\np_values &lt;- 2 * pnorm(-abs(t_values))\n\nmean_form_table &lt;- data.frame(\n  Estimate = estimates, `Std. Error` = std_errors,\n  `t value` = t_values, `Pr(&gt;|t|)` = p_values, check.names = FALSE\n)\n\nar_coefs &lt;- mean_form_table[grepl(\"^ar\", rownames(mean_form_table)), \"Estimate\"]\nmean_est &lt;- mean_form_table[\"intercept\", \"Estimate\"]\nse_mean &lt;- mean_form_table[\"intercept\", \"Std. Error\"]\n\nconstant_c &lt;- mean_est * (1 - sum(ar_coefs))\nse_constant_approx &lt;- abs(1 - sum(ar_coefs)) * se_mean\nt_val_c &lt;- constant_c / se_constant_approx\np_val_c &lt;- 2 * pnorm(-abs(t_val_c))\n\nconstant_form_table &lt;- mean_form_table[!grepl(\"intercept\", rownames(mean_form_table)), ]\nconstant_row &lt;- data.frame(\n  Estimate = constant_c, `Std. Error` = se_constant_approx,\n  `t value` = t_val_c, `Pr(&gt;|t|)` = p_val_c,\n  row.names = \"constant\", check.names = FALSE\n)\nconstant_form_table &lt;- rbind(constant_row, constant_form_table)\n\ncat(\"\\nDerived Constant-Form Coefficient Table:\\n\")\n\n\nDerived Constant-Form Coefficient Table:\n\nprint(round(constant_form_table, 6))\n\n         Estimate Std. Error  t value Pr(&gt;|t|)\nconstant 1.692506   0.197211 8.582201 0.000000\nar1      0.159899   0.101006 1.583071 0.113405\nar2      0.287243   0.101454 2.831270 0.004636\n\n#  EXTENDED RESIDUAL DIAGNOSTICS\n# -------------------------------------------------------------------\ncat(\"\\n\\n--- Performing Extended Residual Diagnostics on Final Model ---\\n\")\n\n\n\n--- Performing Extended Residual Diagnostics on Final Model ---\n\ncheckresiduals(final_model)\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,0,0) with non-zero mean\nQ* = 6.1896, df = 6, p-value = 0.4023\n\nModel df: 2.   Total lags used: 8\n\nmodel_residuals &lt;- residuals(final_model) # Here for ARIMA model\n\nresid_lagmax &lt;- 16 # Maximum lag length for residual autocorrelations and tests\n\n# Plot ACF without default x-axis\nacf(model_residuals, lag.max = resid_lagmax, main = \"ACF of residuals\", xaxt = \"n\")\n# Manually add x-axis ticks at correct positions\naxis(1, at = 0:resid_lagmax, labels = 0:resid_lagmax * frequency(model_residuals))\n\n\n\n\n\n\n\n# Perform Ljung-Box test\nlb_test &lt;- Box.test(model_residuals, lag = resid_lagmax, type = \"Ljung-Box\", \n                    fitdf = length(final_model$coef))\ncat(\"\\nLjung-Box test p-value:\", round(lb_test$p.value, 3), \"\\n\")\n\n\nLjung-Box test p-value: 0.176 \n\n# --- Extended Ljung-Box Test ---\ncat(\"\\nExtended Ljung-Box Test for multiple lags:\\n\")\n\n\nExtended Ljung-Box Test for multiple lags:\n\nlags_to_test &lt;- c(4, 8, 12, 16, 20)\nlb_results &lt;- data.frame(Lag=integer(), \"Q-statistic\"=double(), \"p-value\"=double())\nfor (l in lags_to_test) {\n  test &lt;- Box.test(model_residuals, type = \"Ljung-Box\", lag = l, fitdf = length(final_model$coef))\n  lb_results[nrow(lb_results) + 1,] &lt;- c(l, round(test$statistic, 3), round(test$p.value, 3))\n}\nprint(lb_results)\n\n  Lag Q.statistic p.value\n1   4       1.899   0.168\n2   8       6.190   0.288\n3  12      15.344   0.082\n4  16      17.546   0.176\n5  20      23.709   0.128\n\n# --- McLeod-Li Test ---\ncat(\"\\nExtended McLeod-Li Test for multiple lags:\\n\")\n\n\nExtended McLeod-Li Test for multiple lags:\n\nml_results &lt;- data.frame(\n  Lag = integer(),\n  p_value = numeric()\n)\n\nfor (l in lags_to_test) {\n  test &lt;- TSA::McLeod.Li.test(y = model_residuals, gof.lag = l)\n  pval &lt;- test$p.values[l] # Extract p-value for the current lag\n  ml_results[nrow(ml_results) + 1, ] &lt;- c(l, round(pval, 3)) # Append row: lag and p-value\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(ml_results) # This package does not print out the values of the test \n\n  Lag p_value\n1   4   0.567\n2   8   0.621\n3  12   0.504\n4  16   0.567\n5  20   0.472\n\n                  # statistic (only p-values)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Model selection of ARMA model</span>"
    ]
  },
  {
    "objectID": "TSE-ch8.html",
    "href": "TSE-ch8.html",
    "title": "9  Forecasting with ARMA models",
    "section": "",
    "text": "9.1 Properties of the conditional expectation\nBefore considering forecasting with an ARMA(\\(p\\),\\(q\\)) process, we examine a general situation where the objective is to forecast the (scalar) random variable \\(Y\\) using the realized value \\(\\boldsymbol{X}=\\boldsymbol{x}\\) of a random vector \\(\\boldsymbol{X}\\). A result from probability theory tells us that the optimal forecast, in the sense of minimized mean square error, is the conditional expected value of \\(Y\\) given \\(\\boldsymbol{X}=\\boldsymbol{x}\\). In other words, \\[\\begin{equation*}\n    \\mathsf{E}\\left[\\left(Y-\\mathsf{E}\\left(Y\\left\\vert \\boldsymbol{X}=\\boldsymbol{x}\\right.\\right)\\right)^{2}\\right]\\leq\\mathsf{E}\\left[\\left(Y-g\\left(\\boldsymbol{x}\\right)\\right)^{2}\\right]\n\\end{equation*}\\] for any function \\(g\\left(\\boldsymbol{x}\\right)\\) of \\(\\boldsymbol{x}\\) (assuming the expectations above are finite).\nThe above definition can be generalized to the case where the random vector \\(\\boldsymbol{X}\\) might be infinite-dimensional. This case will be encountered in the ARMA(\\(p\\),\\(q\\)) context assuming that all the values of the process \\(\\left\\{y_{t},\\text{ }t=0,\\pm1,\\ldots\\right\\}\\) that precede the forecast origin (the time when the forecast is constructed) are known.\nFor our purposes, it suffices to know some simple properties of the conditional expected value (which also hold in the case of an infinite-dimensional conditioning variable). In this course, we will use the following properties of the conditional expected value:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting with ARMA models</span>"
    ]
  },
  {
    "objectID": "TSE-ch8.html#properties-of-the-conditional-expectation",
    "href": "TSE-ch8.html#properties-of-the-conditional-expectation",
    "title": "9  Forecasting with ARMA models",
    "section": "",
    "text": "Extra: Some details on conditional expectation\n\n\nIn the case of continuous distributions, the conditional expectation of \\(Y\\) given \\(\\boldsymbol{X} = \\boldsymbol{x]\\) is defined by the equation \\[\\begin{equation*}\n    \\mathsf{E}\\left(Y\\left\\vert \\boldsymbol{X} = \\boldsymbol{x} \\right.\\right)=\\int_{-\\infty}^{\\infty}yf_{Y|\\boldsymbol{X}}\\left(y; \\boldsymbol{x} \\right)dy=\\int_{-\\infty}^{\\infty}y\\frac{f_{Y,\\boldsymbol{X}} \\left(y,\\boldsymbol{x}\\right)}{f_{\\boldsymbol{X}}\\left(\\boldsymbol{x}\\right)}dy,\n\\end{equation*}\\] where\n\n\\(f_{Y,\\boldsymbol{X}}\\left(y, \\boldsymbol{x}\\right)\\) is the joint density function of the random vector \\(\\left(Y,\\boldsymbol{X} \\right)\\),\n\\(f_{\\boldsymbol{X}}\\left(\\boldsymbol{x} \\right)=\\int_{-\\infty }^{\\infty}f_{Y,\\boldsymbol{X}}\\left(y,\\boldsymbol{x}\\right) dy\\) is the marginal density function of \\(\\boldsymbol{X}\\), and\n\\(f_{Y|\\boldsymbol{X}}\\left(y;\\boldsymbol{x}\\right)=f_{Y,\\boldsymbol{X}}\\left(y,\\boldsymbol{x} \\right)/f_{\\boldsymbol{X}}\\left(\\boldsymbol{x} \\right)\\) defines the conditional density function of \\(Y\\) given \\(\\boldsymbol{X} = \\boldsymbol{x}\\).\n\nWhen \\(\\boldsymbol{x}\\) varies over the possible values of the random vector \\(\\boldsymbol{X}\\), \\(\\mathsf{E}\\left(Y\\left\\vert \\boldsymbol{X} = \\boldsymbol{x} \\right.\\right)\\) as a function of \\(\\boldsymbol{x}\\) defines a random variable for which it is natural to use the notation \\(\\mathsf{E}\\left(Y\\left\\vert \\boldsymbol{X} \\right.\\right)\\).\n\n\n\n\n\nCEV1: \\(\\mathsf{E}\\left(aY_{1}+bY_{2}\\left\\vert \\boldsymbol{X} \\right.\\right)=a\\mathsf{E}\\left(Y_{1}\\left\\vert \\boldsymbol{X} \\right.\\right) +b\\mathsf{E}\\left(Y_{2}\\left\\vert \\boldsymbol{X} \\right.\\right)\\), when \\(a\\) are \\(b\\) constants.\nCEV2: \\(\\mathsf{E}\\left(Y\\left\\vert \\boldsymbol{X} \\right.  \\right)=\\mathsf{E}\\left(Y\\right)\\), when \\(Y\\) and \\(\\boldsymbol{X}\\) are independent random variables.\nCEV3: \\(\\mathsf{E}\\left(Y\\right)=\\mathsf{E}\\left[\\mathsf{E}\\left(Y\\left\\vert \\boldsymbol{X} \\right.\\right)\\right]\\) (so-called law of iterated expectations)\nCEV4: \\(\\mathsf{E}\\left[h\\left(\\boldsymbol{X} \\right)Y\\left\\vert \\boldsymbol{X} \\right.\\right]=h\\left(\\boldsymbol{X} \\right)\\mathsf{E}\\left(Y\\left\\vert \\boldsymbol{X} \\right.\\right)\\) for any function \\(h\\) (assuming the expected value of \\(h\\left(\\boldsymbol{X}\\right)Y\\) exists and is finite).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting with ARMA models</span>"
    ]
  },
  {
    "objectID": "TSE-ch8.html#ennusteidenmuodostus",
    "href": "TSE-ch8.html#ennusteidenmuodostus",
    "title": "9  Forecasting with ARMA models",
    "section": "9.2 Forming forecasts",
    "text": "9.2 Forming forecasts\nForecasting an ARMA(\\(p\\),\\(q\\)) process. As the discussion above suggests, we consider forecasting an ARMA(\\(p\\),\\(q\\)) process assuming that the entire infinitely long history of the process is known.\n\n\n\nIn what follows, we again assume that the constant term is included in the model equation to control a nonzero mean of \\(y_t\\) (i.e. \\(\\mathsf{E}(y_t) = \\mu \\neq 0\\)).\n\nWhen we are interested in forecasting the levels of \\(y_t\\), which is often and typically the case, it is important to consider how forecasts will be obtained for original levels if using demeaning of the time series and/or extracting a trend component. That is, all the possible transformations must be carefully taken into account in forecast computations.\nWe will also assume that the ARMA(\\(p\\),\\(q\\)) process under consideration is stationary and invertible and that the innovation term satisfies the condition \\(u_{t}\\sim\\mathsf{iid}\\left(0,\\sigma^{2}\\right)\\).\n\nIn the calculations that follow, we repeatedly make use of the above-mentioned properties of the conditional expected value (CEV1–CEV4).\n \nAR(1) case. Consider forecasting an AR(\\(p\\)) process, and for simplicity, we first also assume that \\(p=1\\).\n\nIn one-step forecasting, the aim is to forecast the value \\(y_{t+1}\\), when the preceding history \\(\\left\\{y_{t},y_{t-1},\\ldots\\right\\}\\) of the process is known.\nMore generally, in \\(h\\)-step forecasting, the aim is to forecast the value of \\(y_{t+h}\\), when \\(\\left\\{y_{t},y_{t-1},\\ldots\\right\\}\\) is known.\n\nFor brevity, denote the conditional expectation as \\[\\begin{equation*}\n\\mathsf{E}\\left(y_{t+h}\\left\\vert y_{s},\\text{ }s\\leq t\\right.\\right)=\\mathsf{E}_{t}\\left(  y_{t+h}\\right), \\quad \\left(h\\geq1\\right).\n\\end{equation*}\\] Taking conditional expectations of both sides of the AR(1) equation (forwarded first by one period) \\[\\begin{equation*}\n    y_{t+1} = \\nu + \\phi_{1}y_{t}+u_{t+1}\n\\end{equation*}\\] leads to (see CEV1) \\[\\begin{equation*}\n   \\mathsf{E}_{t}\\left(y_{t+1}\\right) = \\nu + \\phi_{1}\\mathsf{E}_{t}\\left(y_{t}\\right)+\\mathsf{E}_{t}\\left(u_{t+1}\\right).\n\\end{equation*}\\]\n\nUnder the stationarity condition \\(\\left\\vert \\phi_{1}\\right\\vert&lt;1\\), the variables \\(y_{t},y_{t-1},\\ldots\\) depend only on the variables \\(u_{t},u_{t-1},\\ldots\\). Recall the MA(\\(\\infty\\)) representation of \\(y_t\\) to see this.\nTherefore, in the conditional expectation \\(\\mathsf{E}_{t}\\left(u_{t+1}\\right)\\), the conditioning random variables \\(\\left\\{y_{s},s\\leq t\\right\\}\\) are independent of \\(u_{t+1}\\). Following the property CEV2, we then obtain \\[\\begin{equation*}\n\\mathsf{E}_{t}\\left(u_{t+1}\\right)=\\mathsf{E}\\left(u_{t+1}\\right)=0.\n\\end{equation*}\\] More generally it holds \\[\\begin{equation*}\n\\mathsf{E}_{t}\\left(u_{t+k}\\right)=\\mathsf{E}\\left(u_{t+k}\\right)=0, \\quad k \\ge 1.\n\\end{equation*}\\]\nMoreover, \\(\\mathsf{E}_{t}\\left(y_{t}\\right)=y_{t}\\) (see CEV4: \\(y_t\\) is included in the information set at time \\(t\\)).\n\nPutting the above results together, we obtain \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(y_{t+1}\\right) = \\nu + \\phi_{1}y_{t}.\n\\end{equation*}\\]\nWhen forecasting \\(y_{t+2}\\), we obtain \\[\\begin{eqnarray*}\n    \\mathsf{E}_{t}\\left(y_{t+2}\\right) &=& \\nu + \\phi_{1}\\mathsf{E}_{t}\\left(y_{t+1}\\right)+\\mathsf{E}_{t}\\left(u_{t+2}\\right) \\\\\n    &=& \\nu + \\phi_{1}\\mathsf{E}_{t}\\left(y_{t+1}\\right) \\\\\n    &=& (1+\\phi_1) \\nu + \\phi_{1}^{2}y_{t},\n\\end{eqnarray*}\\] and inductively (given \\(|\\phi_1|&lt;1\\)) \\(h\\)-period forecast \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(y_{t+h}\\right)= (1+\\phi_1 + \\cdots + \\phi^h_1) \\nu + \\phi_{1}^{h}y_{t} =\n    \\mu + \\phi_{1}^{h}y_{t}.\n\\end{equation*}\\]\n \nAR(\\(p\\)) case. In the case of a general AR(\\(p\\)) process, we use a similar steps to obtain forecasts. Taking conditional expectations of both sides of \\[\\begin{equation*}\n    y_{t+1} = \\nu + \\phi_{1}y_{t}+\\cdots+\\phi_{p}y_{t+1-p}+u_{t+1},\n\\end{equation*}\\] and using similar arguments as in the case \\(p=1\\), including the results \\(\\mathsf{E}_{t}\\left(u_{t+1}\\right)=0\\) and \\(\\mathsf{E}_{t}\\left(y_{t-j}\\right)=y_{t-j}\\) \\(\\left(j\\geq0\\right)\\), we obtain \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(y_{t+1}\\right) = \\nu + \\phi_{1}y_{t}+\\cdots+\\phi_{p}y_{t+1-p}.\n\\end{equation*}\\] When forecasting \\(y_{t+2}\\), we similarly obtain \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(y_{t+2}\\right)= \\nu + \\phi_{1}\\mathsf{E}_{t}(y_{t+1})+\\phi_{2}y_{t}+\\cdots+\\phi_{p}y_{t+2-p},\n\\end{equation*}\\] where the conditional expected value on the right hand side could be replaced with the expression obtained for it above.\n\nIn other words, the variable \\(y_{t+1}\\), unknown at time \\(t\\), has been replaced with its forecast \\(\\mathsf{E}_{t}(y_{t+1})\\).\nThe variables \\(y_{t},\\ldots,y_{t+2-p}\\) are known at time \\(t\\). Therefore, they remain on the right hand side as they are included in the information set at the forecast origin.\n\nInductively, it is straightforward to see that the above generalizes to forecasting \\(h\\) periods ahead \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(y_{t+h}\\right) = \\nu + \\phi_{1}\\mathsf{E}_{t}(y_{t+h-1})+\\phi_{2}\\mathsf{E}_{t}(y_{t+h-2})+\\cdots+\\phi_{p}\\mathsf{E}_{t}(y_{t+h-p}), \\quad h\\geq1,\n\\end{equation*}\\] where \\(\\mathsf{E}_{t}\\left(y_{t+h-j}\\right)=y_{t+h-j}\\) for \\(h\\leq j\\). This means that forecasts can be computed recursively, starting with the one-step-ahead case \\(h=1\\), and proceeding one step at a time to the forecast horizons \\(h=2\\), \\(h=3,\\ldots\\).\n \nARMA(\\(p,q\\)) case. To obtain forecasting formulae for an ARMA(\\(p\\),\\(q\\)) process is quite similar to forecasting with an AR(\\(p\\)) process.\n\nWhen the stationarity condition holds, \\(y_{t}-\\mu = \\sum_{j=0}^{\\infty}\\psi_{j}u_{t-j}\\), from which it follows that \\(u_{t+i}\\), \\(i\\geq1\\), is independent of the variables \\(\\left\\{y_{t},y_{t-1},\\ldots\\right\\}\\).\nTherefore, \\(\\mathsf{E}_{t}(u_{t+i})=\\mathsf{E}(u_{t+i})=0\\) for all \\(i\\geq0\\) (see CEV2).\nOn the other hand, when the invertibility condition holds, \\(u_{t}=\\sum_{j=0}^{\\infty}\\pi_{j}(y_{t-j}-\\mu)\\), and therefore \\(\\mathsf{E}_{t}(u_{t-i})=u_{t-i}\\) for all \\(i\\geq0\\) (see CEV4).\n\nTaking conditional expectations of both sides of the equation defining an ARMA(\\(p\\),\\(q\\)) and using the above-mentioned properties, we obtain \\(h\\)-period ahead forecast \\[\\begin{eqnarray*}\n    \\mathsf{E}_{t}(y_{t+h}) &=& \\nu +  \\phi_{1}\\mathsf{E}_{t}(y_{t+h-1})+\\cdots+\\phi_{p}\\mathsf{E}_{t}(y_{t+h-p})\n    +  \\theta_{1}\\mathsf{E}_{t}(u_{t+h-1})+\\cdots+\\theta_{q}\\mathsf{E}_{t}(u_{t+h-q}), \\quad h \\geq 1,\n\\end{eqnarray*}\\] where \\(\\mathsf{E}_{t}\\left(y_{t+h-j}\\right)=y_{t+h-j}\\) for \\(h\\leq j\\), and \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(u_{t+h-j}\\right)=\\left\\{\n    \\begin{array}{cc}\n         &u _{t+h-j}, \\,\\, \\mathrm{when} \\,\\, h\\leq j, \\\\\n         &0, \\,\\, \\mathrm{when}\\,\\, h&gt;j.\n    \\end{array}\n    \\right.\n\\end{equation*}\\] Using this forecast formula, forecasts can again be computed recursively, starting with the one-step-ahead case \\(h=1\\), and proceeding one step at a time to the cases \\(h=2,\\) \\(h=3,\\ldots\\).\n \nThe discussion above makes the unrealistic assumption that the parameters of the examined process are known.\n\nIn practice, unknown parameters are replaced by their estimates, and in this case the above-mentioned optimality of forecasts (forecast construction) only holds approximately.\n\nMoreover, for ARMA models, in practice the innovation terms appearing in the forecasting formulae above (not in the AR(\\(p\\)) case) have to be computed using the observed time series, so in the formula \\(u_{t}=\\sum_{j=0}^{\\infty}\\pi_{j}(y_{t-j}-\\mu)\\) we have to truncate the sum at some point (e.g., \\(\\sum_{j=0}^{t-1}\\pi_{j} (y_{t-j}-\\mu)\\)).\n\nA popular alternative is to calculate \\(u_{t}\\) using the difference equation \\[\\begin{equation*}\n  u_{t}=y_{t} - \\nu - \\phi_{1}y_{t-1}-\\cdots-\\phi_{p}y_{t-p}-\\theta_{1}u_{t-1}-\\cdots-\\theta_{q}u_{t-q}, \\quad t=1,2,\\ldots,\n\\end{equation*}\\] where the initial values \\(y_{0},\\ldots,y_{-p}\\) can be assumed known (that is, observed), and the initial values \\(u_{0},\\ldots,u_{-q}\\) can be chosen to be \\(u_{0}=\\cdots=u_{-q}=0\\) (that is, the unconditional mean of the \\(u_{t}\\)). When \\(t\\) is large, the effect of initial values is negligible.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting with ARMA models</span>"
    ]
  },
  {
    "objectID": "TSE-ch8.html#prediction-intervals",
    "href": "TSE-ch8.html#prediction-intervals",
    "title": "9  Forecasting with ARMA models",
    "section": "9.3 Prediction intervals",
    "text": "9.3 Prediction intervals\n\nAn alternative way to approach forecasting an ARMA(\\(p\\),\\(q\\)) process is via the MA\\(\\left(\\infty\\right)\\) representation. Taking conditional expectations of both sides of the equation \\[\\begin{equation*}\n    y_{t+h}= \\mu + \\sum_{j=0}^{\\infty}\\psi_{j}u_{t+h-j},\n\\end{equation*}\\] and using results of conditional expectations used above, we obtain \\[\\begin{equation*}\n    \\mathsf{E}_{t}(y_{t+h}) = \\mu + \\sum_{j=h}^{\\infty}\\psi_{j}u_{t+h-j}.\n\\end{equation*}\\] This formula is not useful in practice, but it is convenient for investigating the properties of the forecast error \\(y_{t+h}-\\mathsf{E}_{t}(y_{t+h})\\). From the two equations above, we obtain \\[\\begin{equation*}\n    y_{t+h}-\\mathsf{E}_{t}(y_{t+h})=\\sum_{j=0}^{h-1}\\psi_{j}u_{t+h-j}.\n\\end{equation*}\\] Note, in particular, that the forecast error of the one-step forecast is \\(u_{t+1}\\).\n\nTaking (unconditional) expectations from both sides of the last equation shows that the forecast \\(\\mathsf{E}_{t}(y_{t+h})\\) is unbiased in the sense that the forecast error has an expected value of zero: \\[\\begin{equation*}\n\\mathsf{E}\\left[y_{t+h}-\\mathsf{E}_{t}(y_{t+h})\\right]=0.\n\\end{equation*}\\]\nFurthermore, we can compute the variance of the forecast error. With straightforward computation, we obtain \\[\\begin{equation*}\n  \\mathsf{Var}\\Big(y_{t+h}-\\mathsf{E}_{t}(y_{t+h})\\Big)=\\sigma^{2}\\sum_{j=0}^{h-1}\\psi_{j}^{2}\\equiv\\sigma_{h}^{2}.\n\\end{equation*}\\]\nImportantly, note that as \\(h\\) increases, the forecast converges (in mean square) to the mean of the process, whereas the variance of the forecast error \\(\\sigma_{h}^{2}\\) converges to the variance of the process being predicted (\\(y_{t}\\)), namely \\(\\sigma^{2}\\sum_{j=0}^{\\infty}\\psi_{j}^{2}\\).\n\n \nIf one assumes that \\(u_{t}\\sim\\mathsf{nid}\\left(0,\\sigma^{2}\\right)\\), then \\[\\begin{equation*}\n    y_{t+h}-\\mathsf{E}_{t}(y_{t+h})\\sim\\mathsf{N}\\left(0,\\sigma_{h}^{2}\\right)\n\\end{equation*}\\] and, therefore, \\(y_{t+h}\\) is contained in the interval \\[\\begin{equation*}\n\\mathsf{E}_{t}(y_{t+h})\\pm1.96\\sigma_{h}\n\\end{equation*}\\] with 95% probability (in repeated sampling). In practice, the unknown parameters in the forecast \\(\\mathsf{E}_{t}(y_{t+h})\\) and in the standard deviation \\(\\sigma_{h}\\) are replaced by their estimates, and hence the normality of the forecast errors hold only approximately.\n \nEmpirical example (continue). In the previous two sections, we concluded that an AR(2) model to be one possible candidate for the U.S real GDP growth (1985:Q1–2007:Q2). The estimated AR(2) model \\[\\begin{equation*}\n    y_{t} = \\underset{\\left(0.197\\right) }{1.693} + \\underset{\\left(0.101\\right) }{0.160} y_{t-1} + \\underset{\\left(0.101\\right) }{0.287} y_{t-1} + \\widehat{u}_{t}, \\quad \\widehat{\\sigma}^{2}=3.536,\n\\end{equation*}\\] where under the estimates in brackets are the approximate standard errors. Therefore, if now computing 8-quarter-ahead predictions starting from the last observation (2007:Q2) are obtained with the forecasting formulae above applied in the case of \\(p=2\\) (and \\(q=0\\)) and \\(h=8\\). The resulting (point) forecasts of the estimated AR(2) model and their 80 and 95% confidence intervals are presented below.\n        Point Forecast      Lo 80    Hi 80      Lo 95    Hi 95\n2007 Q3       2.427727 0.01787587 4.837578 -1.2578223 6.113277\n2007 Q4       2.781453 0.34098863 5.221917 -0.9509150 6.513820\n2008 Q1       2.834604 0.28036903 5.388840 -1.0717615 6.740971\n2008 Q2       2.944709 0.38002888 5.509388 -0.9776305 6.867048\n2008 Q3       2.977582 0.40040373 5.554759 -0.9638718 6.919035\n2008 Q4       3.014464 0.43506842 5.593860 -0.9303814 6.959310\n2009 Q1       3.029805 0.44884169 5.610767 -0.9174375 6.977047\n2009 Q2       3.042852 0.46149492 5.624209 -0.9049929 6.990696\nThe figure below depicts the 8-step forecasts and the confidence intervals. As we can see, the forecast converges towards the mean (3.10) when the forecast horizon lengthens. The confidence intervals are quite broad.\n\n\n\n\n\n\n\n\n\n\nFigure: 8-step-ahead forecasts for the U.S. real GDP growth and their 80 and 95% confidence intervals from the AR(2) process.\n\n \nObviously these forecasts, constructed at 2007:Q2, for the years 2008 and 2009 turned out to be too optimistic due to the Great Financial Crisis (2008–2009) and the worst recession in almost 100 years occurred in that time! It can generally be concluded that there were no econometric models able to predict such a depression. Therefore, these forecasts can rather be seen as a scenario type of forecast how the U.S. economy could have been evolved without the financial crisis.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting with ARMA models</span>"
    ]
  },
  {
    "objectID": "TSE-ch8.html#introduction-to-out-of-sample-forecasting",
    "href": "TSE-ch8.html#introduction-to-out-of-sample-forecasting",
    "title": "9  Forecasting with ARMA models",
    "section": "9.4 Introduction to out-of-sample forecasting",
    "text": "9.4 Introduction to out-of-sample forecasting\nIn various fields, particularly economics and finance, it is crucial to assess the reliability and accuracy of forecasts generated by estimated time series models. A popular method for this evaluation is (pseudo) out-of-sample forecasting. This approach aims to mimic a real-time forecasting scenario as closely as possible, using only information that would have been available at the time the forecast was made.\nThe fundamental idea is to evaluate the forecast performance by systematically reserving a portion of the data, unknown to the model during its estimation, to serve as a “future” against which forecasts are compared. This method is termed at times “pseudo” out-of-sample forecasting because, while we are simulating real-time forecasting, we do have full knowledge of the entire dataset beforehand in our analysis. In a “true” real-time forecasting, future observations are, by definition, unknown.\n\nNotice that backtesting in the financial industry is essentially a form of out-of-sample forecasting applied specifically to financial data and trading strategies. While the underlying principle is the same, the terminology and focus can differ slightly. While a time series analyst might use out-of-sample forecasting to evaluate how accurately they can predict a stock’s price, a financial analyst would use backtesting to see if a strategy based on those predictions would have actually made money.\n\n \nThe out-of-sample forecasting process generally involves the following steps:\n\nDefine initial estimation sample: An initial segment of the time series, say the first \\(T\\) observations (\\(y_1, \\dots, y_T\\)), is used to estimate the model’s parameters. This period is often referred to as the training sample or estimation sample.\nDefine forecasting horizon (\\(h\\)): Like above where we introduced how to construct forecasts with ARMA models, we specify how many periods ahead we want to forecast (e.g., \\(h=1\\) for one-period-ahead, \\(h=2\\) for two-periods-ahead, etc.).\nDefine forecasting evaluation period: A subsequent segment of the data, known as the test sample or forecasting sample, is set aside. This period spans from \\(y_{T+1}\\) up to \\(y_{T+m+h-1}\\), where \\(m\\) is the number of distinct forecast origins (i.e., the number of times we re-estimate the model and generate a new \\(h\\)-step forecast).\nIterative forecasting and parameter updating: The core of (pseudo) out-of-sample forecasting is its iterative nature. We start by estimating the model using the initial \\(T\\) observations, then we forecast \\(h\\) periods ahead. We then “advance time” by one period, potentially re-estimate the model with updated information, and generate a new \\(h\\)-period-ahead forecast. This process is repeated \\(m\\) times.\n\n \nLet’s clarify the notation with an example. Suppose we want to evaluate two-period-ahead forecasts (\\(h=2\\)).\n\nLet \\(T\\) denote the end of our initial estimation sample.\nLet \\(j\\) be an index for the forecast origin, running from \\(j=0, 1, \\dots, m-1\\). For simplicity, let \\(m=4\\).\n\nThe sequence of forecasts at different forecast origins and their corresponding actual values would be:\n\nForecast origin \\(T\\) (i.e., \\(j=0\\)):\n\nEstimate the model using data \\(y_1, \\dots, y_T\\).\nGenerate a \\(h=2\\) period forecast, denoted \\(\\widehat{y}_{T+2|T}\\).\nThis forecast is compared against the actual observation \\(y_{T+2}\\).\nThe forecast error is \\(\\widehat{e}_{T+2} = y_{T+2} - \\widehat{y}_{T+2|T}\\).\n\nForecast origin \\(T+1\\) (i.e., \\(j=1\\)):\n\nUpdate the estimation sample. This is where rolling vs. expanding parameter updating window (see below) becomes relevant.\nGenerate a \\(h=2\\) period forecast, denoted \\(\\widehat{y}_{T+3|T+1}\\).\nThis forecast is compared against the actual observation \\(y_{T+3}\\).\nThe forecast error is \\(\\widehat{e}_{T+3} = y_{T+3} - \\widehat{y}_{T+3|T+1}\\).\n\nForecast origin \\(T+2\\) (i.e., \\(j=2\\)):\n\nUpdate the estimation sample.\nGenerate a \\(h=2\\) period forecast, denoted \\(\\widehat{y}_{T+4|T+2}\\).\nThis forecast is compared against the actual observation \\(y_{T+4}\\).\nThe forecast error is \\(\\widehat{e}_{T+4} = y_{T+4} - \\widehat{y}_{T+4|T+2}\\).\n\nForecast origin \\(T+3\\) (i.e., \\(j=3\\)):\n\nUpdate the estimation sample.\nGenerate a \\(h=2\\) period forecast, denoted \\(\\widehat{y}_{T+5|T+3}\\).\nThis forecast is compared against the actual observation \\(y_{T+5}\\).\nThe forecast error is \\(\\widehat{e}_{T+5} = y_{T+5} - \\widehat{y}_{T+5|T+3}\\).\n\n\nIn general, for \\(j=0, 1, \\dots, m-1\\), we compute the \\(h\\)-period forecast from the information available at \\(T+j\\), denoted \\(\\widehat{y}_{T+h+j|T+j}\\). The corresponding forecast error is: \\[\\begin{equation*}\n\\widehat{e}_{T+h+j} = y_{T+h+j} - \\widehat{y}_{T+h+j|T+j}\n\\end{equation*}\\]\n \nParameter updating strategies. In a real forecasting situation, the information set used for model estimation typically includes the most recent observations. The way this information set is updated over time defines different parameter estimation strategies:\n\nExpanding window: In this approach, the estimation sample grows by one observation in each iteration. If the initial estimation sample is \\(y_1, \\dots, y_T\\), the next estimation sample will be \\(y_1, \\dots, y_{T+1}\\), then \\(y_1, \\dots, y_{T+2}\\), and so on. This means older observations are always retained. This is suitable when one believes that all past data is equally relevant for estimating the current parameters.\nRolling window: Here, the estimation sample has a fixed size (e.g., \\(T\\) observations) and “rolls” forward by one observation in each iteration. If the initial sample is \\(y_1, \\dots, y_T\\), the next sample will be \\(y_2, \\dots, y_{T+1}\\), then \\(y_3, \\dots, y_{T+2}\\), and so on. This approach gives more weight to recent observations and is useful when model parameters are suspected to change over time (i.e., the presence of structural breaks or regime changes).\n\nThe accompanying R code (R Lab below) allows you to select either a rolling or expanding window strategy using the window_type parameter.\n \nEvaluating forecast accuracy. Once the forecast errors (\\(\\widehat{e}_{T+h+j}\\)) for the entire test sample are computed, a typical measure for evaluating forecast accuracy is the Mean Squared Forecast Error (MSFE): \\[\\begin{equation*}\nMSFE = \\frac{1}{m} \\sum_{j=0}^{m-1} \\widehat{e}_{T+h+j}^2.\n\\end{equation*}\\] The MSFE penalizes larger errors more heavily due to the squaring forecast errors. Its square root, the Root Mean Squared Forecast Error (RMSFE), is also commonly used as it is in the same units as the original series, making it easier to interpret.\nAnother popular alternative is the Mean Absolute Forecast Error (MAFE): \\[\\begin{equation*}\nMAFE = \\frac{1}{m} \\sum_{j=0}^{m-1} |\\widehat{e}_{T+h+j}|.\n\\end{equation*}\\] The MAFE is less sensitive to outliers than the MSFE.\nThe objective of this exercise is to identify the model(s) that produce the smallest MSFE and/or MAFE values, indicating superior forecasting performance.\n \nEmpirical example (continue). Let us continue an illustration of out-of-sample forecasting of the quarterly U.S. real GDP growth. Assume that we start out-of-sample forecasting in 2010:Q1 and construct one (\\(h=1\\)) and four (\\(h=4\\)) quarter out-of-sample forecasts for the forecasting sample (test) sample period 2010:Q1–2019:Q4. We compare the performance of AR(1), AR(2), AR(3) and AR(4) models.\n\nThe first estimation sample is between 1985:Q1–2009:Q4.\nObviously, based on the previous results (for the sample period excluding the Great Financial Crisis) suggest the AR(2) model.\nParameter updating is done via expanding and rolling window approaches separately.\nMSFEs suggest that except \\(h=4\\) horizon and rolling window approach (there AR(4) model), AR(3) model yields the smallest MSFE in different comparisons.\n\n\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=1, expanding window) ---\n$`AR(1)`\n[1] 3.010018\n$`AR(2)`\n[1] 2.810865\n$`AR(3)`\n[1] 2.760154\n$`AR(4)`\n[1] 3.451758\n\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=1, rolling window) ---\n$`AR(1)`\n[1] 3.032567\n$`AR(2)`\n[1] 2.793521\n$`AR(3)`\n[1] 2.731007\n$`AR(4)`\n[1] 3.481733\n\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=4, expanding window) ---\n$`AR(1)`\n[1] 2.628715\n$`AR(2)`\n[1] 2.662022\n$`AR(3)`\n[1] 2.457973\n$`AR(4)`\n[1] 2.467844\n\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=4, rolling window) ---\n$`AR(1)`\n[1] 2.602836\n$`AR(2)`\n[1] 2.633853\n$`AR(3)`\n[1] 2.496958\n$`AR(4)`\n[1] 2.478106",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting with ARMA models</span>"
    ]
  },
  {
    "objectID": "TSE-ch8.html#r-lab",
    "href": "TSE-ch8.html#r-lab",
    "title": "9  Forecasting with ARMA models",
    "section": "9.5 R Lab",
    "text": "9.5 R Lab\nAll the R codes considered in this section are compiled in the following links.\n\nR Lab: Forecast computation in ARMA models\n\n\n\n#  FORECASTING\n# -------------------------------------------------------------------\n\n# ARMA MODELLING\n\n# U.S. GDP GROWTH ANALYSIS: DETAILED SELECTION & EXTENDED DIAGNOSTICS\n# Final version with restored comparative model selection (AR vs. ARMA)\n# and a dedicated step for the user to choose the final model.\n\n\n#  LOAD LIBRARIES\n# -------------------------------------------------------------------\npackages_to_load &lt;- c(\"tseries\", \"forecast\", \"ggplot2\", \"readxl\", \"dplyr\", \n                      \"lubridate\", \"astsa\", \"TSA\")\nfor (pkg in packages_to_load) {\n  if (!require(pkg, character.only = TRUE)) {\n    #install.packages(pkg) # uncomment this\n    library(pkg, character.only = TRUE)\n  }\n}\n\n\n#  DATA ACQUISITION AND PREPARATION\n# -------------------------------------------------------------------\nstart_date &lt;- \"1984-10-01\"\nend_date   &lt;- \"2007-04-01\"\n\nfull_data &lt;- read_excel(\"GDPC1-qdata.xlsx\")\n\nraw_data &lt;- full_data %&gt;%\n  rename(date = Date, value = GDPC1) %&gt;%\n  filter(date &gt;= as.Date(start_date) & date &lt;= as.Date(end_date))\n\ngdp_ts &lt;- ts(raw_data$value, \n             start = c(year(raw_data$date[1]), quarter(raw_data$date[1])), \n             frequency = 4)\n\ngdp_growth &lt;- diff(log(gdp_ts)) * 400  # Annualized growth rate\ngdp_growth &lt;- na.omit(gdp_growth)\n\n\n#  VISUALIZATION AND MANUAL IDENTIFICATION\n# -------------------------------------------------------------------\nautoplot(gdp_growth) + ggtitle(\"(Annualized) U.S. quarterly real GDP growth rate (1985:Q1-2007:Q2)\")\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nAcf(gdp_growth, main=\"ACF of GDP Growth\")  # forecast package\nPacf(gdp_growth, main=\"PACF of GDP Growth\") # forecast package\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n# --- Extended Ljung-Box Test ---\ncat(\"\\nExtended Ljung-Box Test for multiple lags:\\n\")\n\n\nExtended Ljung-Box Test for multiple lags:\n\nlags_to_test &lt;- c(4, 8, 12, 16, 20)\nlb_results &lt;- data.frame(Lag=integer(), \"Q-statistic\"=double(), \"p-value\"=double())\nfor (l in lags_to_test) {\n  test &lt;- Box.test(gdp_growth, type = \"Ljung-Box\", lag = l, fitdf = 0)\n  lb_results[nrow(lb_results) + 1,] &lt;- c(l, round(test$statistic, 3), round(test$p.value, 3))\n}\nprint(lb_results)\n\n  Lag Q.statistic p.value\n1   4      16.487   0.002\n2   8      19.494   0.012\n3  12      27.942   0.006\n4  16      32.897   0.008\n5  20      34.679   0.022\n\n# --- McLeod-Li Test ---\ncat(\"\\nExtended McLeod-Li Test for multiple lags:\\n\")\n\n\nExtended McLeod-Li Test for multiple lags:\n\nml_results &lt;- data.frame(\n  Lag = integer(),\n  p_value = numeric()\n)\n\nfor (l in lags_to_test) {\n  test &lt;- TSA::McLeod.Li.test(y = gdp_growth, gof.lag = l)\n  pval &lt;- test$p.values[l] # Extract p-value for the current lag\n  ml_results[nrow(ml_results) + 1, ] &lt;- c(l, round(pval, 3)) # Append row: lag and p-value\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint(ml_results) # This package does not print out the values of the test \n\n  Lag p_value\n1   4   0.342\n2   8   0.564\n3  12   0.533\n4  16   0.688\n5  20   0.717\n\n# statistic (only p-values)\n\n\n\n#  DETAILED MODEL SELECTION (using Conditional MLE)\n# -------------------------------------------------------------------\ncat(\"\\n--- Running Detailed Model Selection ---\\n\")\n\n\n--- Running Detailed Model Selection ---\n\n# --- (i) - Restricting to AR Models Only ---\ncat(\"\\n--- SCENARIO 1: AR-ONLY MODELS ---\\n\")\n\n\n--- SCENARIO 1: AR-ONLY MODELS ---\n\nar_aic &lt;- auto.arima(gdp_growth, seasonal = FALSE, max.q =0, method = \"CSS\", ic = \"aic\")\ncat(\"\\nBest AR model selected by AIC:\\n\")\n\n\nBest AR model selected by AIC:\n\nprint(ar_aic)\n\nSeries: gdp_growth \nARIMA(3,0,0) with non-zero mean \n\nCoefficients:\n         ar1     ar2      ar3    mean\n      0.1855  0.2976  -0.0966  3.0185\ns.e.  0.1037  0.1012   0.1042  0.3184\n\nsigma^2 = 3.458:  log likelihood = -183.02\n\nar_bic &lt;- auto.arima(gdp_growth, seasonal = FALSE, max.q = 0, method = \"CSS\", ic = \"bic\")\ncat(\"\\nBest AR model selected by BIC:\\n\")\n\n\nBest AR model selected by BIC:\n\nprint(ar_bic)\n\nSeries: gdp_growth \nARIMA(2,0,0) with non-zero mean \n\nCoefficients:\n         ar1     ar2    mean\n      0.1599  0.2872  3.0614\ns.e.  0.1010  0.1015  0.3567\n\nsigma^2 = 3.536:  log likelihood = -184.02\n\n# --- (ii) - Allowing ARMA Models ---\ncat(\"\\n--- SCENARIO 2: ARMA MODELS ---\\n\")\n\n\n--- SCENARIO 2: ARMA MODELS ---\n\narma_aic &lt;- auto.arima(gdp_growth, seasonal = FALSE, method = \"CSS\", ic = \"aic\")\ncat(\"\\nBest ARMA model selected by AIC:\\n\")\n\n\nBest ARMA model selected by AIC:\n\nprint(arma_aic)\n\nSeries: gdp_growth \nARIMA(0,0,2) with non-zero mean \n\nCoefficients:\n         ma1     ma2    mean\n      0.2047  0.2998  3.0893\ns.e.  0.1031  0.0964  0.2919\n\nsigma^2 = 3.556:  log likelihood = -183.27\n\narma_bic &lt;- auto.arima(gdp_growth, seasonal = FALSE, method = \"CSS\", ic = \"bic\")\ncat(\"\\nBest ARMA model selected by BIC:\\n\")\n\n\nBest ARMA model selected by BIC:\n\nprint(arma_bic)\n\nSeries: gdp_growth \nARIMA(0,0,0) with non-zero mean \n\nCoefficients:\n        mean\n      3.1001\ns.e.  0.2089\n\nsigma^2 = 3.97:  log likelihood = -189.25\n\n#  CHOOSE YOUR FINAL MODEL\n# -------------------------------------------------------------------\n# After reviewing the 4 models above, assign your choice to 'final_model'.\n# The default is the AR model selected by AIC.\ncat(\"\\n\\n--- Selecting Final Model for Detailed Analysis ---\\n\")\n\n\n\n--- Selecting Final Model for Detailed Analysis ---\n\n#final_model &lt;- ar_aic   # &lt;--- SET YOUR CHOICE HERE\nfinal_model &lt;- ar_bic\n# final_model &lt;- arma_aic\n# final_model &lt;- arma_bic  \n\n\n\n\n\n\n\n# Computing forecasts. Forecast origin is the last observation of time series\n\n# Notice that this part requires the running of ARMA modelling code above in \n# Sections 5-7 (i.e. \"final model\" contain estimation result for gdp growth).\n\ncat(\"\\n\\n--- Forecasting from Final Model ---\\n\")\n\n\n\n--- Forecasting from Final Model ---\n\ngdp_forecast &lt;- forecast(final_model, h=8, level = c(80, 95))\ngdp_forecast\n\n        Point Forecast      Lo 80    Hi 80      Lo 95    Hi 95\n2007 Q3       2.427727 0.01787587 4.837578 -1.2578223 6.113277\n2007 Q4       2.781453 0.34098863 5.221917 -0.9509150 6.513820\n2008 Q1       2.834604 0.28036903 5.388840 -1.0717615 6.740971\n2008 Q2       2.944709 0.38002888 5.509388 -0.9776305 6.867048\n2008 Q3       2.977582 0.40040373 5.554759 -0.9638718 6.919035\n2008 Q4       3.014464 0.43506842 5.593860 -0.9303814 6.959310\n2009 Q1       3.029805 0.44884169 5.610767 -0.9174375 6.977047\n2009 Q2       3.042852 0.46149492 5.624209 -0.9049929 6.990696\n\nautoplot(gdp_forecast) + ggtitle(\"Forecast from Final Model\")\n\n\n\n\n\n\n\n\n\n\nR Lab: Out-of-sample forecasting\n\n\n\n# OOS FORECASTING (with AR models)\n\n# See OUT-OF-SAMPLE FORECASTING FOR GDP GROWTH at the end after developing\n# ar_forecast function\n\n# LOAD LIBRARIES\n# -------------------------------------------------------------------\npackages_to_load &lt;- c(\"tseries\", \"forecast\", \"ggplot2\", \"readxl\", \"dplyr\",\n                      \"lubridate\", \"astsa\", \"TSA\", \"writexl\") \nfor (pkg in packages_to_load) {\n  if (!require(pkg, character.only = TRUE)) {\n    #install.packages(pkg) # uncomment this\n    library(pkg, character.only = TRUE)\n  }\n}\n\n\n# DATA ACQUISITION AND PREPARATION\n# -------------------------------------------------------------------\n# This section defines the *full range* of data available for your analysis.\n# The 'start_date' and 'end_date' here constrain the raw data loaded from the Excel file.\n# The out-of-sample forecasting will operate *within* this prepared 'gdp_growth' series.\n\nstart_date &lt;- \"1984-10-01\"\nend_date   &lt;- \"2018-10-01\" # This is your desired end date for the *filtered* data\n\n# --- IMPORTANT: Ensure 'GDPC1-qdata.xlsx' exists in your working directory ---\n# --- and contains data up to or beyond the 'end_date' specified above. ---\n\n\nfull_data &lt;- read_excel(\"GDPC1-qdata.xlsx\")\n\nraw_data &lt;- full_data %&gt;%\n  rename(date = Date, value = GDPC1) %&gt;%\n  filter(date &gt;= as.Date(start_date) & date &lt;= as.Date(end_date)) # This filter now correctly uses your end_date\n\ngdp_ts &lt;- ts(raw_data$value,\n             start = c(year(raw_data$date[1]), quarter(raw_data$date[1])),\n             frequency = 4)\n\ngdp_growth &lt;- diff(log(gdp_ts)) * 400  # Annualized growth rate\ngdp_growth &lt;- na.omit(gdp_growth) # Remove NA from differencing\n\n# Display a glimpse of the prepared gdp_growth series\ncat(\"\\n--- GDP Growth Series Prepared ---\\n\")\n\n\n--- GDP Growth Series Prepared ---\n\nprint(head(gdp_growth))\n\n         Qtr1     Qtr2     Qtr3     Qtr4\n1985 3.857259 3.506480 6.062904 2.962797\n1986 3.717661 1.797142                  \n\nprint(tail(gdp_growth))\n\n         Qtr1     Qtr2     Qtr3     Qtr4\n2017                   3.141726 4.482971\n2018 3.240478 2.117685 2.487042 0.566176\n\ncat(\"Full length of gdp_growth series available for forecasting:\", length(gdp_growth), \"observations.\\n\")\n\nFull length of gdp_growth series available for forecasting: 136 observations.\n\ncat(\"Time attributes:\", paste(start(gdp_growth), collapse=\"-\"), \"to\", paste(end(gdp_growth), collapse=\"-\"), \"at frequency\", frequency(gdp_growth), \"\\n\")\n\nTime attributes: 1985-1 to 2018-4 at frequency 4 \n\n# --- MODIFIED AR_FORECAST FUNCTION ---\n# Allows selection of initial estimation sample period by its end index or date.\nar_forecast &lt;- function(\n    data,                # Your time series data (numeric vector or ts object)\n    p_values,            # Vector of p values to compare (e.g., c(1, 2, 3))\n    train_end_index = NULL, # End index of the initial training sample\n    train_end_date = NULL,  # End date of the initial training sample (if data is ts)\n    min_train_obs = NULL,   # Minimum number of observations in training sample (now a soft check)\n    window_type = c(\"rolling\", \"expanding\"), # Window type for parameter updating\n    forecast_horizon = 1, # Forecast horizon h (number of periods ahead to forecast)\n    plot_results = TRUE  # Whether to plot the results\n) {\n  \n  # Input validation\n  if (!is.numeric(data) && !inherits(data, \"ts\")) stop(\"data must be a numeric vector or a ts object.\")\n  if (!is.numeric(p_values) || any(p_values &lt;= 0) || any(p_values != round(p_values))) stop(\"p_values must be positive integers.\")\n  if (is.null(train_end_index) && is.null(train_end_date)) {\n    stop(\"Either train_end_index or train_end_date must be provided to define the initial training sample.\")\n  }\n  if (!is.null(train_end_index) && (!is.numeric(train_end_index) || train_end_index &lt;= 0 || train_end_index != round(train_end_index))) stop(\"train_end_index must be a positive integer.\")\n  if (!is.null(min_train_obs) && (!is.numeric(min_train_obs) || min_train_obs &lt;= 0 || min_train_obs != round(min_train_obs))) stop(\"min_train_obs must be a positive integer.\")\n  window_type &lt;- match.arg(window_type)\n  if (!is.numeric(forecast_horizon) || forecast_horizon &lt;= 0 || forecast_horizon != round(forecast_horizon)) stop(\"forecast_horizon must be a positive integer.\")\n  \n  n &lt;- length(data)\n  \n  # Determine initial training sample size based on index or date\n  initial_train_size &lt;- NULL\n  if (!is.null(train_end_date)) {\n    if (!inherits(data, \"ts\")) stop(\"train_end_date can only be used with a 'ts' object.\")\n    \n    # Convert the character date to a 'Date' object first\n    target_date_obj &lt;- as.Date(train_end_date)\n    \n    # Extract year and appropriate sub-period\n    target_year &lt;- year(target_date_obj)\n    target_freq &lt;- frequency(data)\n    \n    target_sub_period &lt;- NULL\n    if (target_freq == 4) { # Quarterly data\n      target_sub_period &lt;- quarter(target_date_obj)\n    } else if (target_freq == 12) { # Monthly data\n      target_sub_period &lt;- month(target_date_obj)\n    } else if (target_freq == 1) { # Yearly data\n      target_sub_period &lt;- 1 # Not relevant, but keeps structure\n    } else {\n      stop(\"Unsupported frequency for train_end_date. Only 1 (yearly), 4 (quarterly), 12 (monthly) are directly supported.\")\n    }\n    \n    # Create a time series index value from the target date\n    target_time_value &lt;- target_year\n    if (target_freq &gt; 1) {\n      target_time_value &lt;- target_time_value + (target_sub_period - 1) / target_freq\n    }\n    \n    # Find the index in the time series data that matches or is closest to target_time_value\n    # Use which.min(abs(...)) to find the closest time point if an exact match isn't present\n    initial_train_size &lt;- which.min(abs(time(data) - target_time_value))\n    \n    # Check if the found index's time value is truly before or at the target\n    if (time(data)[initial_train_size] &gt; target_time_value + (1/(target_freq*2)) && time(data)[initial_train_size] &gt; target_time_value) { # Add a small tolerance\n      stop(paste(\"train_end_date (\", train_end_date, \") resulted in an index where the time point (\", time(data)[initial_train_size], \") is *after* the target date. Please ensure the date is within the series range or select a valid end date.\", sep=\"\"))\n    }\n    \n    # Ensure that the found index isn't past the end of the series\n    if (initial_train_size &gt; n) {\n      stop(paste(\"train_end_date (\", train_end_date, \") is after the end of the provided data series.\", sep=\"\"))\n    }\n    \n  } else { # Use train_end_index\n    initial_train_size &lt;- train_end_index\n  }\n  \n  # Final check on initial_train_size\n  if (is.null(initial_train_size) || initial_train_size &lt; 1 || initial_train_size &gt; n) {\n    stop(\"Calculated initial training sample size is invalid or out of bounds for the data series.\")\n  }\n  \n  # Apply min_train_obs if provided, primarily as a check or warning\n  if (!is.null(min_train_obs) && initial_train_size &lt; min_train_obs) {\n    warning(paste(\"Specified train_end_index/date results in an initial training sample (\", initial_train_size, \") smaller than min_train_obs (\", min_train_obs, \"). Proceeding with the smaller size.\", sep=\"\"))\n  }\n  \n  if (initial_train_size &lt; max(p_values) + 1) {\n    stop(\"Initial training sample size (\", initial_train_size, \") is too small to fit an AR model for the largest p value (\", max(p_values), \"). Increase train_end_index/date.\")\n  }\n  if (initial_train_size + forecast_horizon &gt; n) {\n    stop(\"The initial training sample period is too long (\", time(data)[initial_train_size], \") given the forecast horizon (\", forecast_horizon, \") and total data length (\", time(data)[n], \"). Not enough data left for testing.\")\n  }\n  \n  cat(paste0(\"\\n--- Initial Sample Selection ---\\n\"))\n  cat(paste0(\"Initial training (estimation) sample ends at index: \", initial_train_size, \"\\n\"))\n  cat(paste0(\"Corresponding time point: \", time(data)[initial_train_size], \"\\n\"))\n  cat(paste0(\"Out-of-sample forecasting (test) period starts at index: \", initial_train_size + 1, \"\\n\"))\n  cat(paste0(\"Corresponding time point: \", time(data)[initial_train_size + 1], \"\\n\"))\n  \n  \n  # Initialize lists to store forecasts and errors\n  all_forecasts &lt;- list() # This will store the forecast series\n  all_msfe &lt;- list()      # This will store only the MSFE values\n  \n  for (p in p_values) {\n    cat(paste0(\"\\n--- Forecasting for AR(\", p, \") ---\\n\"))\n    \n    # The length of the 'forecasts' vector corresponds to the number of forecast origins\n    num_forecast_origins &lt;- n - initial_train_size - forecast_horizon + 1\n    if (num_forecast_origins &lt;= 0) {\n      warning(paste0(\"Not enough data to make any forecasts for AR(\", p, \") with current settings. Skipping.\"))\n      all_msfe[[paste0(\"AR(\", p, \")\")]] &lt;- NA\n      all_forecasts[[paste0(\"AR(\", p, \")_series\")]] &lt;- NA\n      next\n    }\n    forecasts_for_p &lt;- numeric(num_forecast_origins)\n    \n    \n    # Loop through the test set (i.e., each forecast origin)\n    for (i in 0:(num_forecast_origins - 1)) {\n      current_train_end_idx &lt;- initial_train_size + i\n      \n      if (window_type == \"rolling\") {\n        start_roll_idx &lt;- current_train_end_idx - initial_train_size + 1\n        # The rolling window should always be `initial_train_size` long.\n        # If current_train_end_idx == initial_train_size (first iter), start_roll_idx == 1.\n        # If current_train_end_idx == initial_train_size + 1, start_roll_idx == 2.\n        # This ensures the window size is constant.\n        train_data &lt;- data[start_roll_idx:current_train_end_idx]\n      } else { # expanding\n        train_data &lt;- data[1:current_train_end_idx]\n      }\n      \n      # If train_data is too short for AR(p), skip this iteration\n      if (length(train_data) &lt;= p) {\n        warning(paste(\"Training data (length:\", length(train_data), \") too short for AR(\", p, \") for current window (end:\", time(data)[current_train_end_idx], \"). Skipping forecast.\", sep=\"\"))\n        forecasts_for_p[i + 1] &lt;- NA\n        next\n      }\n      if (length(unique(train_data)) &lt; 2) { # Check for constant data, which can break lm\n        warning(paste(\"Training data is constant for AR(\", p, \") for current window (end:\", time(data)[current_train_end_idx], \"). Skipping forecast.\", sep=\"\"))\n        forecasts_for_p[i + 1] &lt;- NA\n        next\n      }\n      \n      # Create lagged variables for lm\n      lag_matrix &lt;- matrix(NA, nrow = length(train_data) - p, ncol = p)\n      for (j in 1:p) {\n        lag_matrix[, j] &lt;- train_data[(p - j + 1):(length(train_data) - j)]\n      }\n      ar_df &lt;- data.frame(Y = train_data[(p + 1):length(train_data)], lag_matrix)\n      names(ar_df)[-1] &lt;- paste0(\"L\", 1:p)\n      \n      # Fit AR(p) model using lm\n      lm_model &lt;- lm(Y ~ ., data = ar_df)\n      lm_coeffs &lt;- coef(lm_model)\n      \n      # Handle cases where some coefficients might be NA\n      if (any(is.na(lm_coeffs))) {\n        warning(paste(\"Model fitting failed (NA coefficients) for AR(\", p, \") at current window (end:\", time(data)[current_train_end_idx], \"). Skipping forecast.\", sep=\"\"))\n        forecasts_for_p[i + 1] &lt;- NA\n        next\n      }\n      \n      lm_intercept &lt;- lm_coeffs[1]\n      lm_ar_coeffs &lt;- lm_coeffs[-1]\n      \n      # Store the last p observations from the training data for forecasting\n      forecast_input_values &lt;- tail(train_data, p)\n      \n      # Forecast h steps ahead\n      h_step_forecast_values &lt;- numeric(forecast_horizon)\n      \n      for (h_idx in 1:forecast_horizon) {\n        values_to_use_for_h_step &lt;- numeric(p)\n        for (j in 1:p) {\n          if (h_idx - j &gt;= 1) { # Use previous forecasts from this h-step sequence\n            values_to_use_for_h_step[j] &lt;- h_step_forecast_values[h_idx - j]\n          } else { # Use actual observed values from forecast_input_values\n            values_to_use_for_h_step[j] &lt;- forecast_input_values[abs(h_idx - j) + 1]\n          }\n        }\n        h_step_forecast_values[h_idx] &lt;- lm_intercept + sum(lm_ar_coeffs * values_to_use_for_h_step)\n      }\n      \n      forecasts_for_p[i + 1] &lt;- h_step_forecast_values[forecast_horizon]\n    }\n    \n    # Calculate MSFE for the specific horizon h\n    start_actual_index &lt;- initial_train_size + forecast_horizon\n    end_actual_index &lt;- start_actual_index + length(forecasts_for_p) - 1\n    \n    end_actual_index &lt;- min(end_actual_index, n) # Ensure end_actual_index does not exceed data length\n    \n    actuals_for_comparison &lt;- data[start_actual_index:end_actual_index]\n    valid_forecasts &lt;- forecasts_for_p[1:length(actuals_for_comparison)] # Trim forecasts if actuals vector is shorter\n    \n    valid_indices &lt;- !is.na(valid_forecasts)\n    if (sum(valid_indices) == 0) {\n      warning(paste(\"No valid forecasts generated for AR(\", p, \"). MSFE cannot be computed.\", sep=\"\"))\n      all_msfe[[paste0(\"AR(\", p, \")\")]] &lt;- NA\n      all_forecasts[[paste0(\"AR(\", p, \")_series\")]] &lt;- NA\n    } else {\n      msfe &lt;- mean((actuals_for_comparison[valid_indices]-valid_forecasts[valid_indices])^2)\n      all_msfe[[paste0(\"AR(\", p, \")\")]] &lt;- msfe\n      all_forecasts[[paste0(\"AR(\", p, \")_series\")]] &lt;- valid_forecasts\n    }\n    \n    cat(paste0(\"AR(\", p, \") Forecasts Generated. MSFE (h=\", forecast_horizon, \"): \", round(all_msfe[[paste0(\"AR(\", p, \")\")]], 6), \"\\n\")) # More precision for MSFE\n  }\n  \n  # Plotting results\n  if (plot_results && length(all_msfe) &gt; 0) {\n    plot_start_idx &lt;- initial_train_size + forecast_horizon\n    plot_end_idx &lt;- n\n    if (plot_start_idx &gt; plot_end_idx || length(data[plot_start_idx:plot_end_idx]) == 0) {\n      warning(\"Not enough data to plot forecasts in the test period.\")\n      return(list(msfe_results = all_msfe, all_forecast_series = all_forecasts))\n    }\n    \n    x_axis_labels &lt;- time(data)[plot_start_idx:plot_end_idx] # Use time attributes for x-axis\n    \n    par(mfrow = c(1, 1), mar = c(4, 4, 3, 2) + 0.1)\n    \n    # Collect all forecast series to determine appropriate y-axis limits\n    all_plot_values &lt;- c(data[plot_start_idx:plot_end_idx])\n    for(s_name in names(all_forecasts)) {\n      if(!is.na(all_forecasts[[s_name]][1])) { # Only add non-NA series\n        all_plot_values &lt;- c(all_plot_values, all_forecasts[[s_name]])\n      }\n    }\n    \n    plot(x_axis_labels, data[plot_start_idx:plot_end_idx], type = \"l\", col = \"black\", lwd = 2,\n         xlab = \"Date\", ylab = \"GDP Growth (%)\", main = paste0(\"Actual vs. Forecasts (h=\", forecast_horizon, \", \", window_type, \" window)\"),\n         xlim = range(x_axis_labels),\n         ylim = range(all_plot_values, na.rm=TRUE) # Dynamically adjust ylim based on all valid series\n    )\n    \n    # Add initial training end marker\n    abline(v = time(data)[initial_train_size], lty = 2, col = \"grey\", lwd = 1.5)\n    text(time(data)[initial_train_size], par(\"usr\")[4], \"Train End\", pos = 4, col = \"grey40\", cex = 0.8)\n    \n    # Plot forecasts\n    p_val_names &lt;- names(all_msfe) # Use the names from MSFE list\n    colors &lt;- rainbow(length(p_val_names))\n    lty_values &lt;- 2:(length(p_val_names) + 1)\n    legend_labels &lt;- c(\"Actual\")\n    legend_cols &lt;- c(\"black\")\n    legend_lwd &lt;- c(2)\n    legend_lty &lt;- c(1)\n    \n    for (k in 1:length(p_val_names)) {\n      p_str &lt;- p_val_names[k]\n      forecast_values &lt;- all_forecasts[[paste0(p_str, \"_series\")]] # Retrieve the actual series\n      if (!is.null(forecast_values) && !is.na(forecast_values[1])) {\n        lines(x_axis_labels[1:length(forecast_values)], forecast_values, col = colors[k], lty = lty_values[k], lwd = 1)\n        legend_labels &lt;- c(legend_labels, p_str)\n        legend_cols &lt;- c(legend_cols, colors[k])\n        legend_lwd &lt;- c(legend_lwd, 1)\n        legend_lty &lt;- c(legend_lty, lty_values[k])\n      }\n    }\n    legend(\"topleft\", legend = legend_labels, col = legend_cols, lwd = legend_lwd, lty = legend_lty, cex = 0.8)\n    \n    par(mfrow = c(1, 1)) # Reset plot layout\n  }\n  \n  # Return only MSFE values in the main list, and forecasts in a sub-list if needed\n  return(list(msfe_results = all_msfe, all_forecast_series = all_forecasts))\n}\n\n\n# --- OUT-OF-SAMPLE FORECASTING FOR GDP GROWTH ---\ncat(\"\\n\\n#####################################################\")\n\n\n\n#####################################################\n\ncat(\"\\n### RUNNING OUT-OF-SAMPLE FORECASTING FOR GDP GROWTH ###\")\n\n\n### RUNNING OUT-OF-SAMPLE FORECASTING FOR GDP GROWTH ###\n\ncat(\"\\n#####################################################\\n\")\n\n\n#####################################################\n\n# -------------------------------------------------------------------\n# &gt;&gt;&gt; SELECTION OF TRAINING (ESTIMATION) AND FORECASTING (TEST) SAMPLE &lt;&lt;&lt;\n# This is where you define the split between your initial estimation period\n# and where the out-of-sample forecasting exercise begins.\n#\n# This selection operates *within* the 'gdp_growth' series defined in the\n# 'DATA ACQUISITION AND PREPARATION' section.\n#\n# Example: If 'gdp_growth' runs from 1985 Q1 to 2007 Q1:\n#   - Setting `initial_train_end_date = \"2000-10-01\"` means the initial\n#     training sample will be from 1985 Q1 up to and including 2000 Q4.\n#     (Assuming the gdp_growth series covers 2000 Q4).\n#   - The out-of-sample forecasting period will then start from 2001 Q1.\n# -------------------------------------------------------------------\n\n# Define initial training sample end by specific date (recommended for time series)\n# Make sure this date exists within the 'gdp_growth' time series.\n# Here, we end the initial training sample at the end of 2000 Q4.\n# (Verify that 2000-10-01 corresponds to an actual point in your 'gdp_growth' series\n# after differencing and NA removal.)\n\n#initial_train_end_date_selected &lt;- \"2000-10-01\" # This corresponds to 2000 Q4 for quarterly data\ninitial_train_end_date_selected &lt;- \"2009-10-01\" # \n\n\ncat(paste0(\"\\n--- Forecasting Run 1: 1-Quarter Ahead Forecasts (Expanding Window) ---\\n\"))\n\n\n--- Forecasting Run 1: 1-Quarter Ahead Forecasts (Expanding Window) ---\n\ngdp_forecast_results_h1 &lt;- ar_forecast(\n  data = gdp_growth,\n  p_values = c(1, 2, 3, 4), # Explicitly requesting AR(1) to AR(4)\n  train_end_date = initial_train_end_date_selected,\n  window_type = \"expanding\",\n  forecast_horizon = 1, # 1-quarter ahead forecast\n  plot_results = TRUE\n)\n\n\n--- Initial Sample Selection ---\nInitial training (estimation) sample ends at index: 100\nCorresponding time point: 2009.75\nOut-of-sample forecasting (test) period starts at index: 101\nCorresponding time point: 2010\n\n--- Forecasting for AR(1) ---\nAR(1) Forecasts Generated. MSFE (h=1): 3.010018\n\n--- Forecasting for AR(2) ---\nAR(2) Forecasts Generated. MSFE (h=1): 2.810865\n\n--- Forecasting for AR(3) ---\nAR(3) Forecasts Generated. MSFE (h=1): 2.760154\n\n--- Forecasting for AR(4) ---\nAR(4) Forecasts Generated. MSFE (h=1): 3.451758\n\n\n\n\n\n\n\n\ncat(\"\\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=1, expanding window) ---\\n\")\n\n\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=1, expanding window) ---\n\nprint(gdp_forecast_results_h1$msfe_results)\n\n$`AR(1)`\n[1] 3.010018\n\n$`AR(2)`\n[1] 2.810865\n\n$`AR(3)`\n[1] 2.760154\n\n$`AR(4)`\n[1] 3.451758\n\ncat(\"-----------------------------------------------------------------------\\n\")\n\n-----------------------------------------------------------------------\n\ncat(paste0(\"\\n--- Forecasting Run 1: 1-Quarter Ahead Forecasts (Rolling Window) ---\\n\"))\n\n\n--- Forecasting Run 1: 1-Quarter Ahead Forecasts (Rolling Window) ---\n\ngdp_forecast_results_h1 &lt;- ar_forecast(\n  data = gdp_growth,\n  p_values = c(1, 2, 3, 4), # Explicitly requesting AR(1) to AR(4)\n  train_end_date = initial_train_end_date_selected,\n  window_type = \"rolling\",\n  forecast_horizon = 1, # 1-quarter ahead forecast\n  plot_results = TRUE\n)\n\n\n--- Initial Sample Selection ---\nInitial training (estimation) sample ends at index: 100\nCorresponding time point: 2009.75\nOut-of-sample forecasting (test) period starts at index: 101\nCorresponding time point: 2010\n\n--- Forecasting for AR(1) ---\nAR(1) Forecasts Generated. MSFE (h=1): 3.032567\n\n--- Forecasting for AR(2) ---\nAR(2) Forecasts Generated. MSFE (h=1): 2.793521\n\n--- Forecasting for AR(3) ---\nAR(3) Forecasts Generated. MSFE (h=1): 2.731007\n\n--- Forecasting for AR(4) ---\nAR(4) Forecasts Generated. MSFE (h=1): 3.481733\n\n\n\n\n\n\n\n\ncat(\"\\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=1, rolling window) ---\\n\")\n\n\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=1, rolling window) ---\n\nprint(gdp_forecast_results_h1$msfe_results)\n\n$`AR(1)`\n[1] 3.032567\n\n$`AR(2)`\n[1] 2.793521\n\n$`AR(3)`\n[1] 2.731007\n\n$`AR(4)`\n[1] 3.481733\n\ncat(\"-----------------------------------------------------------------------\\n\")\n\n-----------------------------------------------------------------------\n\ncat(paste0(\"\\n--- Forecasting Run 2: 4-Quarter Ahead Forecasts (Expanding Window) ---\\n\"))\n\n\n--- Forecasting Run 2: 4-Quarter Ahead Forecasts (Expanding Window) ---\n\ngdp_forecast_results_h4 &lt;- ar_forecast(\n  data = gdp_growth,\n  p_values = c(1, 2, 3, 4), # Explicitly requesting AR(1) to AR(4)\n  train_end_date = initial_train_end_date_selected, # Same initial training end\n  window_type = \"expanding\",\n  forecast_horizon = 4, # 4-quarters ahead forecast\n  plot_results = TRUE\n)\n\n\n--- Initial Sample Selection ---\nInitial training (estimation) sample ends at index: 100\nCorresponding time point: 2009.75\nOut-of-sample forecasting (test) period starts at index: 101\nCorresponding time point: 2010\n\n--- Forecasting for AR(1) ---\nAR(1) Forecasts Generated. MSFE (h=4): 2.628715\n\n--- Forecasting for AR(2) ---\nAR(2) Forecasts Generated. MSFE (h=4): 2.662022\n\n--- Forecasting for AR(3) ---\nAR(3) Forecasts Generated. MSFE (h=4): 2.457973\n\n--- Forecasting for AR(4) ---\nAR(4) Forecasts Generated. MSFE (h=4): 2.467844\n\n\n\n\n\n\n\n\ncat(\"\\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=4, expanding window) ---\\n\")\n\n\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=4, expanding window) ---\n\nprint(gdp_forecast_results_h4$msfe_results)\n\n$`AR(1)`\n[1] 2.628715\n\n$`AR(2)`\n[1] 2.662022\n\n$`AR(3)`\n[1] 2.457973\n\n$`AR(4)`\n[1] 2.467844\n\ncat(\"-----------------------------------------------------------------------\\n\")\n\n-----------------------------------------------------------------------\n\ncat(paste0(\"\\n--- Forecasting Run 2: 4-Quarter Ahead Forecasts (Rolling Window) ---\\n\"))\n\n\n--- Forecasting Run 2: 4-Quarter Ahead Forecasts (Rolling Window) ---\n\ngdp_forecast_results_h4 &lt;- ar_forecast(\n  data = gdp_growth,\n  p_values = c(1, 2, 3, 4), # Explicitly requesting AR(1) to AR(4)\n  train_end_date = initial_train_end_date_selected, # Same initial training end\n  window_type = \"rolling\",\n  forecast_horizon = 4, # 4-quarters ahead forecast\n  plot_results = TRUE\n)\n\n\n--- Initial Sample Selection ---\nInitial training (estimation) sample ends at index: 100\nCorresponding time point: 2009.75\nOut-of-sample forecasting (test) period starts at index: 101\nCorresponding time point: 2010\n\n--- Forecasting for AR(1) ---\nAR(1) Forecasts Generated. MSFE (h=4): 2.602836\n\n--- Forecasting for AR(2) ---\nAR(2) Forecasts Generated. MSFE (h=4): 2.633853\n\n--- Forecasting for AR(3) ---\nAR(3) Forecasts Generated. MSFE (h=4): 2.496958\n\n--- Forecasting for AR(4) ---\nAR(4) Forecasts Generated. MSFE (h=4): 2.478106\n\n\n\n\n\n\n\n\ncat(\"\\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=4, rolling window) ---\\n\")\n\n\n--- MSFE Results for AR(1), AR(2), AR(3), AR(4) (h=4, rolling window) ---\n\nprint(gdp_forecast_results_h4$msfe_results)\n\n$`AR(1)`\n[1] 2.602836\n\n$`AR(2)`\n[1] 2.633853\n\n$`AR(3)`\n[1] 2.496958\n\n$`AR(4)`\n[1] 2.478106\n\ncat(\"-----------------------------------------------------------------------\\n\")\n\n-----------------------------------------------------------------------\n\n# Accessing specific MSFE values:\n# gdp_forecast_results_h1$msfe_results$`AR(1)`\n# gdp_forecast_results_h4$msfe_results$`AR(4)`\n\n# Accessing specific forecast series:\n# gdp_forecast_results_h1$all_forecast_series$`AR(2)_series`\n\n\n\nExtra: Notes for attached out-of-sample forecasting code\n\n\nConnecting to the Provided R Code\nThe R program, specifically the ar_forecast function, implements the pseudo out-of-sample forecasting methodology for Autoregressive (AR) models as described above.\n\nData Acquisition and Preparation: The initial section of the R code, labelled DATA ACQUISITION AND PREPARATION, loads the GDPC1-qdata.xlsx file and transforms it into the gdp_growth time series. This gdp_growth series represents the full dataset available for the forecasting exercise. All subsequent training and test sample selections operate within the bounds of this gdp_growth series.\nInitial Training and Test Sample Selection: The section &gt;&gt;&gt; SELECTION OF TRAINING (ESTIMATION) AND FORECASTING (TEST) SAMPLE &lt;&lt;&lt; within the R code is crucial for defining the initial split. Here, you specify the initial_train_end_date_selected (or initial_train_end_idx_selected).\n\nInitial Training Sample (Estimation Sample): The data from the beginning of gdp_growth up to and including the initial_train_end_date_selected comprises the initial training sample (our \\(y_1, \\dots, y_T\\)).\nForecasting (Test) Sample: The data after initial_train_end_date_selected up to the end of the gdp_growth series forms the basis of the out-of-sample test period. The first forecast in the evaluation will be made for the observation immediately following initial_train_end_date_selected.\n\nParameters in ar_forecast:\n\ndata = gdp_growth: This is your \\(y_1, \\dots, y_N\\) series.\np_values = c(1, 2, 3, 4): This allows you to compare the forecasting performance of AR(1), AR(2), AR(3), and AR(4) models simultaneously.\ntrain_end_date = initial_train_end_date_selected: This directly sets the \\(T\\) value, defining the end of your initial estimation period.\nwindow_type = \"rolling\" (or \"expanding\"): Corresponds directly to the parameter updating strategies discussed above.\nforecast_horizon = 1 (or 4): This sets your \\(h\\) value (e.g., \\(h=1\\) for one-quarter-ahead forecasts, \\(h=4\\) for four-quarter-ahead forecasts).\nThe ar_forecast function then iteratively:\n\nEstimates an AR(p) model on the current training window.\nGenerates an \\(h\\)-step ahead forecast.\nCalculates the forecast error.\nUpdates the training window according to window_type.\n\n\nMSFE Calculation and Reporting: The function automatically computes the MSFE for each AR(p) model and each specified forecast_horizon over the entire out-of-sample period (which contains \\(m\\) forecast origins). The results, specifically the MSFE values, are returned and printed by the R script, allowing for direct comparison of model performance.\n\nBy running the accompanying R code, you can practically apply these theoretical concepts to evaluate the out-of-sample forecasting capabilities of different AR(p) models for GDP growth, understanding how parameter updating and forecast horizons influence performance.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Forecasting with ARMA models</span>"
    ]
  },
  {
    "objectID": "TSE-ch9.html",
    "href": "TSE-ch9.html",
    "title": "10  Volatility modelling: AR-GARCH model",
    "section": "",
    "text": "10.1 Modelling conditional variance\nARMA models can be characterized as models for the conditional mean of a stationary process.\nThe conditional variance of an ARMA process (model) \\(y_t\\) is \\[\\begin{eqnarray*}\n\\mathsf{Var}_{t-1}(y_t) &=& \\mathsf{E}_{t-1}(y_t-\\mathsf{E}_{t-1}(y_t))^2 \\\\\n&=& \\mathsf{E}_{t-1}(u^2_t) \\\\\n&=& \\mathsf{E}(u^2_t) \\qquad (\\mathrm{CEV2}) \\\\\n&=& \\sigma ^{2}.\n\\end{eqnarray*}\\] This shows that evident systematic variation in conditional variance, that several time series contain, cannot be taken into consideration with an ARMA model. Typical examples of such time series are financial time series and especially different asset return series.\nEmpirical example. The NASDAQ 100 (ticker symbol ^NDX) is a major stock market index that tracks the performance of 100 of the largest non-financial companies listed on the NASDAQ stock market (U.S. stock market).\nLet us consider excess stock returns of the NASDAQ 100 over the 3-Month Treasury Bill rate. As a simple approximation, we can think that we are interested in the percentage changes (log-differences) of the NASDAQ 100 index adjusted for risk-free rate return (see Transformations in Section 1). Below we depict the daily excess stock returns between 1.1.2003–30.9.2025.\nThe return series seems quite stationary in terms of its level. Below we depict the sample autocorrelation and partial autocorrelation functions for the first 40 lags and their approximate 95% critical bounds. These suggest that there is some autocorrelation but its degree is not very high.\nThe asset return series exhibits, however, some even more clear and typical variation, which is reflected by the autocorrelation function of the squared observations: All the reported autocorrelation coefficients of squared observations exhibit clear positive autocorrelation exceeding the 95%-critical bound. The approximate \\(p\\)-value of the McLeod Li test with \\(H=10\\), and also with other selections, is zero with four decimal precision.\nIn what follows, heteroskedasticity similar to that of the stock return series above is thought to be not related with changes in unconditional variance, but rather with changes in the conditional variance where conditioning is on the past values of the series. Next, we will first consider some general aspects of models used in modelling conditional variance, and then focus on some particular models that are most common in practice.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Volatility modelling: AR-GARCH model</span>"
    ]
  },
  {
    "objectID": "TSE-ch9.html#modelling-conditional-variance",
    "href": "TSE-ch9.html#modelling-conditional-variance",
    "title": "10  Volatility modelling: AR-GARCH model",
    "section": "",
    "text": "When the invertibility condition holds, ARMA processes have the AR(\\(\\infty\\)) representation from which it can be seen that the conditional expected value of \\(y_{t}\\), conditional on the past of the process \\(\\left\\{y_{t-1},y_{t-2},\\ldots\\right\\}\\), is \\(\\mathsf{E}_{t-1}(y_t)=-\\sum_{j=1}^{\\infty}\\pi_{j} (y_{t-j}-\\mu)\\).\n\n\n\n\n\nIt includes 100 of the largest domestic and international companies listed on NASDAQ, weighted by a modified market capitalization method to limit the influence of the very large firms. The index excludes financial companies and is heavily concentrated in the technology sector and dominated by a few extremely large companies like (as in 2025) NVIDIA Corporation (NVDA), Microsoft Corp. (MSFT), Apple Inc. (AAPL) and Amazon.com Inc. (AMZN).\nThe NASDAQ 100 is widely viewed as the benchmark for large-cap growth stocks.\n\n\n\n\nFigure: Excess stock returns of the NASDAQ 100 stock market index.\n\n\n\n\nTypically asset returns are almost non-autocorrelated\nFor this sample period and NASDAQ returns, the resulting Ljung-Box test statistics reject the null hypothesis of no autocorrelation with all the relevant statistical significance levels.\n\n\n\nFigure: Sample autocorrelations and partial autocorrelations in NASDAQ 100 excess returns.\n\n\n\n\nThere are periods during which the variation of the series is either larger or smaller than on average. This “volatility clustering” is certain type of heteroskedasticity.\nWhen the definition of variance is taken into account, it is natural to investigate this using the autocorrelation function of the squared observations. Notable are also the large difference between large and small absolute values, which suggests a more fat-tailed distribution than the normal distribution.\n\n\n\nFigure: Sample autocorrelations for the squared stock return observations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Volatility modelling: AR-GARCH model</span>"
    ]
  },
  {
    "objectID": "TSE-ch9.html#model-formulation",
    "href": "TSE-ch9.html#model-formulation",
    "title": "10  Volatility modelling: AR-GARCH model",
    "section": "10.2 Model formulation",
    "text": "10.2 Model formulation\nIn this section, we consider an AR-GARCH model determined by the following two equations \\[\\begin{equation*}\ny_{t} = \\nu + \\phi_{1}y_{t-1}+\\cdots+\\phi_{p}y_{t-p}+u_{t},\n\\end{equation*}\\] \\[\\begin{equation*}\nu_{t} = h_{t}^{1/2}\\varepsilon_{t},\\quad \\varepsilon_{t}\\sim\\mathsf{iid}\\left(0,1\\right),\n\\end{equation*}\\] where \\(h_{t}\\) is a function of the variables \\(u_{t-j}\\), \\(j&gt;0\\), and the error term \\(\\varepsilon_{t}\\) is assumed to be independent of the variables \\(y_{t-j}, \\, j&gt;0\\), and hence also of the variables \\(u_{t-j}\\), \\(j&gt;0\\). In other words, we consider an AR(\\(p\\)) model whose error term is conditionally heteroskedastic.\n\nThe coefficients \\(\\phi_{1},\\ldots,\\phi_{p}\\) are assumed to satisfy the (sufficient) stationarity condition of the AR(\\(p\\)) process: \\(1 - \\phi_1 z - \\cdots - \\phi_p z^p = \\phi(z)  \\neq 0, \\, \\mathrm{when} \\, |z| \\le 1\\).\n\nRegarding the conditional variance, by its definition, the variance of a random variable is associated with the squares of the random variable. Therefore, it seems natural that the conditional variance \\(h_t\\) would depend on the past squared values of the process. For concreteness sake, consider a general GARCH(\\(r\\),\\(s\\)) model \\[\\begin{equation*}\n    h_{t}=\\omega+\\beta_{1}h_{t-1}+\\cdots+\\beta_{r}h_{t-r}+\\alpha_{1}u_{t-1}^{2}+\\cdots+\\alpha_{s}u_{t-s}^{2},\n\\end{equation*}\\] whose parameters are assumed to satisfy the required conditions for non-negativeness, identification, and strict stationarity (to be considered below).\n\nAs before, we denote the conditional expectation as \\(\\mathsf{E}_{t-1}\\left(\\cdot\\right)=\\mathsf{E}\\left(\\cdot\\left\\vert y_{s},\\text{ }s\\leq t-1\\right.\\right)\\). Because \\(u_{t}\\) is a function of the variables \\(y_{t}\\), …, \\(y_{t-p}\\), the conditional variance \\(h_{t}\\) is a function of the variables \\(y_{t-j}\\), \\(j&gt;0\\). Therefore, we can use the same arguments as considered, e.g., in forecast construction (see CEV4) to conclude that in the AR-GARCH model \\[\\begin{equation*}\n\\mathsf{E}_{t-1}\\left(  u_{t}\\right)  =h_{t}^{1/2} \\mathsf{E}_{t-1}\\left(\\varepsilon_{t}\\right)  =h_{t}^{1/2}\\mathsf{E}\\left(\\varepsilon_{t}\\right)=0.\n\\end{equation*}\\] Together with this result, the model equations of the AR-GARCH model can be used to justify the following two results \\[\\begin{equation*}\n\\mathsf{E}_{t-1}(y_t) = \\nu + \\phi_{1} y_{t-1}+ \\cdots + \\phi_{p}y_{t-p} \\quad \\mathrm{and} \\quad   \\mathsf{Var}_{t-1}(y_t)=h_t.\n\\end{equation*}\\]\n\nThe latter result can be justified by noticing that \\(y_{t} = \\mathsf{E}_{t-1}\\left(y_{t}\\right) + u_{t}\\) and that \\[\\begin{eqnarray*}\n\\mathsf{Var}_{t-1}\\left(y_{t}\\right) &=& \\mathsf{E}_{t-1}(y_t - \\mathsf{E}_{t-1}(y_t))^2 \\\\\n&=& \\mathsf{E}_{t-1}\\left(u_{t}^{2}\\right) \\\\\n&=& h_t \\mathsf{E}(\\varepsilon_{t}^{2}) \\\\\n&=& h_{t},\n\\end{eqnarray*}\\] where the result CEV2 and the identity \\(u_{t}^{2}=h_{t}\\varepsilon_{t}^{2}\\) is used (see the general model definition above).\n\nThese two results demonstrate that the conditional mean and conditional variance of the process \\(y_{t}\\) depend on the past values of the process. Based on the above points, building a volatility model, that is selecting the model equation for \\(h_t\\), roughly consists of the following steps:\n\nFinding an adequate specification for the conditional mean \\(\\mathsf{E}_{t-1}(y_t)\\) (e.g., to select a suitable AR or ARMA model, and including also possible deterministic terms) is necessary to obtain a suitable specification for the conditional variance.\n\n\n\nChecking for the (conditional) heteroskedasticity of the error term. This can be based on residuals, as they are empirical counterparts of the error terms. That is testing “ARCH effects” using, e.g., the McLeod-Li test.\nFinding a sufficient specification for the conditional variance \\(h_t\\). There are a lot of different alternative model specifications to GARCH(\\(r,s\\)) suggested in the econometric literature. Replacing it with some of the alternatives yield a straightforward extension of the AR-GARCH model introduced above.\nEstimation of the full model can be carried out by the method of maximum likelihood.\n\n \n\nExtra: Relation to risk management and Value-at-Risk (VaR)\n\n\nThe core motivation for modeling conditional heteroskedasticity (time-varying volatility \\(h_t\\)) in asset returns \\(y_t\\) is its direct and profound impact on risk management, particularly in calculating measures like Value-at-Risk (VaR).\nThe question that often arises in risk management is: What is the maximum expected loss over a specific time horizon with a given confidence level? This loss is the Value-at-Risk (VaR) threshold \\(c\\). The VaR threshold \\(c\\) is the loss level such that the probability of the actual loss \\(y_{t+1}\\) being less than or equal to \\(c\\) is a fixed, small probability \\(\\pi\\) (e.g., 1% or 5%): \\[\\begin{equation*}\nP_t(y_{t+1} \\le c ) = G \\Bigg(\\frac{c - \\mathsf{E}_{t}(y_{t+1})}{ h^{1/2}_{t+1}} \\Bigg) = \\pi,\n\\end{equation*}\\] where \\(\\mathsf{E}_{t}(y_{t+1})\\) is the conditional mean (return forecast), \\(h_{t+1}\\) is the conditional variance forecast (volatility), and \\(G(\\cdot)\\) is the cumulative distribution function (CDF) of the standardized innovation \\(\\frac{y_{t+1} - \\mathsf{E}_{t}(y_{t+1})}{ h^{1/2}_{t+1}}\\).\nThe Value-at-Risk is the most commonly used risk measure. For the VaR model to be a useful and reliable tool in practice, the specification of the conditional variance \\(h_t\\) is critical. Ignoring or otherwise misspecifying the time-varying nature of volatility (for example, treating \\(h_t\\) as a constant) leads to severely flawed risk estimates.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Volatility modelling: AR-GARCH model</span>"
    ]
  },
  {
    "objectID": "TSE-ch9.html#garch11-and-archs-models",
    "href": "TSE-ch9.html#garch11-and-archs-models",
    "title": "10  Volatility modelling: AR-GARCH model",
    "section": "10.3 GARCH(1,1) and ARCH(\\(s\\)) models",
    "text": "10.3 GARCH(1,1) and ARCH(\\(s\\)) models\nInstead of the general GARCH(\\(r,s\\)) model, the GARCH(1,1) model \\[\\begin{equation*}\nh_t = \\omega + \\beta_1 h_{t-1} + \\alpha_1 u^2_{t-1}\n\\end{equation*}\\] has been found adequate for most (financial) time series data.\n\nIn this GARCH(1,1) case, the non-negativeness of \\(h_t\\) requires \\(\\omega &gt; 0\\) and \\(\\alpha_1, \\beta_1 \\ge 0\\). Moreover, for \\(\\beta_1\\) to be identified, \\(\\alpha_1\\) must be strictly positive (\\(\\alpha_1 &gt; 0\\)).\nIn the GARCH(\\(r,s\\)) model, these non-negativeness conditions are clearly more complicated.\n\nAnother special case of GARCH models is obtained when \\(r=0\\). That is the “GARCH-part” is missing and the GARCH(\\(r,s\\)) model reduces to an ARCH(\\(s\\)) model \\[\\begin{eqnarray*}\nh_t = \\omega +  \\sum_{i=1}^{s} \\alpha_i u^2_{t-1}.\n\\end{eqnarray*}\\]\nIn line with the idea of capturing volatility clustering with ARCH and GARCH models, the periods of high (and low) conditional variance tend to persist. This is easy to see with the ARCH(1) model (\\(s=1\\)) \\[\\begin{equation*}\nh_t = \\omega + \\alpha_1 u^2_{t-i},\n\\end{equation*}\\] where \\(\\omega, \\alpha_1 \\ge 0\\). A large shock (\\(u^2_{t-1}\\)) increases \\(h_t\\), which then subsequently increases \\(u^2_t\\) and \\(h_{t+1}\\). This same logic holds also for more general ARCH and GARCH models. Moreover, in the ARCH(1) model, assuming normality of \\(\\varepsilon_t\\), the (unconditional) kurtosis of \\(u_t\\) is \\[\\begin{equation*}\n\\frac{\\mathrm{E}(u^4_t)}{\\mathrm{E}(u^2_t)^2} = \\frac{3 (1 - \\alpha^2_1)}{1 - 3 \\alpha^2_1}.\n\\end{equation*}\\] Kurtosis is finite (i.e. the 4th moment exists) if \\(3 \\alpha^2_1 &lt; 1\\) and larger than 3 implied by the normal distribution. The ARCH(1) model is hence capable of capturing excess kurtosis often present in financial data.\n\nLarge outliers (in absolute value) appear more often than implied by the innovations in (observed) asset returns.\nSimilar result on excess kurtosis holds also for more general ARCH and GARCH models, but the formulae become more complicated.\n\n \nAs a summary of large past research, the GARCH(1,1) model has generally been found a successful and parsimonious alternative to the ARCH(\\(s\\)) model with a typically large \\(s\\) for asset returns. Why? By recursive substitutions, we get \\[\\begin{eqnarray*}\nh_t &=& \\omega + \\beta_1 h_{t-1} + \\alpha_1 u^2_{t-1} \\\\\n&=& \\omega + \\beta_1 \\omega + \\beta_1^2 h_{t-2} + \\alpha_1 \\beta_1 u^2_{t-2} + \\alpha_1 u^2_{t-1} \\\\\n& \\vdots & \\\\\n&=& \\omega \\sum_{j=0}^{k} \\beta_1^j + \\alpha_1 \\sum_{j=0}^{k} \\beta_1^j u^2_{t-1-j} + \\beta_1^{k+1} h_{t-k-1}.\n\\end{eqnarray*}\\] This suggests that in the case \\(\\beta_1 &lt; 1\\), \\[\\begin{equation*}\nh_t = \\omega \\sum_{j=0}^{\\infty} \\beta_1^j + \\alpha_1 \\sum_{j=0}^{\\infty} \\beta_1^j u^2_{t-1-j},\n\\end{equation*}\\] which is a particular kind of ARCH(\\(\\infty\\)) form. Therefore, we can conclude that GARCH(1,1) is able to capture volatility clustering in a parsimonious way by specifying only three parameters.\nAnother perspective on the GARCH models, and specifically to the GARCH(1,1) model, is obtained by adding \\(u_t^2\\) on both sides of the model equation. Therefore, the GARCH(1,1) can be rewritten \\[\\begin{equation*}\nu^2_t = \\omega + (\\alpha_1 + \\beta_1) u^2_{t-1} + \\xi_t - \\beta_1 \\xi_{t-1},\n\\end{equation*}\\] where \\(\\xi_t = u^2_t - h_t = h_t(\\varepsilon^2_t-1)\\) and \\(\\mathrm{E}(\\xi_t)=0\\). This is an ARMA(1,1) type of presentation for \\(u_t^2\\).\n\nAs for the ARMA(1,1) process, the condition for weak and strict stationarity is \\(\\alpha_1 + \\beta_1 &lt; 1\\) (together with restrictions \\(0 \\le \\alpha_1, \\beta_1 \\le 1\\) to, e.g., guarantee the non-negativity of \\(h_t\\)).\nMoreover, the unconditional variance (provided stationarity) is \\[\\begin{equation*}\n\\mathrm{E}(u^2_t) = \\frac{\\omega}{1 - \\alpha_1 - \\beta_1}.\n\\end{equation*}\\]\n\n \n\nExtra: Special case of the AR-GARCH with zero conditional mean\n\n\nThe AR-GARCH model presentation above contains also the special case of \\(\\mathsf{E}_{t-1}(y_t) = 0\\) often considered in various references (books etc.). This leads to the notation \\[\\begin{equation*}\nu_t= y_t = h^{1/2}_t \\varepsilon_t, \\quad \\varepsilon_t \\thicksim \\mathrm{iid}(0,1).\n\\end{equation*}\\] Therefore, in this case the GARCH(1,1) model reduces to \\[\\begin{equation*}\n    h_{t}=\\omega+\\beta_1 h_{t-1}+\\alpha_1 y_{t-1}^{2}\n\\end{equation*}\\] and in the ARCH(\\(s\\)) case, we get \\[\\begin{equation*}\nh_t = \\omega +  \\sum_{i=1}^{s} \\alpha_i y^2_{t-i}.\n\\end{equation*}\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Volatility modelling: AR-GARCH model</span>"
    ]
  },
  {
    "objectID": "TSE-ch9.html#parameter-estimation",
    "href": "TSE-ch9.html#parameter-estimation",
    "title": "10  Volatility modelling: AR-GARCH model",
    "section": "10.4 Parameter estimation",
    "text": "10.4 Parameter estimation\nIf it is assumed that the error term is Gaussian \\[\\begin{equation*}\n\\varepsilon_{t}\\sim\\mathsf{nid}\\left(0,1\\right),\n\\end{equation*}\\] the (conditional) likelihood function can be derived following analogous principles as in ARMA models. In other words, assuming \\(\\varepsilon_t \\thicksim \\mathsf{nid}(0,1)\\), we get \\[\\begin{equation*}\nu_t = h^{1/2}_t \\varepsilon_t, \\quad \\varepsilon_t \\thicksim \\mathsf{nid}(0,1),\n\\end{equation*}\\] where \\(h_t = h_t(y_{t-1}, y_{t-2},\\ldots)\\), \\(\\varepsilon_t\\) and vector \\((y_{t-1}, y_{t-2},\\ldots)\\) are independent, and \\(u_t = y_t - \\mathsf{E}_{t-1}(y_t)\\). Assume also that \\(y_t\\) is stationary. Given the above assumptions and the conditional moments (conditional mean and variance) of the AR-GARCH model, we get the conditional density function of the observation \\(y_t\\) as \\[\\begin{equation*}\ny_t|\\{y_{t-j}, \\, j \\ge 1 \\} \\thicksim \\mathsf{N}(\\nu + \\phi_1 y_{t-1} + \\cdots + \\phi_p y_{t-p}, h_t).\n\\end{equation*}\\] Therefore, it can be concluded that the conditional distribution of \\(y_{t}\\), conditional on \\(\\ubar{\\mathbf{Y}}_{t-1}=(y_1, \\ldots y_{t-1}), \\, t=1,\\ldots,T\\), is Gaussian with conditional mean \\(\\nu + \\phi_{1}y_{t-1}+\\cdots+\\phi_{p}y_{t-p}\\) and conditional variance \\(h_{t}\\) where the model equation for \\(h_{t}\\) needs to be specified.\nUsing the observed time series and \\(l\\) initial values \\(y_{-l},\\ldots, y_0, y_1, \\ldots, y_T\\), as in the parameter estimation of ARMA models, this leads to the conditional joint density function \\[\\begin{equation*}\n\\prod_{t=1}^{T} f_{y_t|\\ubar{\\mathbf{Y}}_{t-1}} = (2\\pi)^{-T/2} \\cdot\n\\prod_{t=1}^{T} h^{-1/2}_t \\cdot \\mathrm{exp}\\Big(-\\frac{1}{2} \\sum_{t=1}^{T} \\frac{u^2_t}{h_t} \\Big).\n\\end{equation*}\\] Denote the parameter vector of the AR-GARCH model \\(\\boldsymbol{\\vartheta}=\\left(\\boldsymbol{\\phi,\\delta}\\right)\\) where \\(\\boldsymbol{\\phi} = (\\nu, \\phi_1,\\ldots, \\phi_p)\\) contains parameters related to the conditional mean and \\(\\boldsymbol{\\delta} = (\\omega, \\alpha_1, \\ldots, \\alpha_s, \\beta_1, \\ldots, \\beta_r)\\) contains the parameters of the model for the conditional variance. The log-likelihood then becomes (omitting constant terms not dependent on the model parameters) \\[\\begin{equation*}\nl(\\boldsymbol{\\vartheta}) = \\sum_{t=1}^{T} l_t(\\boldsymbol{\\vartheta}) =  -\\frac{1}{2} \\sum_{t=1}^{T} \\mathrm{log}\\, h_t(\\boldsymbol{\\phi}, \\boldsymbol{\\delta}) - \\frac{1}{2} \\sum_{t=1}^{T} \\frac{u_t(\\boldsymbol{\\phi})^2}{h_t(\\boldsymbol{\\phi},\\boldsymbol{\\delta})},\n\\end{equation*}\\] with \\(u_t(\\boldsymbol{\\phi}) = y_t - \\nu - \\phi_1 y_{t-1} - \\cdots - \\phi_p y_{t-p}\\) and \\(h_t(\\boldsymbol{\\phi}, \\boldsymbol{\\delta})\\) are functions of the parameters \\(\\boldsymbol{\\phi}\\) and \\(\\boldsymbol{\\delta}\\). Maximization of the likelihood function is performed using numerical methods similar to the ARMA models.\n\nAs an example, in the GARCH(1,1) case \\(r=s=1\\) and hence \\(h_{t} = \\omega + \\beta_1 h_{t-1} + \\alpha_1 u_{t-1}^{2},\\) for the conditional variance. This means that \\(\\boldsymbol{\\delta} = (\\omega, \\alpha_1, \\beta_1)\\).\n\n \nIf the normality assumption of the error term \\(\\varepsilon_t \\thicksim \\mathsf{nid}(0,1)\\) made above holds, the usual asymptotic properties of the maximum likelihood estimator (consistency and asymptotic normality) hold. If the Gaussianity assumption is found inappropriate, one could either (or both):\n\nUse a non-Gaussian distribution, like the \\(t\\)-distribution for the error term. This leads to the different form for the log-likelihood function, but the general lines to obtain it are the same as above.\nModify the asymptotic distribution and interpret the maximum likelihood estimator (MLE) as quasi-MLE (QMLE).\n\nConcerning the QMLE approach, if the likelihood function is maximized at \\(\\boldsymbol{\\widehat{\\vartheta}}=(\\boldsymbol{\\widehat{\\phi},\\widehat{\\delta})}\\), it can be shown, even without the normality assumption and under general assumptions and regularity conditions, that it holds \\[\\begin{equation*}\n\\widehat{\\boldsymbol{\\vartheta}} \\underset{as}{\\sim}\\mathsf{N}\\left(\\boldsymbol{\\vartheta},\\boldsymbol{V}\\left(\\boldsymbol{\\vartheta}\\right)^{-1} \\boldsymbol{B} \\left(\\boldsymbol{\\vartheta}\\right)  \\boldsymbol{V}\\left(\\boldsymbol{\\vartheta}\\right)^{-1}\\right),\n\\end{equation*}\\] where \\[\\begin{equation*}\n    \\boldsymbol{V}\\left(\\boldsymbol{\\vartheta}\\right) = \\mathsf{E}\\left[-\\partial^{2}l(\\boldsymbol{\\vartheta})/\\partial\\boldsymbol{\\vartheta}\\partial\\boldsymbol{\\vartheta}^{\\prime}\\right]\n\\end{equation*}\\] and \\[\\begin{equation*}\n\\boldsymbol{B}\\left(\\boldsymbol{\\vartheta}\\right)  =\\mathsf{E}\\left[\\sum_{t=1}^{T}\\left(\\frac{\\partial}{\\partial\\boldsymbol{\\vartheta}}l_{t}(\\boldsymbol{\\vartheta})\\right)  \\left(\\frac{\\partial}{\\partial\\boldsymbol{\\vartheta}}l_{t}(\\boldsymbol{\\vartheta})\\right)^{\\prime}\\right],\n\\end{equation*}\\] where \\(l_{t}(\\boldsymbol{\\vartheta})\\) is as described above. Using the empirical counterparts of the matrices \\(\\boldsymbol{V} \\left(\\boldsymbol{\\vartheta}\\right)\\) and \\(\\boldsymbol{B}\\left(\\boldsymbol{\\vartheta}\\right)\\), namely \\[\\begin{equation*}\n    \\widehat{\\boldsymbol{V}}(\\boldsymbol{\\widehat{\\vartheta})}=-\\partial^{2}l(\\boldsymbol{\\widehat{\\vartheta}})/\\partial\\boldsymbol{\\vartheta}\\partial\\boldsymbol{\\vartheta}^{\\prime} \\quad \\mathrm{and} \\quad \\widehat{\\boldsymbol{B}}(\\boldsymbol{\\widehat{\\vartheta})}=\\sum_{t=1}^{T}\\left(  \\frac{\\partial}{\\partial\\boldsymbol{\\vartheta}}l_{t}(\\boldsymbol{\\widehat{\\vartheta}})\\right)\\left(\\frac{\\partial}{\\partial\\boldsymbol{\\vartheta}}l_{t}(\\boldsymbol{\\widehat{\\vartheta}})\\right)^{\\prime}\n\\end{equation*}\\] the asymptotic distribution of the estimator \\(\\boldsymbol{\\widehat{\\vartheta}}\\) presented above can be used to form approximate standard errors and Wald tests about the parameter \\(\\boldsymbol{\\vartheta}\\).\n\nIf the normality assumption (i.e. \\(\\varepsilon_t \\thicksim \\mathsf{nid}(0,1)\\)) holds, then \\(\\boldsymbol{V}\\left(\\boldsymbol{\\vartheta}\\right) = \\boldsymbol{B}\\left(\\boldsymbol{\\vartheta}\\right)\\), and the expressions above for the (QMLE) asymptotic distribution and the standard errors and Wald tests simplify to the usual MLE case.\nThe practical message from above is that it is often reasonable to rely on the QMLE-based asymptotic distribution result and resulting robust standard errors, such as so called Bollerslev-Wooldridge standard errors, for different parameter estimates of the AR-GARCH model. The rationale is that model specification, including the distribution assumption \\(\\varepsilon_t \\thicksim \\mathsf{nid}(0,1)\\), is often not entirely correctly specified. Hence allowing for the additional robustness to the possible misspecification through QMLE estimates is advisable.\n\n \nEmpirical example (continue). Consider the estimation result of the AR-GARCH model with Gaussian error term for the excess stock returns of the NASDAQ 100 index. For simplicity, fit a relatively simple AR(1)-GARCH(1,1) model (see above with selections \\(p=r=s=1\\)). That is, we specify\n\nAR(1) model for the conditional mean.\nGARCH(1,1) model for the conditional variance\n\nThat is, we estimate an AR(1) model with GARCH(1,1) errors. The estimation result of the maximum likelihood estimation, based on the normality assumption of \\(\\varepsilon_t\\), yields \\[\\begin{eqnarray*}\ny_{t} &=& \\underset{{\\left(0.013\\right) }}{0.087} - \\underset{{\\left(0.013\\right) }}{0.044} y_{t-1} +\n\\widehat{u}_{t}\n\\notag \\\\\n&& \\\\\n\\widehat{h}_{t} &=&\\underset{{\\left(0.006\\right) }}{0.035} +\\underset{{\\left(\n0.013\\right) }}{0.876}\\widehat{h}_{t-1}+\\underset{{\\left( 0.012 \\right) }}{0.105}\\widehat{u}_{t-1}^{2},  \\notag\n\\end{eqnarray*}\\] Here we report the robust standard errors under the parameter estimates (see the QMLE asymptotic distribution result). The full estimation result provided by rugarch package in R yields the following results:\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.087229    0.012889   6.7679 0.000000\nar1    -0.043797    0.014149  -3.0954 0.001965\nomega   0.035129    0.004868   7.2160 0.000000\nalpha1  0.104804    0.008170  12.8272 0.000000\nbeta1   0.876000    0.008975  97.6078 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.087229    0.012567   6.9412 0.000000\nar1    -0.043797    0.012630  -3.4677 0.000525\nomega   0.035129    0.006497   5.4066 0.000000\nalpha1  0.104804    0.012428   8.4331 0.000000\nbeta1   0.876000    0.012869  68.0716 0.000000\n\nLogLikelihood : -9030.906 \n\nInformation Criteria\n------------------------------------\n                   \nAkaike       3.1583\nBayes        3.1641\nShibata      3.1583\nHannan-Quinn 3.1603\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                     0.1471  0.7014\nLag[2*(p+q)+(p+q)-1][2]    0.6119  0.9317\nLag[4*(p+q)+(p+q)-1][5]    2.0557  0.6952\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      1.998  0.1575\nLag[2*(p+q)+(p+q)-1][5]     4.486  0.1993\nLag[4*(p+q)+(p+q)-1][9]     6.661  0.2292\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]   0.09937 0.500 2.000  0.7526\nARCH Lag[5]   4.68135 1.440 1.667  0.1217\nARCH Lag[7]   5.49080 2.315 1.543  0.1794\n\nThe estimation result shows well the point using the QMLE:\n\nThe estimated coefficients (“AR”,“ARCH” and “GARCH” estimates) are the same for “optimal paramaters” (that is, when we assume that the normality assumption \\(\\varepsilon_t\\) holds) and for QMLE (resulting robust estimation results), butthe estimated standard errors are different. Especially for the GARCH part this means, as often in these circumstances, that the robust standard errors are somwhat higher reflecting the additional uncertainty coming from possible model misspecification.\nAll in all, all the estimated coefficients are statistically significant at the conventional significance levels. Moreover, the typical pattern of the GARCH(1,1) model is also present where the GARCH parameter (here \\(\\beta_1\\)) is larger than the ARCH coefficient \\(\\alpha_1\\), and their sum is relatively close to 1. Estimated volatility (conditional standard deviation \\(\\widehat{h}^{1/2}_t\\)) of the estimated model is depicted below.\n\n\n\n\n\n\n\n\n\n\n\nFigure: Estimated conditional standard deviation based on the AR(1)-GARCH(1,1) model for the NASDAQ 100 excess returns.\n\n \nResidual diagnostics show that:\n\nThere is no remaining autocorrelation in the residuals and squared residuals (based on the Ljung-Box and the McLeod-Li tests and residual autocorrelation coefficients).\nWithout presenting details, (weighted) ARCH LM test tests the null hypothesis that there is no signs of remaining conditional heteroskedasticity in the residuals \\(\\widehat{u}_t\\). In the estimation result above, the p-values of the ARCH LM tests show that the parsimonious AR(1)-GARCH(1,1) is an adequate model.\nThere is some deviation from the normality assumption in the (standardized) residuals. This suggests that the estimates should be interpreted as QMLEs, as above.\n\n\n\n\n\n\n\n\n\n\n\nFigure: Histogram and Q-Q plot of (standardized) residuals.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Volatility modelling: AR-GARCH model</span>"
    ]
  },
  {
    "objectID": "TSE-ch9.html#forecasting",
    "href": "TSE-ch9.html#forecasting",
    "title": "10  Volatility modelling: AR-GARCH model",
    "section": "10.5 Forecasting",
    "text": "10.5 Forecasting\nAssume observations are available up to time \\(t\\) and the object of interest is to forecast the future values of \\(y_{t+k}\\) (\\(k\\geq1\\)). In addition to stationarity, assume that \\(\\mathsf{E}\\left(y_{t}^{2}\\right)&lt;\\infty\\).\nForecasts for \\(y_t\\). From the first equation of the AR-GARCH model, that is the model for the conditional variance, it can be seen that the optimal (in the mean square sense) one-step-ahead forecast is \\[\\begin{equation*}\n\\mathsf{E}_{t}\\left(y_{t+1}\\right) = \\nu + \\phi_{1}y_{t}+\\cdots+\\phi_{p}y_{t+1-p}.\n\\end{equation*}\\] When \\(k\\geq2\\), it can be seen that (cf. forecasting formulae for AR(MA) models) \\[\\begin{equation*}\n    \\mathsf{E}_{t}(u_{t+k})=\\mathsf{E}_{t}\\left[  \\mathsf{E}_{t+k-1}(u_{t+k})\\right]=0,\n\\end{equation*}\\] so that \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(y_{t+k}\\right) = \\nu + \\phi_{1}\\mathsf{E}_{t}(y_{t+k-1})+\\cdots+\\phi_{p}\\mathsf{E}_{t}(y_{t+k-p}), \\quad k=1,2,\\ldots,\n\\end{equation*}\\] where \\(\\mathsf{E}_{t}(y_{t+k-j})=y_{t+k-j}\\) for \\(j\\geq k\\).\n\nIn conclusion, from these and the forecasting formulae for an AR(\\(p\\)) process with a homoskedastic error term, it can be concluded that the optimal forecasts can be formed recursively exactly in the same manner as in the homoskedastic case.\nIn practice, unknown parameters naturally need to be replaced with their estimates, which are based on the finite sample sizes, and hence numerically here forecasts can be slightly different than obtained with an AR(\\(p\\)) model with conditionally homoskedastic errors.\n\n \nAs the above shows, conditional heteroskedasticity does not affect the forecasting formulae, which are the same as in the AR case discussed earlier. However, the presence of conditional heteroskedasticity changes how the prediction intervals are computed (see Extra below).\n\nExtra: Prediction intervals under conditional heteroskedasticity\n\n\nTo see this, note that because the forecasts can be derived exactly as in the homoskedastic AR(\\(p\\)) model, the same also holds for the \\(k\\) step forecast errors. Based on the calculations above, \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(  u_{t+k-j}\\right)  =\\left\\{\n    \\begin{array}{cc}\n         & u_{t+k-j}, \\quad \\mathrm{when} \\quad k\\leq j \\\\\n         & 0, \\quad \\mathrm{when} \\quad k &gt; j,\n    \\end{array}\n    \\right.\n\\end{equation*}\\] so that \\[\\begin{equation*}\n    y_{t+k}-\\mathsf{E}_{t}(y_{t+k})=\\sum_{j=0}^{k-1}\\psi_{j}u_{t+k-j}=\\sum_{j=0}^{k-1}\\psi_{j}h_{t+k-j}^{1/2}\\varepsilon_{t+k-j},\n\\end{equation*}\\] where \\(\\psi_{j}\\) are again the coefficients of the power series \\(\\psi\\left(z\\right)=\\phi\\left(z\\right)^{-1}=\\sum_{j=0}^{\\infty}\\psi_{j}z^{j}\\) (\\(\\psi_{0}=1\\)).\nLet us focus our attention on the one-step-ahead forecast error, whose conditional expected value becomes \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left[y_{t+1}-\\mathsf{E}_{t}(y_{t+1})\\right]  =\\mathsf{E}_{t}(u_{t+1})=0\n\\end{equation*}\\] with conditional variance \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left[\\left(y_{t+1}-\\mathsf{E}_{t}\\left(y_{t+1}\\right)\\right)^{2}\\right]=\\mathsf{E}_{t}\\left(u_{t+1}^{2}\\right)=h_{t+1}.\n\\end{equation*}\\] This shows that the one-step-ahead prediction error is conditionally heteroskedastic, that is, the forecast accuracy depends on what kind of values the process attained at the forecast origin (at time of forecast computation) and just before it.\n\nAs can be seen from the expression of the forecast error, a similar result holds also for forecast horizons longer than one. Therefore, any analysis of forecast accuracy, such as prediction intervals is sensible to be based on the conditional distribution with conditioning on the history up until the date of forecasting.\n\nIf it is assumed that \\(\\varepsilon_{t}\\sim\\mathsf{nid}\\left(  0,1\\right)\\), then the conditional distribution of the one-step-ahead forecast error \\(u_{t}\\) (conditioned on \\(\\left\\{y_{t-1},y_{t-2},\\ldots\\right\\}\\)) is Gaussian with mean zero and variance \\(h_{t}\\).\n\nUsing this result, one can form prediction intervals for the one-step-ahead forecast exactly as for ARMA process.\nFor multiple step ahead forecasts, the situation is not that straightforward, because the conditional distributions of multiple step forecast errors are not Gaussian and have no simple expressions. Therefore, forming prediction intervals for multiple step ahead forecasts is more complicated.\n\n\n \nVolatility forecasts. Concerning forecasting the conditional variance, we assume that observations up to and including time \\(t\\) (i.e. the forecast origin at time \\(t\\)) are available, and that forecasts for \\(h_{t+1}\\), \\(h_{t+2}\\), are desired.\nAs \\(h_{t+1}\\) is a (deterministic) function of the variables \\(y_{t},y_{t-1},\\ldots\\), and thus the first conditional variance we need to forecast is \\(h_{t+2}\\). For simplicity, let us concentrate on the GARCH(1,1) case. Taking conditional expected values (conditional on \\(\\left\\{y_{t},y_{t-1},\\ldots\\right\\}\\)) of both sides of \\[\\begin{equation*}\n    h_{t+2}=\\omega+\\beta_1 h_{t+1}+\\alpha_1 u_{t+1}^{2}\n\\end{equation*}\\] yields the optimal (in the mean squared error sense) forecast of \\(h_{t+2}\\) as \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(h_{t+2}\\right)=\\omega + \\beta_1\\mathsf{E}_{t}\\left(h_{t+1}\\right)+\\alpha_1 \\mathsf{E}_{t}\\left(u_{t+1}^{2}\\right),\n\\end{equation*}\\] where \\(\\mathsf{E}_{t}\\left(h_{t+1}\\right)=h_{t+1}\\) (see CEV4). Moreover, we obtain that (see CEV2 and CEV4) \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(u_{t+1}^{2}\\right) = \\mathsf{E}_{t}\\left(h_{t+1}\\varepsilon_{t+1}^{2}\\right)=h_{t+1}\\mathsf{E}_{t}\\left(\\varepsilon_{t+1}^{2}\\right)=h_{t+1}\\mathsf{E}\\left(\\varepsilon_{t+1}^{2}\\right)=h_{t+1},\n\\end{equation*}\\] so that \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(h_{t+2}\\right)=\\omega+(\\alpha_1+\\beta_1)h_{t+1}.\n\\end{equation*}\\]\nWhen the forecast horizon is \\(k\\geq3\\), in a similar fashion we obtain \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(h_{t+k}\\right) = \\omega+\\beta_1 \\mathsf{E}_{t}\\left(h_{t+k-1}\\right)+\\alpha_1 \\mathsf{E}_{t}\\left(u_{t+k-1}^{2}\\right),\n\\end{equation*}\\] and finally \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(h_{t+k}\\right)=\\omega \\sum_{j=0}^{k-2}\\left(\\alpha_1+\\beta_1\\right)^{j}+\\left(\\alpha_1+\\beta_1\\right)^{k-1}h_{t+1}, \\quad  k=2,3,\\ldots,\n\\end{equation*}\\] where \\(h_{t+1}\\) is a function of variables \\(\\left\\{y_{t},y_{t-1},\\ldots\\right\\}\\) known at the time when forecasts are constructed.\n \n\nExtra: Details for forecasting formulae multiple periods ahead\n\n\nConsider the part \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(h_{t+k}\\right) = \\omega+\\beta_1 \\mathsf{E}_{t}\\left(h_{t+k-1}\\right)+\\alpha_1 \\mathsf{E}_{t}\\left(u_{t+k-1}^{2}\\right),\n\\end{equation*}\\] Here \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(u_{t+k-1}^{2}\\right)=\\mathsf{E}_{t}\\left(h_{t+k-1}\\varepsilon_{t+k-1}^{2}\\right) = \\mathsf{E}_{t}\\left[\\mathsf{E}_{t+k-2}\\left(h_{t+k-1}\\varepsilon_{t+k-1}^{2}\\right)\\right],\n\\end{equation*}\\] where the latter equality can be justified based on a generalization of property CEV3, that is, a generalization of the law of iterated expectations.\nThis generalization says that \\(\\mathsf{E}\\left(Y\\left\\vert X_{2}\\right.\\right)=\\mathsf{E}\\left[\\mathsf{E}\\left(Y\\left\\vert X_{1}\\right.\\right)\\left\\vert X_{2}\\right.\\right]\\), when the components of the (potentially infinite-dimensional) random vector \\(X_{2}\\) are a subset of the components of \\(X_{1}\\) (or more generally, \\(X_{2}\\) is a function of \\(X_{1}\\)).\nBecause \\[\\begin{equation*}\n    \\mathsf{E}_{t+k-2}\\left(h_{t+k-1}\\varepsilon_{t+k-1}^{2}\\right)=h_{t+k-1}\\mathsf{E}_{t+k-2}\\left(\\varepsilon_{t+k-1}^{2}\\right)=h_{t+k-1},\n\\end{equation*}\\] we obtain \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(u_{t+k-1}^{2}\\right)=\\mathsf{E}_{t}\\left(h_{t+k-1}\\right), \\quad k=3,4,\\ldots\\text{ .}\n\\end{equation*}\\] To summarize, we have shown that \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(h_{t+k}\\right) = \\omega+(\\alpha_1+\\beta_1)\\mathsf{E}_{t}\\left(h_{t+k-1}\\right),\\quad k=2,3,\\ldots\\text{ .}\n\\end{equation*}\\] Because \\(\\mathsf{E}_{t}\\left(h_{t+1}\\right)=h_{t+1}\\), we inductively obtain the solution \\[\\begin{equation*}\n    \\mathsf{E}_{t}\\left(h_{t+k}\\right)=\\omega \\sum_{j=0}^{k-2}\\left(\\alpha_1+\\beta_1\\right)^{j}+\\left(\\alpha_1+\\beta_1\\right)^{k-1}h_{t+1}, \\quad  k=2,3,\\ldots,\n\\end{equation*}\\] where \\(h_{t+1}\\) is a function of variables \\(\\left\\{y_{t},y_{t-1},\\ldots\\right\\}\\) known at the time of forecasting.\n\n \nIn practice,\n\nthe unknown parameters \\(\\alpha_1\\), \\(\\beta_1\\), and \\(\\omega\\) have to be replaced by corresponding estimates.\nUnlike in ARMA models, the quantity being predicted is now unobserved, although it can be computed using GARCH model equation for all \\(t\\geq1\\) as long as parameter values and required initial values for \\(h_t\\) and \\(y_t\\) are available. A common choice in practice is to use the sample variance of the observed time series as the initial value \\(h_{0}\\). In the stationary case, the effect of the initial values diminishes as \\(t\\) increases.\nForecasting with a more general GARCH(\\(r,s\\)) model is carried out in principle in the same way as outlined above, although the resulting forecasting formulae become more cumbersome. Deriving interval predictions is also complicated, one major reason for this being that the distribution of the conditional variance deviates heavily from a Gaussian distribution.\nOverall, volatility forecasting is a separate and a large area in financial econometrics that we are not considering in this course more detail.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Volatility modelling: AR-GARCH model</span>"
    ]
  },
  {
    "objectID": "TSE-ch9.html#garch-in-mean-model",
    "href": "TSE-ch9.html#garch-in-mean-model",
    "title": "10  Volatility modelling: AR-GARCH model",
    "section": "10.6 GARCH-in-mean model",
    "text": "10.6 GARCH-in-mean model\nIn the GARCH-in-mean (GARCH-M) model, the conditional variance is allowed to directly affect the conditional mean as well.\nTo simplify the notation, let us consider the first-order special case, i.e., the AR(1)-GARCH(1,1)-M model \\[\\begin{eqnarray*}\n\\nonumber y_t = \\nu + \\phi_1 y_{t-1} + \\delta g(h_t) + u_t, \\quad\nh_t = \\omega + \\alpha_1 u^2_{t-1} + \\beta_1 h_{t-1},\n\\end{eqnarray*}\\] where, as before, \\(u_t = y_t - \\nu - \\phi_1 y_{t-1} - \\delta g(h_t)\\), but now the conditional variance \\(h_t\\) also affects the level of \\(y_t\\) through the function \\(g(h_t)\\). Depending on the situation, the “in-mean effect” can be defined\n\n\\(g(h_t) = h_t\\),\n\\(g(h_t) = \\sqrt{h_t}\\), or\n\\(g(h_t) = \\mathrm{log} (h_t)\\).\n\nA positive coefficient \\(\\delta\\) means that the value of \\(y_t\\) increases when the conditional variance increases.\n\nAs an extension of the GARCH-M version presented above, the orders of the AR and GARCH models can naturally be greater than 1.\nIn general, instead of the AR(\\(p\\)) model, another suitable model for the conditional mean of \\(y_t\\) can be used.\nSimilarly, a model other than GARCH(1,1) can be chosen for the GARCH part.\n\nThe GARCH-M model is used, for example, in financial econometrics (empirical finance) to model the fundamental risk-return relation, where risk (here volatility, measured by the conditional variance or its transformation) is allowed to directly affect the expected return of a security.\nEmpirical example (continue). Let us consider an GARCH-in-mean extension of the AR(1)-GARCH(1,1) model (with Gaussian innovations) obtained above for the NASDAQ 100 excess stock returns. That is we consider the following model for the conditional mean \\[\\begin{eqnarray*}\n\\nonumber y_t = \\nu + \\phi_1 y_{t-1} + \\delta \\sqrt{h_t} + u_t.\n\\end{eqnarray*}\\] That is we include the conditional standard deviation to the conditional mean.\n\nThis is one possible way to examine the fundamental risk-return relationship in (excess) stock returns (here NASDAQ 100 index). The positive risk-return relation is the cornerstone of financial economics.\n\nQMLE-based estimation result obtained with the rugarch package on AR(1)-GARCH(1,1)-M model:\n\nRobust Standard Errors:\n        Estimate  Std. Error   t value Pr(&gt;|t|)\nmu      0.000963    0.043060  0.022373 0.982151\nar1    -0.043796    0.012596 -3.477072 0.000507\narchm   0.085665    0.040331  2.124019 0.033669\nomega   0.035116    0.006429  5.462160 0.000000\nalpha1  0.104836    0.012296  8.526169 0.000000\nbeta1   0.875948    0.012702 68.958838 0.000000\n\nTherefore, it appears that the estimated coefficient of \\(\\delta\\), here “archm”, is positive and also statistically significant at the 5 % significance level based on the robust \\(t\\)-value. Even though the statistical significance is not very strong, the positive risk-return relation can be approved.\n\nResidual diagnostics of this GARCH-M model is essentially the same as obtained without the in-mean effect, and hence the model seems adequate.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Volatility modelling: AR-GARCH model</span>"
    ]
  },
  {
    "objectID": "TSE-ch9.html#r-lab",
    "href": "TSE-ch9.html#r-lab",
    "title": "10  Volatility modelling: AR-GARCH model",
    "section": "10.7 R Lab",
    "text": "10.7 R Lab\n\nR Lab: GARCH and GARCH-M models\n\n\n\n# GARCH and GARCH-M MODELS\n\n# Load the libraries\nlibrary(quantmod)\nlibrary(rugarch)\nlibrary(forecast)\n\n# Set locale to English for dates\nSys.setlocale(\"LC_TIME\", \"English\")\n\n[1] \"\"\n\n## User Inputs\n# --------------------------------------------------------------------------\nstart_date &lt;- \"2003-01-01\" # Start after the dot-com bubble burst\n#end_date   &lt;- Sys.Date()   # Use current date for the end period\nend_date   &lt;- \"2025-09-30\"   # Use current date for the end period\n\n## Data Acquisition and Preparation (NASDAQ 100)\n# --------------------------------------------------------------------------\n# Download NASDAQ 100 (^NDX) and 3-Month T-Bill (^IRX) data\nNDX &lt;- getSymbols(\"^NDX\", src = \"yahoo\", from = start_date, to = end_date, auto.assign = FALSE)\n\nIRX &lt;- getSymbols(\"^IRX\", src = \"yahoo\", from = start_date, to = end_date, auto.assign = FALSE)\n\n# Calculate daily log returns\nnasdaq_returns &lt;- dailyReturn(NDX, type = 'log')\n\n# Prepare daily risk-free rate\nrf_daily &lt;- na.locf(IRX$IRX.Adjusted) / 100 / 252\n\n# Align time series and calculate excess returns\nreturns_data &lt;- merge(nasdaq_returns, rf_daily, join = 'inner')\nexcess_returns &lt;- returns_data$daily.returns - returns_data$IRX.Adjusted\ncolnames(excess_returns) &lt;- \"ExcessReturn\"\n\n# Percentages Remove any NA values to ensure a clean series\nexcess_returns &lt;- na.omit(excess_returns)*100\n\n# Plot the excess returns to see the volatility\nplot(excess_returns, main = \"\")\n# Add the title manually above the plot area (line = 2 is often default, \n# use line = 3 or 4 to move it up)\nmtext(text = paste(\"NASDAQ 100 Daily Excess Returns:\", start_date, \"to\", end_date), \n      side = 3, # Side 3 is the top\n      line = 3.3, # Move it up to line 3.5\n      cex = 1.0) # Optional: adjust text size\n\n\n\n\n\n\n\n#===========================================================================#\n#===========================================================================#\n# # Function to Export Excess Returns to CSV (Simpler Method)\n# # -------------------------------------------------------------------\n# export_excess_returns_to_csv &lt;- function(ts_data, filename = \"NASDAQ_Excess_Returns.csv\") {\n  \n#  # 1. Convert the xts object to a data frame, preserving the Date index\n#  # We explicitly name the columns for clarity in the CSV header.\n#  data_df &lt;- data.frame(\n#    Date = index(ts_data),\n#    ExcessReturn = coredata(ts_data)\n#  )\n  \n#  # 2. Write the data frame to a CSV file\n#  # row.names = FALSE prevents R from writing an unnecessary row number column.\n#  # The file is overwritten if it already exists.\n#  tryCatch({\n#    write.csv(\n#      x = data_df,\n#      file = filename,\n#      row.names = FALSE \n#    )\n#    cat(paste0(\"✅ Successfully exported '\", deparse(substitute(ts_data)), \"' to \", filename, \"\\n\"))\n    \n#  }, error = function(e) {\n#    cat(paste0(\"❌ Error exporting data: \", e$message, \"\\n\"))\n#  })\n#}\n\n# ## Example Usage (Exporting only the 'excess_returns')\n#export_excess_returns_to_csv(\n#  ts_data = excess_returns,\n#  filename = \"NASDAQ_Daily_Excess_Returns.csv\"\n#)\n# #Output CSV will look like this:\n# #Date,ExcessReturn\n# #2000-01-03,0.001234\n# #2000-01-04,-0.005678\n#\n#===========================================================================#\n#===========================================================================#\n\n\n## Preliminary examination (autocorrelations in returns and squared returns)\n# --------------------------------------------------------------------------\n# Plot ACF and PACF for the returns series\npar(mfrow=c(1,2))\nAcf(excess_returns, main=\"ACF of excess returns\")\nPacf(excess_returns, main=\"PACF of excess returns\") \n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n\n# --- Extended Ljung-Box test on possible autocorrelation ---\n\ncat(\"\\nExtended Ljung-Box Test on Excess Returns (for mean dependence):\\n\")\n\n\nExtended Ljung-Box Test on Excess Returns (for mean dependence):\n\nlags_to_test &lt;- c(5, 10, 15, 20) # Test relevant lags (e.g., weekly, bi-weekly)\nlb_results_returns &lt;- data.frame(Lag=integer(), \"Q-statistic\"=double(), \"p-value\"=double())\n\nfor (l in lags_to_test) {\n  # fitdf=0 assumes no model has been fit yet\n  test &lt;- Box.test(excess_returns, type = \"Ljung-Box\", lag = l, fitdf = 0)\n  lb_results_returns[nrow(lb_results_returns) + 1,] &lt;- \n    c(l, round(test$statistic, 3), round(test$p.value, 3))\n}\nprint(lb_results_returns)\n\n  Lag Q.statistic p.value\n1   5      62.373       0\n2  10     101.596       0\n3  15     127.425       0\n4  20     144.725       0\n\n# ----------- Autocorrelation in squared returns ---------------\ncat(\"\\n--- Autocorrelation in squared returns (possible need for ARCH/GARCH models) ---\\n\")\n\n\n--- Autocorrelation in squared returns (possible need for ARCH/GARCH models) ---\n\nAcf(excess_returns^2, main=\"ACF of squared excess returns\")\n\n\n\n\n\n\n\n# McLeod-Li test (preliminary test for ARCH/GARCH effects on returns)\nlags_to_test &lt;- c(5, 10, 15, 20)\nml_results_squared &lt;- data.frame(Lag = integer(), p_value = numeric())\nfor (l in lags_to_test) {\n  test &lt;- TSA::McLeod.Li.test(y = excess_returns, gof.lag = l)\n  ml_results_squared[nrow(ml_results_squared) + 1, ] &lt;- c(l, round(test$p.values[l], 3))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncat(\"\\nExtended McLeod-Li Test (Low p-value suggests GARCH effects):\\n\")\n\n\nExtended McLeod-Li Test (Low p-value suggests GARCH effects):\n\nprint(ml_results_squared)\n\n  Lag p_value\n1   5       0\n2  10       0\n3  15       0\n4  20       0\n\n#===========================================================================#\n#===========================================================================#\n\n## Standard ARMA(1,0)-GARCH(1,1) Model Fitting\n# -------------------------------------------------------------------\n\n# Define lag lengths for ARMA model, typically, e.g., AR(1) or ARMA(0,0) for returns\n# Based on common financial literature and often confirmed by the ACF/PACF plots:\narma_order &lt;- c(1, 0) # Use AR(1) in the mean equation as a common starting point\n\n# GARCH model using Gaussian distribution \n# -------------------------------------------------------------------\nsGARCH_spec_norm &lt;- ugarchspec(\n  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n  mean.model = list(\n    armaOrder = arma_order, \n    include.mean = TRUE,\n    archm = FALSE\n  ),\n  distribution.model = \"norm\" # Gaussian (Normal) Distribution\n)\n\ncat(\"\\n[2] Fitting Standard ARMA(1,0)-GARCH(1,1) Model with Gaussian Dist...\\n\")\n\n\n[2] Fitting Standard ARMA(1,0)-GARCH(1,1) Model with Gaussian Dist...\n\nsGARCH_fit_norm &lt;- ugarchfit(spec = sGARCH_spec_norm, data = excess_returns)\nprint(sGARCH_fit_norm)\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.087228    0.012889   6.7679 0.000000\nar1    -0.043798    0.014149  -3.0955 0.001965\nomega   0.035129    0.004868   7.2160 0.000000\nalpha1  0.104804    0.008170  12.8272 0.000000\nbeta1   0.876000    0.008975  97.6084 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.087228    0.012567   6.9411 0.000000\nar1    -0.043798    0.012630  -3.4678 0.000525\nomega   0.035129    0.006497   5.4066 0.000000\nalpha1  0.104804    0.012428   8.4331 0.000000\nbeta1   0.876000    0.012869  68.0719 0.000000\n\nLogLikelihood : -9030.906 \n\nInformation Criteria\n------------------------------------\n                   \nAkaike       3.1583\nBayes        3.1641\nShibata      3.1583\nHannan-Quinn 3.1603\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                     0.1471  0.7013\nLag[2*(p+q)+(p+q)-1][2]    0.6119  0.9317\nLag[4*(p+q)+(p+q)-1][5]    2.0557  0.6952\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      1.998  0.1575\nLag[2*(p+q)+(p+q)-1][5]     4.486  0.1993\nLag[4*(p+q)+(p+q)-1][9]     6.661  0.2292\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]   0.09938 0.500 2.000  0.7526\nARCH Lag[5]   4.68140 1.440 1.667  0.1217\nARCH Lag[7]   5.49086 2.315 1.543  0.1794\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  1.9784\nIndividual Statistics:              \nmu     0.13778\nar1    0.06113\nomega  0.13219\nalpha1 0.15678\nbeta1  0.32215\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.28 1.47 1.88\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                    t-value      prob sig\nSign Bias           2.72947 6.363e-03 ***\nNegative Sign Bias  0.07265 9.421e-01    \nPositive Sign Bias  2.11258 3.468e-02  **\nJoint Effect       30.89455 8.946e-07 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     194.4    4.382e-31\n2    30     218.9    4.838e-31\n3    40     243.2    2.493e-31\n4    50     251.7    4.746e-29\n\n\nElapsed time : 0.1198461 \n\n# Extract the standardized residuals (should follow the assumed distribution: 'std')\nstd_residuals &lt;- residuals(sGARCH_fit_norm, standardize = TRUE)\n# Convert the xts object to a simple vector for plotting\nstd_residuals_vec &lt;- as.numeric(std_residuals) \n\n# Set up the plotting area for two side-by-side plots\npar(mfrow=c(1, 2))\npar(mar = c(5.1, 4.1, 4.1, 2.1)) # Reset margins\n\n# A. Histogram of Standardized Residuals\nhist(std_residuals_vec, breaks = 50, # Use more bins for smooth visualization\n     freq = FALSE,  main = \"\", xlab = \"\")\n# Add the density curve of the assumed Standard Normal distribution (mean 0, SD 1)\ncurve(dnorm(x, mean = 0, sd = 1), \n      col = \"blue\", lwd = 2, add = TRUE)\n\n# Q-Q Plot of Standardized Residuals\nqqnorm(std_residuals_vec,  main = \"\", ylab = \"Sample Quantiles\",\n       xlab = \"Theoretical Quantiles\")\n# Add the theoretical Normal distribution line (qqline uses dnorm/qnorm by default)\nqqline(std_residuals_vec, col = \"red\", lwd = 2) \n\n\n\n\n\n\n\n# Note: Since the default qqline() assumes a Normal distribution, \n# you can omit the explicit 'distribution' argument for a Normal fit.\n\n# Reset the plotting area\npar(mfrow=c(1, 1))\n\n\n#  For comparison, a Model using Student's t-Distribution \n# -------------------------------------------------------------------\n#  (standard practice for data with heavy tails, like asset returns)\nsGARCH_spec_std &lt;- ugarchspec(\n  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n  mean.model = list(\n    armaOrder = arma_order, \n    include.mean = TRUE,\n    archm = FALSE          # Explicitly disable GARCH-M\n  ),\n  distribution.model = \"std\" # Student's t-distribution (to capture heavy tails)\n)\n\ncat(\"\\n[1] Fitting Standard ARMA(1,0)-GARCH(1,1) Model with Student's t-Dist...\\n\")\n\n\n[1] Fitting Standard ARMA(1,0)-GARCH(1,1) Model with Student's t-Dist...\n\nsGARCH_fit_t &lt;- ugarchfit(spec = sGARCH_spec_std, data = excess_returns)\nprint(sGARCH_fit_t)\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : std \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.114676    0.012170   9.4231 0.000000\nar1    -0.042438    0.013406  -3.1656 0.001547\nomega   0.024663    0.005087   4.8483 0.000001\nalpha1  0.108962    0.009871  11.0390 0.000000\nbeta1   0.882977    0.009653  91.4727 0.000000\nshape   6.416826    0.565187  11.3535 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.114676    0.011663   9.8328 0.000000\nar1    -0.042438    0.011501  -3.6900 0.000224\nomega   0.024663    0.005676   4.3448 0.000014\nalpha1  0.108962    0.011927   9.1355 0.000000\nbeta1   0.882977    0.011556  76.4056 0.000000\nshape   6.416826    0.593852  10.8054 0.000000\n\nLogLikelihood : -8926.771 \n\nInformation Criteria\n------------------------------------\n                   \nAkaike       3.1223\nBayes        3.1292\nShibata      3.1223\nHannan-Quinn 3.1247\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                     0.1241  0.7246\nLag[2*(p+q)+(p+q)-1][2]    0.6215  0.9283\nLag[4*(p+q)+(p+q)-1][5]    2.1915  0.6557\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      1.570  0.2102\nLag[2*(p+q)+(p+q)-1][5]     3.412  0.3369\nLag[4*(p+q)+(p+q)-1][9]     5.269  0.3909\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3] 0.0001625 0.500 2.000  0.9898\nARCH Lag[5] 3.6196421 1.440 1.667  0.2117\nARCH Lag[7] 4.4431113 2.315 1.543  0.2871\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  3.1217\nIndividual Statistics:              \nmu     0.31901\nar1    0.04839\nomega  0.50894\nalpha1 0.30348\nbeta1  0.75538\nshape  0.96792\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.49 1.68 2.12\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value      prob sig\nSign Bias           2.7538 5.910e-03 ***\nNegative Sign Bias  0.6101 5.418e-01    \nPositive Sign Bias  2.4020 1.634e-02  **\nJoint Effect       31.1906 7.750e-07 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     96.64    2.166e-12\n2    30    117.01    1.571e-12\n3    40    130.35    9.011e-12\n4    50    123.47    2.329e-08\n\n\nElapsed time : 0.317642 \n\n## Visualization of Conditional Volatility (Standard Deviation)\n# -------------------------------------------------------------------\n# Extract the conditional standard deviation (sigma) from the preferred model (t-dist)\n# sigma(sGARCH_fit_std) returns an xts object of the conditional standard deviation.\nsigma_t &lt;- sigma(sGARCH_fit_norm)\n# sigma_t &lt;- sigma(sGARCH_fit_t)\n\n# Use the previous margin settings for a clean plot\npar(mar = c(5.1, 4.1, 4.1, 2.1)) \n\n# Plot the Conditional Standard Deviation\nplot(sigma_t, \n  #   main = \"Fitted Conditional Standard Deviation (Risk) of NASDAQ 100\",\n     main = \"\",\n     ylab = \"Conditional Standard Deviation\",\n     xlab = \"Year\",\n     xaxt = 'n') # Suppress default x-axis labels\n\n# Add custom x-axis labels for the years (reusing the logic from before)\nyearly_indices &lt;- endpoints(sigma_t, on = \"years\")\nyearly_indices &lt;- yearly_indices[yearly_indices &gt; 0] \nbreak_points &lt;- index(sigma_t)[yearly_indices + 1] \n\naxis.Date(side = 1,                 \n          x = index(sigma_t),\n          at = break_points,        \n          format = \"%Y\",            \n          tcl = -0.5,               \n          cex.axis = 0.9)\n\n\n\n\n\n\n\n#===========================================================================#\n#===========================================================================#\n\n## ARMA(1,0)-GARCH(1,1)-M Model (Gaussian Innovations)\n# -------------------------------------------------------------------\n\narma_order &lt;- c(1, 0) # Use AR(1) in the mean equation as a common starting point\n\nsGARCH_M_spec_norm &lt;- ugarchspec(\n  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n  mean.model = list(\n    armaOrder = arma_order, \n    include.mean = TRUE,\n    archm = TRUE,           # ENABLE GARCH-M\n    archpow = 1             # Use conditional Standard Deviation (sigma_t)\n  ),\n  distribution.model = \"norm\" # \"norm\" for Gaussian errors\n)\n\ncat(\"\\nFitting Standard ARMA(1,0)-GARCH(1,1)-M Model with Gaussian Innovations...\\n\")\n\n\nFitting Standard ARMA(1,0)-GARCH(1,1)-M Model with Gaussian Innovations...\n\nsGARCH_M_fit_norm &lt;- ugarchfit(spec = sGARCH_M_spec_norm, data = excess_returns)\nprint(sGARCH_M_fit_norm)\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.000958    0.043445  0.02205 0.982408\nar1    -0.043797    0.014146 -3.09616 0.001960\narchm   0.085669    0.041125  2.08316 0.037237\nomega   0.035116    0.004880  7.19531 0.000000\nalpha1  0.104836    0.008174 12.82560 0.000000\nbeta1   0.875948    0.008990 97.44068 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error   t value Pr(&gt;|t|)\nmu      0.000958    0.042979  0.022289 0.982217\nar1    -0.043797    0.012596 -3.477172 0.000507\narchm   0.085669    0.040261  2.127817 0.033352\nomega   0.035116    0.006429  5.462242 0.000000\nalpha1  0.104836    0.012296  8.526287 0.000000\nbeta1   0.875948    0.012702 68.958979 0.000000\n\nLogLikelihood : -9028.731 \n\nInformation Criteria\n------------------------------------\n                   \nAkaike       3.1579\nBayes        3.1649\nShibata      3.1579\nHannan-Quinn 3.1603\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                     0.2175  0.6409\nLag[2*(p+q)+(p+q)-1][2]    0.5861  0.9403\nLag[4*(p+q)+(p+q)-1][5]    1.6563  0.8069\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      1.987  0.1587\nLag[2*(p+q)+(p+q)-1][5]     4.523  0.1956\nLag[4*(p+q)+(p+q)-1][9]     6.715  0.2242\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]    0.1205 0.500 2.000  0.7285\nARCH Lag[5]    4.7158 1.440 1.667  0.1195\nARCH Lag[7]    5.5140 2.315 1.543  0.1775\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  2.0765\nIndividual Statistics:              \nmu     0.18139\nar1    0.05804\narchm  0.18419\nomega  0.12958\nalpha1 0.15311\nbeta1  0.30998\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.49 1.68 2.12\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value      prob sig\nSign Bias           2.7404 6.155e-03 ***\nNegative Sign Bias  0.1819 8.557e-01    \nPositive Sign Bias  2.0326 4.214e-02  **\nJoint Effect       29.7181 1.582e-06 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     206.1    2.061e-33\n2    30     237.4    1.416e-34\n3    40     263.9    3.472e-35\n4    50     264.0    3.115e-31\n\n\nElapsed time : 0.39013 \n\n# Check the 'archm' coefficient (in-mean parameter)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Volatility modelling: AR-GARCH model</span>"
    ]
  },
  {
    "objectID": "TSE-ch10.html",
    "href": "TSE-ch10.html",
    "title": "11  Multivariate time series models",
    "section": "",
    "text": "11.1 Predictive regressions: Background and starting point\nLet us assume, for simplicity, throughout this section that we have two sets of variables: \\(y_t\\) is still the dependent variable and one (\\(M=1\\)) or generally multiple (\\(M&gt;1\\)) additional predictive variables \\((x_{1t}, \\ldots, x_{Mt})\\). We are mainly interested in a linear regression model, but now with time series, where the lags of predictive variables are used as predictors. For example, one possible model specification is \\[\\begin{equation*}\ny_t= \\beta_0 + x_{1, t-1} \\beta_1 + \\cdots + x_{M, t-1} \\beta_M + u_t \\equiv \\boldsymbol{x}^{\\prime}_t \\boldsymbol{\\beta} + u_t,\n\\end{equation*}\\] where \\(\\boldsymbol{x}_t\\) contains predictors and \\(u_t\\) is \\(\\mathsf{iid}\\) error term or at least serially uncorrelated and uncorrelated with the regressors \\(\\boldsymbol{x}_t\\). The parameters of this type of model, contained in \\(\\boldsymbol{\\beta}\\), can be estimated by the (conditional) least squares (in a similar fashion as discussed, e.g., in connection to the AR(\\(p\\)) model. That is minimizing the OLS criterion (with required initial values) \\[\\begin{equation*}\n\\widehat{\\boldsymbol{\\beta}} = \\mathrm{arg}\\,\\underset{\\boldsymbol{\\beta}}{\\mathrm{min}} \\sum_{t=1}^{T} (y_t -\\boldsymbol{x}^{\\prime}_t \\boldsymbol{\\beta})^2,\n\\end{equation*}\\] resulting in the OLS estimator (and estimates) \\[\\begin{equation*}\n\\widehat{\\boldsymbol{\\beta}} = \\Big(\\sum_{t=1}^{T} \\boldsymbol{x}_t \\boldsymbol{x}^{\\prime}_t \\Big)^{-1}  \\sum_{t=1}^{T} \\boldsymbol{x}_t y_{t}.\n\\end{equation*}\\]\nIn contrast to the model specification above, at times we are also considering other linear regressions:\nSubsequent (sub)sections are organized as follows:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multivariate time series models</span>"
    ]
  },
  {
    "objectID": "TSE-ch10.html#predictive-regressions-background-and-starting-point",
    "href": "TSE-ch10.html#predictive-regressions-background-and-starting-point",
    "title": "11  Multivariate time series models",
    "section": "",
    "text": "Simultaneous values of predictive variables, instead of their lags, can be used as predictors in certain circumstances and applications. As an example and for simplicity \\(K=1\\), the model can be \\[\\begin{equation*}\ny_t= \\beta_0 + x_{1t} \\beta_1 + u_t.\n\\end{equation*}\\]\nIncluding also the lags of \\(y_t\\) on the right hand side of the model equation. These are then autoregressive models with additional predictive variables, often denoted by ARX or ADL (Autoregressive Distributed Lag) models. One example of such a model is \\[\\begin{equation*}\ny_t= \\beta_0 + \\phi_1 y_{t-1} + \\beta_1 x_{1, t-1} + u_t,\n\\end{equation*}\\] which can be straightforwardly extended by allowing more lags of \\(y_t\\) and \\(x_{1t}\\).\nIncluding an autocorrelated error term is also possible. That is, especially in the somewhat already distant literature \\(u_t = \\phi u_{t-1} + \\varepsilon_t, \\, \\varepsilon_t \\thicksim \\mathsf{iid}(0,\\sigma^2)\\) (see, e.g., Verbeek, Chapters 4.6-4.7). Notice that adding the lags of \\(y_t\\), we can try to address the possible autocorrelation in the error term.\n\n\n\nThe case where all the (dependent and predictive/explanatory) variables are stationary variables.\nWhen there are also nonstationary variables present.\nAt end of this section, we also briefly introduce the case where \\(K\\) is high-dimensional. That is the number of predictive variables can be very large. Otherwise in this section, we restrict ourselves to the idea that have only one or only a few predictive variables (i.e. \\(K\\) is relatively small).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multivariate time series models</span>"
    ]
  },
  {
    "objectID": "TSE-ch10.html#stationary-variables",
    "href": "TSE-ch10.html#stationary-variables",
    "title": "11  Multivariate time series models",
    "section": "11.2 Stationary variables",
    "text": "11.2 Stationary variables\nLet us start with the case (assumption) that all variables \\(y_t\\) and \\(x_{1t}, \\ldots, x_{Kt}\\) are stationary, which means that their respective time series plots do not contain strong stochastic or deterministic trends. Broadly speaking then, under the following two conditions \\(\\mathrm{(i)}\\) and \\(\\mathrm{(ii)}\\), the OLS estimator \\(\\widehat{\\boldsymbol{\\beta}}\\) of the parameters of interest in a linear regression model \\[\\begin{equation*}\ny_t= \\boldsymbol{x}^{\\prime}_t \\boldsymbol{\\beta} + u_t,\n\\end{equation*}\\] where \\(\\boldsymbol{x}_t = (x_{1t}, \\ldots, x_{Mt})\\) or, e.g., \\(\\boldsymbol{x}_t = (x_{1, t-1}, \\ldots, x_{M, t-1})\\), is consistent and asymptotically normal when\n\n\\(\\mathrm{(i)}\\) The error term \\(u_t\\) is serially uncorrelated and uncorrelated with the regressors included in \\(\\boldsymbol{x}_t\\).\n\\(\\mathrm{(ii)}\\) All the regressors in \\(\\boldsymbol{x}_t\\) are either deterministic or stationary random variables.\n\nIf not otherwise mentioned, we assume that \\(\\mathrm{(i)}\\) and \\(\\mathrm{(ii)}\\) are valid.\nAfter estimation of the model, the same residual diagnostic methods as for ARMA models are available for assessing the residuals and hence the adequacy of the model. The resulting asymptotic distribution result for the OLS estimator is \\[\\begin{equation*}\n\\sqrt{T} (\\widehat{\\boldsymbol{\\beta}}  - \\boldsymbol{\\beta}) \\underset{d}{\\longrightarrow} \\mathsf{N}\\Bigg(\\boldsymbol{0}, \\Big(\\sum_{t=1}^{T} \\boldsymbol{x}_t \\boldsymbol{x}^{\\prime}_t \\Big)^{-1} \\Bigg),\n\\end{equation*}\\] where \\(\\underset{d}{\\longrightarrow}\\) denotes converge in distribution. It turns out that, in fact, even (mild) autocorrelation of the residuals can be accepted without losing many of the useful asymptotic properties of the (conditional) OLS estimator. If autocorrelation occurs after the initial formation of the model, we can proceed in essentially two different ways (see detailed discussion below):\n\nform a (linear) model where the error term is autocorrelated.\n(preferable alternative) adjust the covariance matrix of the estimated parameters \\(\\widehat{\\boldsymbol{\\beta}}\\) (see below)\n\n \n\nExtra: Remaining autocorrelation in the linear (predictive) regression\n\n\nConsider a linear regression (as above), but this time we set ARMA such as an MA(1) structure for the error term. This would lead to the model \\[\\begin{equation*}\ny_t = \\boldsymbol{x}^{\\prime}_t \\boldsymbol{\\beta} + u_t, \\quad u_t = a_t + \\theta_1 a_{t-1},\n\\end{equation*}\\] where \\(a_t\\) is now an $ process (or white noise). However, in macro and especially financial econometrics, it is typical to proceed using the latter option, which is discussed next.\nAs stated, the OLS estimator can be shown to be still consistent and asymptotically normal, even if the error term is autocorrelated and possibly also (conditionally) heteroskedastic. This is practically the message of the quasi-maximum likelihood estimator (QMLE).\n\nCf., for example, the section concerning the AR-GARCH model.\n\nIn such a situation, a heteroskedasticity-autocorrelation consistent (HAC) covariance matrix estimator is used, indicating that the usual” standard errors of the parameter estimates it produces can be replaced with HAC counterparts.\n\nWithout this adjustment, the standard errors are often too small, which correspondingly increases the absolute value of the \\(t\\)-test statistics testing the statistical significance of individual regression coefficients (reducing the \\(p\\)-values).\nOne formulation is based on the Newey and West (1987) estimator.\n\nFormally, let’s consider the OLS estimator and the usual covariance matrix \\[\\begin{equation*}\n\\widehat{\\boldsymbol{\\beta}} = \\Big(\\sum_{t=1}^{T} \\boldsymbol{x}_t \\boldsymbol{x}_t^{\\prime} \\Big)^{-1} \\sum_{t=1}^{T} \\boldsymbol{x}_t y_t, \\quad \\mathsf{Cov}(\\widehat{\\beta}) = \\sigma^2 \\Big(\\sum_{t=1}^{T} \\boldsymbol{x}_t \\boldsymbol{x}_t^{\\prime} \\Big)^{-1},\n\\end{equation*}\\] where \\(\\mathsf{Var}(u_t) = \\sigma^2\\). The general HAC estimator can be written as \\[\\begin{equation*}\n\\mathsf{Cov}(\\widehat{\\boldsymbol{\\beta}})_{HAC} = \\Big(\\sum_{t=1}^{T} \\boldsymbol{x}_t \\boldsymbol{x}_t^{\\prime} \\Big)^{-1} \\widehat{\\boldsymbol{C}}_{HAC} \\Big(\\sum_{t=1}^{T} \\boldsymbol{x}_t \\boldsymbol{x}_t^{\\prime} \\Big)^{-1},\n\\end{equation*}\\] where different choices for the middle term \\(\\widehat{\\boldsymbol{C}}_{HAC}\\) lead to different estimators. For example, in the case of the Newey and West (1987) estimator \\[\\begin{equation*}\n\\widehat{\\boldsymbol{C}}_{HAC} = \\sum_{t=1}^{T} \\widehat{u}^2_t \\boldsymbol{x}_t \\boldsymbol{x}_t^{\\prime} + \\sum_{j=1}^l w_j , \\sum_{s=j+1}^{T} (\\boldsymbol{x}_s \\widehat{u}_s \\widehat{u}_{s-j} x^{\\prime}_{s-j} + \\boldsymbol{x}_{s-j} \\widehat{u}_{s-j} \\widehat{u}_s \\boldsymbol{x}^{\\prime}_{s}), \\end{equation*}\\] where \\(l\\) is the so-called bandwidth parameter and \\(w_j\\) is an appropriate weighting function.\n\nFor example, in the case of the so-called Bartlett kernel function\n\\[\\begin{equation*} w_j = 1- j/l.\n\\end{equation*}\\]\nNewey and West recommended choosing the parameter \\(l\\) as the integer part of the expression \\(4(T/100)^{2/9}\\).\n\nFurthermore, occasionally \\(\\widehat{\\boldsymbol{C}}_{HAC}\\) may also be specified (only) allowing for heteroskedasticity (but not autocorrelation, “HC” vs. “HAC”) with the alternative (this is the so-called White estimator) \\[\\begin{equation*}\n\\widehat{\\boldsymbol{C}}_{HC} = \\sum{t=1}^{T} \\widehat{u}^2_t x_t x_t^{\\prime}.\n\\end{equation*}\\]",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multivariate time series models</span>"
    ]
  },
  {
    "objectID": "TSE-ch10.html#forecasting-with-predictive-variables",
    "href": "TSE-ch10.html#forecasting-with-predictive-variables",
    "title": "11  Multivariate time series models",
    "section": "11.3 Forecasting with predictive variables",
    "text": "11.3 Forecasting with predictive variables\nLet us consider forecast construction more detail. When attempting to predict the value of \\(y_{t+h}\\) with external predictive variables, additional complications arise especially when the forecast horizon \\(h\\) lengthens. As an example, consider a simple model (the main arguments generalize to more general models) containing only two lags of one predictive variable \\(x_t\\) \\[\\begin{equation*}\ny_t= \\beta_0 + \\beta_1 x_{1, t-1} + \\beta_2 x_{1, t-2} + u_t.\n\\end{equation*}\\] As in the ARMA case, one-step forecast is the conditional expectation given the information set at time \\(t\\).  That is \\[\\begin{equation*}\n\\mathsf{E}_t(y_{t+1}) = \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{1,t-1}.\n\\end{equation*}\\] From this, it is immediately apparent that additional challenges arise if the forecast horizon \\(h\\) lengthens longer than one (\\(h=1\\)). In other words, multi-step forecasts (\\(h&gt;1\\)) seem to require forecasts for \\(x_{1t}\\).\n\nVector autoregressive models provide one alternative to solve this problem when building a joint (multiple-equation) model for \\(y_t\\) and \\(x_{1t}\\)\n\nAn alternative to this is to use predictive models that are specific to each forecast horizon. In that case, when using the predictive information available at time \\(t\\), we can specify \\[\\begin{equation*}\ny_{t+h}= \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{1,t-1} + e_{t+h},\n\\end{equation*}\\] where \\(e_t\\) is a zero-mean error term and the forecast can then be constructed “directly” as \\[\\begin{equation*}\n\\mathsf{E}_t(y_{t+h})= \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{1,t-1}.\n\\end{equation*}\\] Naturally here the parameters \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2)\\), and finally their estimates, are then specific to the forecast horizon \\(h\\).\n\nThe properties of the error term \\(e_t\\) can be complicated due to overlapping forecasting horizon from the forecast origin at time \\(t\\) to \\(t+h\\).\nWhat is introduced above for a simple model containing only one predictive variable can be generalized straightforwardly to the case where we have \\(K\\) predictors such as more lags of \\(y_t\\) as predictors.\n\nOverall, this same principle of \\(h\\)-period predictive models can be seen as a building block for various extended predictive regressions such as the ones containing elements of machine and statistical learning.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multivariate time series models</span>"
    ]
  },
  {
    "objectID": "TSE-ch10.html#extra-regularized-predictive-regressions",
    "href": "TSE-ch10.html#extra-regularized-predictive-regressions",
    "title": "11  Multivariate time series models",
    "section": "11.4 Extra: Regularized predictive regressions",
    "text": "11.4 Extra: Regularized predictive regressions\nLet us briefly focus on predicting \\(y_{t+h}\\) at time \\(t\\) (that is the forecast origin) given the collection of \\(K\\) predictive variables in \\(\\boldsymbol{x}_t = (x_{1t},\\ldots,x_{Mt})\\), which may also contain some lagged values of \\(y_t\\) (as discussed above).\n\nThis is the predictive regression we already briefly considered above, but now predicting with high-dimensional predictors.\n\nThe challenge of big dependent data. The focus here is on big dependent data, meaning we’re dealing with a large collection of potential predictors where the number of variables, \\(M\\), can be substantial.\n\nIn some cases, \\(M\\) might even be greater than the sample length \\(T\\) (\\(M &gt; T\\)) used for estimating the model parameters.\nHere for “big dependent data” we mean that the number of (predictive) time series is large. “Dependent” refers to autocorrelation (serial correlation) in the data. In this section, our dependent variable \\(y_t\\) is still scalar-valued (i.e. only one time series) and hence we consider single-equation models.\n\nTraditional limitation. When \\(M\\) is large relative to \\(T\\), traditional estimation techniques, like Ordinary Least Squares (OLS), as introduced above, fail or produce highly unstable and unreliable results.\n\nThe classic OLS estimator is not well-defined if \\(M &gt; T\\) (instead \\(M &lt; &lt; T\\)).\nEven if \\(M\\) is less than \\(T\\) but still large, the resulting model can suffer from overfitting, where it fits the noise in the historical data too closely, leading to poor out-of-sample forecasting performance.\n\nNeed for structure: To overcome the curse of dimensionality - the problems that arise when the number of variables grows - we need methods that can impose structure or constraints on the predictive relationship. This is essential for selecting the most relevant predictors and stabilizing the parameter estimates in a data-rich environment.\nRegularized estimation and sparsity. To enable effective predictive regressions in this high-dimensional setting, where the predictor space can be very large, a class of techniques known as regularized estimators is employed. Regularization involves adding a penalty term to the standard loss function (like the sum of squared errors) that we aim to minimize. This penalty discourages the model from assigning large values to the coefficients, effectively shrinking them towards zero. Key examples (briefly without details in this course):\n\nRidge estimator: This technique adds a penalty proportional to the sum of the squared coefficients. It stabilizes the estimates by shrinking all coefficients but doesn’t set any exactly to zero.\nLASSO (Least Absolute Shrinkage and Selection Operator): This method uses a penalty based on the sum of the absolute values of the coefficients. Crucially, the LASSO has the ability to perform automatic variable selection by setting the coefficients of irrelevant predictors from \\(\\boldsymbol{x}_t\\) exactly to zero. This produces a sparse model, meaning that only a few (the most important) predictors are ultimately used.\nElastic Net: This is a hybrid that combines the penalties of both Ridge and LASSO. It’s often used to leverage the variable selection of LASSO while retaining the grouping effect and stability of Ridge, especially when predictors are highly correlated.\n\nRelevance: These estimators are foundational in modern forecasting and (statistical) machine learning, particularly when integrating vast amounts of data—such as financial indicators, survey data, or text-based predictors—into an economic forecasting model. They provide a principled way to manage the trade-off between bias (from shrinkage) and variance (from the large number of predictors) to achieve superior out-of-sample forecasting accuracy.\nA more detailed treatment of these specific estimators is reserved for the Advanced Time Series Econometrics course.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multivariate time series models</span>"
    ]
  },
  {
    "objectID": "TSE-ch11.html",
    "href": "TSE-ch11.html",
    "title": "12  Basics of vector autoregression",
    "section": "",
    "text": "12.1 VAR(\\(p\\)) process\nEquivalently, \\[\\begin{equation*}\n\\boldsymbol{y}_t = \\boldsymbol{\\nu} + \\boldsymbol{A}_1 \\boldsymbol{y}_{t-1} + \\boldsymbol{u}_t,\n\\end{equation*}\\] where now \\(\\boldsymbol{y}_t = (y_{1t}, y_{2t})\\), \\(\\boldsymbol{\\nu}_t = (\\nu_{1}, \\nu_{2})\\) and \\(\\boldsymbol{u}_t = (u_{1t}, u_{2t})\\) are two \\(2 \\times 1\\) vectors.\nAs an extension of two-variable and first-order case, \\(K\\)-variable VAR(\\(p\\)) process is \\[\\begin{equation*}\n\\boldsymbol{y}_t = \\boldsymbol{\\nu} + \\boldsymbol{A}_1 \\boldsymbol{y}_{t-1} + \\cdots +  \\boldsymbol{A}_p \\boldsymbol{y}_{t-p}  + \\boldsymbol{u}_t, \\quad \\boldsymbol{u}_t \\thicksim \\mathsf{iid}(\\boldsymbol{0}, \\boldsymbol{\\Sigma}_u)\n\\end{equation*}\\] where \\(y_t= (y_{1t} \\, \\cdots \\, y_{Kt})\\) is a \\(K \\times 1\\) vector of variables and \\(\\boldsymbol{A}_1, \\ldots, \\boldsymbol{A}_p\\) are \\(K \\times K\\) coefficient matrices. In other words, the VAR(\\(p\\)) process depends on the \\(p\\) lags of \\(y_{1t}, \\ldots, y_{Kt}\\). The error term (“shock” or “innovation”) \\(\\boldsymbol{u}_t = (u_{1t}, \\ldots , u_{Kt})\\) is an unobserved \\(K \\times 1\\) (vector) iid process.\nA companion form: The VAR(\\(p\\)) process can be rewritten \\[\\begin{eqnarray*}\n\\boldsymbol{Y}_t = \\boldsymbol{\\nu}^* + \\mathbf{A} \\boldsymbol{Y}_{t-1} + \\boldsymbol{U}_t,\n\\end{eqnarray*}\\] where \\(\\boldsymbol{Y}_t = [\\boldsymbol{y}^{\\prime}_t \\, \\cdots \\, \\boldsymbol{y}^{\\prime}_{t-p+1}]^{\\prime}\\) and \\(\\boldsymbol{\\nu}^{*} = [\\boldsymbol{\\nu}^{\\prime} \\,\\, \\boldsymbol{0}^{\\prime} \\cdots \\, \\boldsymbol{0}^{\\prime}]^{\\prime}\\) are (\\(Kp \\times 1\\)) and \\(\\boldsymbol{U}_t = [\\boldsymbol{u}^{\\prime}_t \\,\\, \\boldsymbol{0}^{\\prime} \\cdots \\, \\boldsymbol{0}^{\\prime}]^{\\prime}\\) are (\\(Kp \\times 1\\)) vectors, and\n\\[\\begin{equation*}\n\\mathbf{A}=\n\\left[\n\\begin{array}{ccccc}\n\\boldsymbol{A}_1    &   \\boldsymbol{A}_2    & \\cdots & \\boldsymbol{A}_{p-1}  & \\boldsymbol{A}_p   \\\\\n\\boldsymbol{I}_K    &   \\boldsymbol{0}_K    & \\cdots & \\boldsymbol{0}_K     & \\boldsymbol{0}_K  \\\\\n\\boldsymbol{0}_K    &  \\boldsymbol{I}_K & \\cdots & \\boldsymbol{0}_K & \\boldsymbol{0}_K  \\\\\n\\vdots  &       & \\ddots & \\vdots       & \\vdots  \\\\\n\\boldsymbol{0}_K    &  \\boldsymbol{0}_k     & \\cdots & \\boldsymbol{I}_K     & \\boldsymbol{0}_K  \\\\\n\\end{array}\\right] \\quad (Kp \\times Kp)\n\\end{equation*}\\] In other words, \\[\\begin{equation*}\n\\left[\n\\begin{array}{c}\n\\boldsymbol{y}_t \\\\\n\\boldsymbol{y}_{t-1} \\\\\n\\vdots \\\\\n\\boldsymbol{y}_{t-p+1}\n\\end{array}\\right] =\n\\left[\n\\begin{array}{c}\n\\boldsymbol{\\nu} \\\\\n\\boldsymbol{0}  \\\\\n\\vdots \\\\\n\\boldsymbol{0}\n\\end{array}\\right]\n+\n\\left[\n\\begin{array}{ccccc}\n\\boldsymbol{A}_1    &   \\boldsymbol{A}_2    & \\cdots & \\boldsymbol{A}_{p-1}  & \\boldsymbol{A}_p   \\\\\n\\boldsymbol{I}_K    &   \\boldsymbol{0}_K    & \\cdots & \\boldsymbol{0}_K     & \\boldsymbol{0}_K  \\\\\n\\boldsymbol{0}_K    & \\boldsymbol{I}_K & \\cdots & \\boldsymbol{0}_K  & \\boldsymbol{0}_K  \\\\\n\\vdots  &       & \\ddots & \\vdots       & \\vdots  \\\\\n\\boldsymbol{0}_K    &  \\boldsymbol{0}_K     & \\cdots & \\boldsymbol{I}_K     & \\boldsymbol{0}_K  \\\\\n\\end{array}\\right]\n\\left[\n\\begin{array}{c}\n\\boldsymbol{y}_{t-1} \\\\\n\\boldsymbol{y}_{t-2} \\\\\n\\vdots \\\\\n\\boldsymbol{y}_{t-p}\n\\end{array}\\right]\n+\n\\left[\n\\begin{array}{c}\n\\boldsymbol{u}_t \\\\\n\\boldsymbol{0}  \\\\\n\\vdots \\\\\n\\boldsymbol{0}\n\\end{array}\\right]\n\\end{equation*}\\] indicating that there holds equalities \\(\\boldsymbol{y}_t = \\boldsymbol{\\nu} + \\boldsymbol{A}_1 \\boldsymbol{y}_{t-1} + \\cdots + \\boldsymbol{A}_p \\boldsymbol{y}_{t-p}  + \\boldsymbol{u}_t\\), \\(\\boldsymbol{y}_{t-1} = \\boldsymbol{y}_{t-1}\\), and so on to \\(\\boldsymbol{y}_{t-p+1}= \\boldsymbol{y}_{t-p+1}\\).\nLag-operator and the lag-polynomial presentation of the VAR(\\(p\\)). Define again the lag-operator as \\(B^k \\boldsymbol{x}_t = \\boldsymbol{x}_{t-k}\\), \\(k=0,\\pm 1, \\pm 2, \\ldots\\), where \\(\\boldsymbol{x}_t\\) might be a vector. Therefore, using the lag-operators, the VAR(\\(p\\)) process can be rewritten \\[\\begin{eqnarray*}\n\\boldsymbol{A}(B) \\boldsymbol{y}_t = \\boldsymbol{\\nu} + \\boldsymbol{u}_t,\n\\end{eqnarray*}\\] where \\(\\boldsymbol{A}(B) = (\\boldsymbol{I}_n - \\boldsymbol{A}_1 B - \\cdots - \\boldsymbol{A}_p B^p)\\).\nStationarity (stability) of the VAR(\\(p\\)) process. A VAR(\\(p\\)) process is (strictly) stationary (stable) (given \\(\\boldsymbol{u}_t \\thicksim \\mathrm{iid}(\\boldsymbol{0}, \\boldsymbol{\\Sigma}_u)\\)) if the roots of the determinant \\[\\begin{equation*}\n\\mathrm{det} \\Big(\\mathbf{I}_K - \\boldsymbol{A}_1 z - \\cdots - \\boldsymbol{A}_p z^p \\Big) = 0\n\\end{equation*}\\] are outside the unit circle in absolute value (i.e., \\(|z| &gt; 1\\)).\nNotice, however, that many statistical software packages, including the vars* package in R, use an alternative but equivalent presentation.\nLinear process. As the AR(\\(p\\)) process, the stationary (stable) VAR(\\(p\\)) process can be written \\[\\begin{equation*}\n\\boldsymbol{y}_t = \\boldsymbol{\\mu} + \\sum_{j=0}^{\\infty} \\boldsymbol{\\Psi}_j \\boldsymbol{u}_{t-j}= \\boldsymbol{\\Psi}(B) \\boldsymbol{u}_t,\n\\end{equation*}\\] where \\(\\boldsymbol{\\Psi}(B) = [\\boldsymbol{A}(B)]^{-1} = \\boldsymbol{\\Psi}_0 + \\boldsymbol{\\Psi}_1 B + \\boldsymbol{\\Psi}_2 B^2 + \\cdots\\) defines matrix-valued lag-polynomial. In addition to this linear process presentation, the sequence \\(\\boldsymbol{\\Psi}_0, \\boldsymbol{\\Psi}_1, \\boldsymbol{\\Psi}_2, \\ldots\\) have also other important role in macroeconometrics. It turns out that the matrix \\(\\boldsymbol{\\Psi}_s\\) (\\(\\boldsymbol{\\Psi}_0 = \\boldsymbol{I}_K\\)) measures the effect of one-unit increase in \\(u_{jt}\\) upon \\(y_{i,t+s}\\), , \\(i,j=1,\\ldots,K\\), or equivalently \\[\\begin{equation*}\n\\boldsymbol{\\Psi}_s = \\frac{\\partial \\boldsymbol{y}_{t+s}}{\\partial \\boldsymbol{u}^{\\prime}_t}.\n\\end{equation*}\\] A function of elements \\(\\boldsymbol{\\Psi}_0\\), \\(\\boldsymbol{\\Psi}_1\\), \\(\\boldsymbol{\\Psi}_2, \\ldots\\) is the (forecast error) impulse response function (IRF) of the VAR(\\(p\\)) process.\nAs in the univariate case, here defined element-by-element, due to the typical assumption \\(\\boldsymbol{\\Psi}_s \\longrightarrow \\boldsymbol{0}, \\, s \\longrightarrow \\infty\\), the effects decay in time. In empirical analysis, the unknown \\(\\boldsymbol{\\Psi}_s\\) must also be replaced by their estimates, obtained from the estimation result of the VAR model.\nVAR(\\(p\\)) model: Estimation and model selection. The stationary VAR(\\(p\\)) model can be estimated by ordinary least squares (OLS).\nAs for the univariate AR(\\(p\\)) model, there are several approaches to determine the lag length \\(p\\) for the VAR(\\(p\\)) model.\nFor the competing models (recall that exactly the same sample period must be used!) the one with the lowest value of the selected information criterion is the selected (preferable) model.\nVAR(\\(p\\)): Model selection and residual diagnostics\nOverall, the model selection can be based on the specification cycle presented in the case of univariate AR models but modified suitable to VARs. However, due to multiple time series and hence more complicated dynamic interrelationships between the variables, autocorrelation functions cannot be directly used to select the lag length \\(p\\).\nThe adequacy of the selected and estimated VAR(\\(p\\)) model can be evaluated by diagnostic checks (like for ARMA models). A well-specified model should capture all the systematic, linear dynamics present in the (multivariate) time series data. The primary goal of these checks is to determine if there is any significant autocorrelation left in the vector of residuals \\(\\widehat{\\boldsymbol{u}}_t = \\boldsymbol{y}_t - \\widehat{\\boldsymbol{y}}_t\\), where \\(\\widehat{\\boldsymbol{y}}_t\\) are the fitted values of the selected VAR model.\nTo formally test for remaining autocorrelation in the residuals, we use a multivariate extension of the Ljung-Box test. This is called the Portmanteau test. It jointly examines whether a group of residual autocovariance matrices up to a certain lag are statistically different from zero. The null hypothesis (\\(H_0\\)) of the test is that there is no serial correlation in the residuals up to a specified lag \\(H\\). The Portmanteau test statistic, often denoted as \\(Q_H\\), is calculated as follows: \\[\\begin{equation*}\nQ_H = T \\sum_{h=1}^{H} \\text{tr}(\\widehat{C}_h^{\\prime} \\widehat{C}_0^{-1} \\widehat{C}_h \\widehat{C}_0^{-1}),\n\\end{equation*}\\] where \\(\\hat{C}_j = \\frac{1}{T} \\sum_{t=j+1}^{T} \\widehat{u}_t \\widehat{u}_{t-j}^{\\prime}\\) is the estimated residual autocovariance matrix at lag \\(j\\) and \\(\\text{tr}(\\cdot)\\) is the trace operator, which sums the diagonal elements of a matrix. Under the null hypothesis of no serial correlation, the \\(Q_H\\) statistic approximately follows a \\(\\chi^2\\)-distribution with \\(K^2(H-p)\\) degrees of freedom and the associated \\(p\\)-value can be constructed.\nShould the model fail in the Portmanteau test or overall in diagnostic checking, the model should be respecified, such as to increase the lag length of the model (e.g., from VAR(\\(p\\)) to VAR(\\(p+1\\)) or higher) and re-estimate it. The logic is that by including more past values, the model can better capture the complex temporal dependencies in the data. After respecification, the diagnostic checks must be performed again to ensure the new model is adequate.\nHowever, especially with complex multivariate data, it is often challenging to find a model that can be considered completely adequate from a diagnostic perspective. When the diagnostic checks indicate that the model is misspecified (for instance, due to some persistent residual autocorrelation), the standard assumptions for maximum likelihood estimation are violated. In such cases, the parameter estimates can be interpreted as Quasi-Maximum Likelihood Estimates (QMLE), a concept that was introduced on a general level in the AR-GARCH section.\nVAR(\\(p\\)): Forecasting\nSimilarly as in the univariate AR(\\(p\\)) model, forecasts can be obtained as conditional expectations, conditional on the information at time \\(t\\). The properties CEV1–CEV4 of the conditional expectation can be extended to the vector-valued case straigtforwardly (element-by-element treatment).\nSimilarly as in the univariate AR models, to include nonzero mean vector to the VAR model is assumed above. Alternatively, you can construct the VAR model for the deamed time series and add the mean vector to the forecasts obtained with the VAR model with demeaned data.\nEmpirical example. Consider an application of a VAR model to the classic dataset from Stock and Watson (2001), which is a benchmark for macroeconomic analysis.\nThe VAR model utilizes three (\\(K=3\\)) key quarterly U.S. macroeconomic variables:\nThese three series are modeled as an interdependent system where each variable is influenced by its own past values and the past values of the other two variables. The following analysis covers the sample period from 1960:Q1 to 2000:Q4 as in Stock and Watson (2001).\nLag length selection: For this analysis and illustration, we follow Stock and Watson (2001) and specify a VAR(4) model. This choice is common in quarterly macroeconomic data as it captures potential year-over-year dynamics.\nParameter estimation: The VAR(4) model is estimated assuming the error terms are normally distributed. Because each equation in the VAR system has the same set of explanatory variables (the lagged values of all variables in the system), Ordinary Least Squares (OLS) applied equation-by-equation is an efficient estimation method.\nResidual diagnostics: After estimation, diagnostic tests are crucial for assessing the model’s adequacy.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Basics of vector autoregression</span>"
    ]
  },
  {
    "objectID": "TSE-ch11.html#varp-process",
    "href": "TSE-ch11.html#varp-process",
    "title": "12  Basics of vector autoregression",
    "section": "",
    "text": "Let us start with a bivariate model. That is we have two stationary time series \\(y_{1t}\\) and \\(y_{2t}\\) with \\(\\mathsf{E}(y_{1t}) = \\mu_1\\) and \\(\\mathsf{E}(y_{2t})=\\mu_2\\). A bivariate (i.e., a two-variable) first-order VAR process (i.e., a bivariate VAR(1) process) is given as \\[\\begin{eqnarray*}\ny_{1t} &=& \\nu_1 + a_{11} y_{1, t-1} + a_{12} y_{2, t-1} + u_{1t} \\\\\ny_{2t} &=& \\nu_2 + a_{21} y_{1, t-1} + a_{22} y_{2, t-1} + u_{2t},\n\\end{eqnarray*}\\] where \\(u_{1t}\\) and \\(u_{2t}\\) are two error terms (independent of the history of \\(y_{1t}\\) and \\(y_{2t}\\)) that may be correlated. In a matrix notation, the VAR(1) process can be rewritten \\[\\begin{eqnarray*}\n\\left[ \\begin{array}{c}\ny_{1t} \\\\\ny_{2t} \\end{array} \\right] =\n\\left[ \\begin{array}{c}\n\\nu_{1} \\\\\n\\nu_{2} \\end{array} \\right] +\n\\left[ \\begin{array}{cc}\na_{11} & a_{12}  \\\\\na_{21} & a_{22}  \\end{array} \\right]\n\\left[ \\begin{array}{c}\ny_{1, t-1} \\\\\ny_{2, t-1} \\end{array} \\right] +\n\\left[ \\begin{array}{c}\nu_{1t} \\\\\nu_{2t} \\end{array} \\right].\n\\end{eqnarray*}\\]\n\n\nIn this notation, likewise throughout this material, with bolded font we emphasize vectors and matrices, to distinguish scalar-valued components and processes.\n\n\n\n\nHence \\(\\mathsf{E}(\\boldsymbol{u}_t)=\\boldsymbol{0}\\), \\(\\mathsf{E}(\\boldsymbol{u}_t \\boldsymbol{u}_t^{\\prime}) = \\boldsymbol{\\Sigma}_u\\), and \\(\\mathsf{E}(\\boldsymbol{u}_t \\boldsymbol{u}_s^{\\prime}) = \\boldsymbol{0}, \\, s \\neq t\\).\nThe bivariate and more general model show the basic idea: The idea of the VAR(\\(p\\)) model is to regress each component of \\(y_t\\) on its own lags and on the lags of the other components. The model can describe lagged or dynamic dependencies among variables. For example, one may ask how \\(y_{2t}\\) affects the future path of \\(y_{1t}\\), and other way round.\n\n\n\n\n\n\n\n\n\nThey first represent the VAR(p) process in its companion form (see above). The stability of the system is then determined by the eigenvalues of the companion matrix \\(\\mathbf{A}\\).\nIn vars package and VAR-function, the part “Roots of the characteristic polynomial” reported by R are precisely these eigenvalues. The key relationship is that these eigenvalues of the companion matrix are the reciprocals of the roots of the determinant polynomial. Therefore, the two conditions are equivalent:\n\nRoots of \\(\\mathrm{det} \\Big(\\mathbf{I}_n - \\boldsymbol{A}_1 z - \\cdots - \\boldsymbol{A}_p z^p \\Big) = 0\\) are outside the unit circle (\\(|z| &gt; 1\\)).\nEigenvalues of the companion matrix \\(\\mathbf{A}\\) are inside the unit circle (\\(|\\lambda| &lt; 1\\)).\n\nSo, when using R’s output, we check if the moduli of the reported roots (the eigenvalues) are all less than 1 to confirm that the VAR model is stable.\n\n\n\n\nIn other words, the row \\(i\\), column \\(j\\) element of \\(\\boldsymbol{\\Psi}_s\\) gives the effect of a one-unit increase in the error term of the \\(j\\)th variable at time \\(t\\) on the \\(i\\)th variable at time \\(t+s\\), holding all other errors constant.\nFor instance, the first column of \\(\\Psi_1\\) gives the effect in period 1 of a unit increase in the error term of the first variable (first shock) in period 0 on each variable in the system.\nEspecially in (structural) macroeconometrics, the VAR model is extended by economic identification assumption, which leads to structural VAR models and identified impulse response functions. These will be considered more detail in Macroeconometrics course.\n\n\n\n\n\nMore specifically, estimation can be done equation by equation by OLS (cf. the model structure).\nIf the distribution of \\(\\boldsymbol{u}_t\\) is known (asssumed to be known), the model can also be estimated by the method of maximum likelihood (ML). Under normality assumption, that is \\(\\boldsymbol{u}_t \\sim \\mathsf{nid}(\\boldsymbol{0}, \\boldsymbol{\\Sigma}_u)\\), the conditional ML estimator is numerically equivalent to the OLS estimator.\nThe OLS estimator is consistent and asymptotically normally distributed. Thus, the \\(t\\)-statistics of the parameter estimates can be used as usual to test for the statistical significance of individual parameter coefficients.\n\n\n\nSequential testing procedure (cf. the sequential testing for univariate models)\nInformation criteria (including intercept parameters):\n\nAIC: \\(\\mathrm{log} (\\mathrm{det}(\\boldsymbol{\\Sigma}_u)) + \\frac{2}{T} K(Kp+1)\\)\nBIC: \\(\\mathrm{log} (\\mathrm{det}(\\boldsymbol{\\Sigma}_u)) + \\frac{\\mathrm{log}(T)}{T} K(Kp+1)\\)\n\n\n\n\n\n\n\nEspecially information criteria, such as AIC and BIC, can be used like in the univariate case.\nAt times the lag length selections correspond the frequency of the data. That is, for the quarterly data \\(p=4\\) and for monthly data \\(p=12\\).\n\n\n\n\nIf autocorrelation is still present in residuals, it means the model has failed to capture some of the predictable patterns in the data, making it unreliable for forecasting or structural analysis.\n\n\n\n\n\n\n\n\nOne-period-ahead forecasts with the VAR(\\(p\\)): \\[\\begin{equation*}\n\\mathsf{E}_t(\\boldsymbol{y}_{t+1}) = \\boldsymbol{\\nu} + \\boldsymbol{A}_1 \\boldsymbol{y}_t + \\cdots +  \\boldsymbol{A}_p \\boldsymbol{y}_{t-p+1}.\n\\end{equation*}\\]\nMultiperiod \\(h\\)-step forecasts can be obtained iteratively following the recursion (cf. the AR(\\(p\\)) case): \\[\\begin{equation*}\n\\mathsf{E}_t(\\boldsymbol{y}_{t+h}) = \\boldsymbol{\\nu} + \\boldsymbol{A}_1 \\mathsf{E}_t(\\boldsymbol{y}_{t+h-1}) + \\cdots + \\boldsymbol{A}_p \\mathsf{E}_t(\\boldsymbol{y}_{t+h-p}),\n\\end{equation*}\\] where \\(\\mathsf{E}_t(\\boldsymbol{y}_{t+h-j}) = \\boldsymbol{y}_{t+h-j}\\) for \\(h \\le j\\).\n\n\n\n\n\nSee J.H. Stock and M.W. Watson (2001). Vector autoregressions. Journal of Economic Perspectives, 115(4), 101–115.\n\n\n\nInflation: Measured as the percent change from the preceding period in the Gross Domestic Product Implicit Price Deflator, expressed at a seasonally adjusted annual rate.\nUnemployment: The seasonally adjusted quarterly average of the civilian unemployment rate.\nInterest Rate: The quarterly average of the effective Federal Funds Rate, which is not seasonally adjusted.\n\n\n\n\nFigure: Stock and Watson (2001): U.S. macroeconomic time series (inflation, unemployment rate and interest rate).\n\n\n\n\n\nA key feature of VAR models is the large number of parameters. In this three-variable VAR(4) model, each of the three equations includes an intercept and four lags of all three variables, resulting in 3×(1+4×3)=39 parameters to be estimated. Due to this high dimensionality, interpreting individual coefficient estimates is often impractical. Instead, analysis typically focuses on system-wide properties through Granger causality tests, as well as impulse response functions and forecast error variance decomposition (Macroeconometrics course)\n\n\n\nPortmanteau Test: The Portmanteau test for serial correlation in the residuals indicates some remaining unmodelled dynamics. The test’s statistically significant result suggests that the VAR(4) specification may not fully capture all the linear dependencies within the data.\nWhile the residuals from the different variable equations appear to have low cross-correlation, an analysis of the squared residuals reveals a different pattern. The presence of autocorrelation in the squared residuals is a strong indicator of conditional heteroskedasticity. This means the volatility of the shocks to the system is not constant over time. While the VAR(4) model may adequately capture the conditional mean of the variables, it fails to account for this time-varying variance in the error terms. A more advanced model, such as a VAR-GARCH (not considered in this course), would be required to model these volatility dynamics.\n\n\nDetails of the estimated VAR(4) model\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Inflation, Unemployment, Fedfunds \nDeterministic variables: const \nSample size: 160 \nLog Likelihood: -396.638 \nRoots of the characteristic polynomial:\n0.9696 0.9696 0.7928 0.7928 0.686 0.686 0.5674 0.5674 0.4609 0.4609 0.2053 0.2053\nCall:\nVAR(y = SW_data, p = 4, type = c(\"const\"), exogen = NULL, lag.max = NULL)\n\n\nEstimation results for equation Inflation: \n========================================== \nInflation = Inflation.l1 + Unemployment.l1 + Fedfunds.l1 + Inflation.l2 + Unemployment.l2 + Fedfunds.l2 + Inflation.l3 + Unemployment.l3 + Fedfunds.l3 + Inflation.l4 + Unemployment.l4 + Fedfunds.l4 + const \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nInflation.l1     0.5886863  0.0821552   7.166 3.47e-11 ***\nUnemployment.l1 -0.8356913  0.4006087  -2.086  0.03870 *  \nFedfunds.l1      0.2370805  0.1082335   2.190  0.03007 *  \nInflation.l2     0.0902554  0.0937902   0.962  0.33747    \nUnemployment.l2  1.3545379  0.6841360   1.980  0.04958 *  \nFedfunds.l2     -0.2168750  0.1453158  -1.492  0.13773    \nInflation.l3     0.1205897  0.0944654   1.277  0.20377    \nUnemployment.l3 -1.0953222  0.6779667  -1.616  0.10833    \nFedfunds.l3     -0.0005773  0.1455216  -0.004  0.99684    \nInflation.l4     0.1866837  0.0846107   2.206  0.02891 *  \nUnemployment.l4  0.4122130  0.3765054   1.095  0.27538    \nFedfunds.l4     -0.0231108  0.1113078  -0.208  0.83581    \nconst            1.0641191  0.3968301   2.682  0.00817 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\nResidual standard error: 1.053 on 147 degrees of freedom\nMultiple R-Squared: 0.8509, Adjusted R-squared: 0.8387 \nF-statistic:  69.9 on 12 and 147 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation Unemployment: \n============================================= \nUnemployment = Inflation.l1 + Unemployment.l1 + Fedfunds.l1 + Inflation.l2 + Unemployment.l2 + Fedfunds.l2 + Inflation.l3 + Unemployment.l3 + Fedfunds.l3 + Inflation.l4 + Unemployment.l4 + Fedfunds.l4 + const \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nInflation.l1     0.0066525  0.0181442   0.367 0.714410    \nUnemployment.l1  1.4696483  0.0884753  16.611  &lt; 2e-16 ***\nFedfunds.l1      0.0006271  0.0239036   0.026 0.979106    \nInflation.l2    -0.0115173  0.0207138  -0.556 0.579040    \nUnemployment.l2 -0.5155262  0.1510930  -3.412 0.000833 ***\nFedfunds.l2      0.0604859  0.0320933   1.885 0.061446 .  \nInflation.l3     0.0317894  0.0208629   1.524 0.129724    \nUnemployment.l3 -0.0057705  0.1497305  -0.039 0.969310    \nFedfunds.l3     -0.0399995  0.0321388  -1.245 0.215265    \nInflation.l4    -0.0200840  0.0186865  -1.075 0.284230    \nUnemployment.l4 -0.0050194  0.0831521  -0.060 0.951947    \nFedfunds.l4      0.0114985  0.0245826   0.468 0.640657    \nconst            0.0854091  0.0876408   0.975 0.331392    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\nResidual standard error: 0.2326 on 147 degrees of freedom\nMultiple R-Squared: 0.9786, Adjusted R-squared: 0.9768 \nF-statistic: 559.3 on 12 and 147 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation Fedfunds: \n========================================= \nFedfunds = Inflation.l1 + Unemployment.l1 + Fedfunds.l1 + Inflation.l2 + Unemployment.l2 + Fedfunds.l2 + Inflation.l3 + Unemployment.l3 + Fedfunds.l3 + Inflation.l4 + Unemployment.l4 + Fedfunds.l4 + const \n\n                Estimate Std. Error t value Pr(&gt;|t|)    \nInflation.l1     0.07293    0.06945   1.050  0.29536    \nUnemployment.l1 -1.38408    0.33865  -4.087 7.16e-05 ***\nFedfunds.l1      0.95325    0.09149  10.419  &lt; 2e-16 ***\nInflation.l2     0.20379    0.07928   2.570  0.01115 *  \nUnemployment.l2  1.27436    0.57832   2.204  0.02911 *  \nFedfunds.l2     -0.40302    0.12284  -3.281  0.00129 ** \nInflation.l3    -0.07175    0.07985  -0.899  0.37035    \nUnemployment.l3 -0.49829    0.57311  -0.869  0.38601    \nFedfunds.l3      0.34476    0.12301   2.803  0.00575 ** \nInflation.l4    -0.04306    0.07152  -0.602  0.54804    \nUnemployment.l4  0.49476    0.31827   1.555  0.12221    \nFedfunds.l4      0.03129    0.09409   0.333  0.73997    \nconst            0.53914    0.33545   1.607  0.11015    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\nResidual standard error: 0.8905 on 147 degrees of freedom\nMultiple R-Squared: 0.9273, Adjusted R-squared: 0.9214 \nF-statistic: 156.3 on 12 and 147 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Inflation Unemployment Fedfunds\nInflation     1.109681     0.001595   0.1486\nUnemployment  0.001595     0.054126  -0.0928\nFedfunds      0.148558    -0.092796   0.7930\n\nCorrelation matrix of residuals:\n             Inflation Unemployment Fedfunds\nInflation     1.000000     0.006508   0.1584\nUnemployment  0.006508     1.000000  -0.4479\nFedfunds      0.158370    -0.447925   1.0000\n\n=====\nPortmanteau Test (adjusted)\n    \nserial.test(VAR_lag_p, lags.pt = 12, type = \"PT.adjusted\") #\n\nChi-squared = 132.39, df = 72, p-value = 1.904e-05\n\n&gt; serial.test(VAR_lag_p, lags.pt = 15, type = \"PT.adjusted\") # \n\nChi-squared = 149.51, df = 99, p-value = 0.0007889\n\n&gt; serial.test(VAR_lag_p, lags.pt = 6, type = \"PT.adjusted\") # \n\nChi-squared = 48.748, df = 18, p-value = 0.0001165",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Basics of vector autoregression</span>"
    ]
  },
  {
    "objectID": "TSE-ch11.html#granger-causality-in-vars",
    "href": "TSE-ch11.html#granger-causality-in-vars",
    "title": "12  Basics of vector autoregression",
    "section": "12.2 Granger causality in VARs",
    "text": "12.2 Granger causality in VARs\nGranger causality is a concept used to determine whether one time series can predict another. It doesn’t imply true causality in the philosophical or scientific sense, but rather predictive causality based on temporal dependence and information content.\n\nGranger causality (note that here “causality” refers to predictive ability, not actual causality) can be examined for single-equation models, such as the ones considered in the previous chapter, examining whether the additional predictor Granger cause \\(y_t\\).\nGranger causality is most often studied in the context of VAR models.\n\nBivariate VAR model (\\(n=2\\)): Granger-causality can be easily tested within a VAR model. Consider first, for simplicity, the bivariate VAR(\\(p\\)) process \\[\\begin{eqnarray*}\n\\left[ \\begin{array}{c}\ny_{1t} \\\\\ny_{2t} \\end{array} \\right] =\n\\left[ \\begin{array}{c}\n\\nu_{1} \\\\\n\\nu_{2} \\end{array} \\right]\n+\n\\sum_{j=1}^{p}\n\\left[ \\begin{array}{cc}\na_{11,j} & a_{12,j}  \\\\\na_{21,j} & a_{22,j}  \\end{array} \\right]\n\\left[ \\begin{array}{c}\ny_{1, t-j} \\\\\ny_{2, t-j} \\end{array} \\right] +\n\\left[ \\begin{array}{c}\nu_{1t} \\\\\nu_{2t} \\end{array} \\right].\n\\end{eqnarray*}\\] If \\(a_{12,j}\\)=0, \\(j=1,2, \\ldots, p\\), then the lags of \\(y_{2t}\\) do not help to forecast \\(y_{1t}\\) and there is no Granger-causality from \\(y_{2t}\\) to \\(y_{1t}\\) (or \\(y_{2t}\\) is not Granger-causal for \\(y_{1t}\\)).\nThe restriction \\(a_{12,1} = a_{12,2} = \\cdots = a_{12,p} = 0\\) implies that there is not such Granger causality relationship.\n\nWhen working with estimated VAR model, this hypothesis can be tested by the Wald or likelihood ratio (LR) test, following \\(\\chi^2_p\\)-distribution under Granger noncausality in this bivariate case.\nAt times, such as in R program, also the \\(F\\)-test statistic (instead of the Wald test statistic) will be used where the critical values are coming from the \\(F\\)-distribution.\n\n \nLarger VAR models (\\(n &gt; 2\\)): When the VAR model contains more than two variables, Granger causality between two variables cannot be tested by testing zero-restrictions in a straightforward manner (as above). As an example, consider the following three-variable VAR process \\[\\begin{eqnarray*}\n\\left[ \\begin{array}{c}\ny_{1t} \\\\\ny_{2t} \\\\\ny_{3t}\n\\end{array} \\right] =\n\\left[ \\begin{array}{c}\n\\nu_{1} \\\\\n\\nu_{2} \\\\\n\\nu_{3} \\end{array} \\right]\n+\n\\sum_{j=1}^{p}\n\\left[ \\begin{array}{ccc}\na_{11,j} & a_{12,j} & a_{13,j}  \\\\\na_{21,j} & a_{22,j} & a_{23,j} \\\\\na_{31,j} & a_{32,j} & a_{33,j} \\\\\n\\end{array} \\right]\n\\left[ \\begin{array}{c}\ny_{1, t-j} \\\\\ny_{2, t-j} \\\\\ny_{3, t-j}\n\\end{array} \\right] +\n\\left[ \\begin{array}{c}\nu_{1t} \\\\\nu_{2t} \\\\\nu_{3t}\n\\end{array} \\right].\n\\end{eqnarray*}\\] If, for instance, \\(a_{12,1}= \\cdots = a_{12,p} = 0\\), the lags of \\(y_{2t}\\) do not help forecasting \\(y_{1t}\\) one period ahead. However, because the lags of \\(y_{2t}\\) may affect \\(y_{3t}\\), which, in turn, affects \\(y_{1t}\\), the lags of \\(y_{2t}\\) may help forecasting \\(y_{1,t+1}\\), \\(y_{1,t+2}\\) etc. In other words, there may be Granger causality through indirect effects.\n \nGenerally, Granger causality from one group of variables to another is defined similarly. Partition \\(\\boldsymbol{y}_t = [\\boldsymbol{y}_{1t}^{\\prime} \\quad \\boldsymbol{y}_{2t}^{\\prime}]^{\\prime}\\), where \\(\\boldsymbol{y}_{1t}\\) is \\(n_1 \\times 1\\) and \\(\\boldsymbol{y}_{2t}\\) is \\(n_2 \\times 1\\) , (\\(n_1+n_2=n\\)), and write the VAR(\\(p\\)) process \\[\\begin{eqnarray*}\n\\left[ \\begin{array}{c}\n\\boldsymbol{y}_{1t} \\\\\n\\boldsymbol{y}_{2t} \\end{array} \\right] =\n\\left[ \\begin{array}{c}\n\\boldsymbol{\\nu}_1 \\\\\n\\boldsymbol{\\nu}_2 \\end{array} \\right]\n+\n\\sum_{j=1}^{p}\n\\left[ \\begin{array}{cc}\n\\boldsymbol{A}_{11,j} & \\boldsymbol{A}_{12,j}  \\\\\n\\boldsymbol{A}_{21,j} & \\boldsymbol{A}_{22,j}  \\end{array} \\right]\n\\left[ \\begin{array}{c}\n\\boldsymbol{y}_{1, t-j} \\\\\n\\boldsymbol{y}_{2, t-j} \\end{array} \\right] +\n\\left[ \\begin{array}{c}\n\\boldsymbol{\\zeta}_{1t} \\\\\n\\boldsymbol{\\zeta}_{2t} \\end{array} \\right],\n\\end{eqnarray*}\\] where \\(\\boldsymbol{A}_{\\cdot \\cdot , \\,j}\\) are generally matrices (but of course may reduce to scalar-valued components in certain situations)\n\nIf, for instance, \\(\\boldsymbol{A}_{12,j} = \\boldsymbol{0}, \\, \\forall j=1, \\ldots, p\\), then the variables in \\(\\boldsymbol{x}_t\\) are not Granger-causal for the variables in \\(\\boldsymbol{w}_t\\).\n\nThe general restriction \\(\\boldsymbol{A}_{12,1} = \\boldsymbol{A}_{12,2} = \\ldots = \\boldsymbol{A}_{12,p} = \\boldsymbol{0}\\) can be tested by the Wald (\\(F\\)-test) or likelihood ratio (LR) test.\n\nThe above restrictions and test statistic apply also when there is a constant term included in the VAR(\\(p\\)).\n\nThe hypothesis of interest \\[\\begin{equation*}\n\\boldsymbol{A}_{12,1} =  \\boldsymbol{A}_{12,2} = \\cdots = \\boldsymbol{A}_{12,p} = \\boldsymbol{0}\n\\end{equation*}\\] implies that there is \\(n_1 n_2 p\\) zero restrictions. Wald and LR tests can be used to test the above null hypothesis.\n\nUnder the null hypothesis, the Wald and LR tests are asympotically \\(\\chi^2_{n_1 n_2 p}\\)-distributed and large values of the test statistics are critical for the null.\nAs mentioned, at times \\(F\\)-test statistic is used where the number of restrictions (the first degree-of-freedom parameter in the \\(F\\)-distribution) is the above number of restrictions.\n\n \nExample: Three-variable process/model. Consider the VAR(1) process/model: \\[\\begin{equation*}\n\\boldsymbol{y}_t = \\left[ \\begin{array}{c}\ny_{1t} \\\\ y_{2t} \\\\ y_{3t}   \\end{array} \\right] =\n\\left[ \\begin{array}{c}\n0 \\\\ 2 \\\\ 1   \\end{array} \\right] +\n\\left[ \\begin{array}{ccc}\n0.5 & 0 & 0 \\\\\n0.1 & 0.1 & 0.3 \\\\\n0 & 0.2 & 0.3\n\\end{array} \\right] \\boldsymbol{y}_{t-1} +\\boldsymbol{u}_t.\n\\end{equation*}\\] Partitioning \\(\\boldsymbol{y}_t = [y_{1t} \\quad \\boldsymbol{y}^{\\prime}_{2t}]^{\\prime}\\) so that \\(\\boldsymbol{y}_{2t} = [y_{2t} \\quad y_{3t}]^{\\prime}\\), \\(\\boldsymbol{y}_{2t}\\) does not Granger cause \\(\\boldsymbol{y}_{1t} = y_{1t}\\) because the matrix \\(\\boldsymbol{A}_{12,1} = \\boldsymbol{0}\\). On the other hand, \\(y_{1t}\\) Granger causes \\(\\boldsymbol{y}_{2t}\\).\n \nInstantaneous causality. In the bivariate VAR, there is no instantaneous causality between \\(y_{1t}\\) and \\(y_{2t}\\) if the errors \\(u_{1t}\\) and \\(u_{2t}\\) are uncorrelated. In general (not necessarily related to the VAR), a variable \\(y_{1t}\\) is said to be instantaneously causal for another variable \\(y_{2t}\\), if knowing the value of \\(y_{1t}\\) in the forecast period helps to improve forecasts of \\(y_{2t}\\).\nIt can be proved that instantaneous causality is symmetric so that if \\(y_{1t}\\) causes \\(y_{2t}\\), then also \\(y_{2t}\\) causes \\(y_{1t}\\). That is the concept is symmetric: This definition does not account for the “direction” of causality which must be determined from other sources.\n\nFor instance, economic theory may suggest that the (instantaneous) causality runs only in one direction. Then significant correlation can be interpreted in favour of a causal relation.\n\nIn systems with more than two variables, there is no instantaneous causality between two groups of variables if the covariance matrix of the errors, \\(\\Sigma_u\\), is block diagonal.\n\nAs an example, if the covariance matrix in a VAR process for \\(\\boldsymbol{y}_t = (y_{1t}, y_{2t}, y_{3t})\\) is \\[\\begin{equation*} \\Sigma_u= \\left[\n\\begin{array}{ccc}\n2.0 &  0  &  0 \\\\\n0 & 1.5  & 0.4 \\\\\n0 & 0.4  & 1.0 \\\\\n\\end{array} \\right],\n\\end{equation*}\\] then there is no instantaneous causality between \\(y_{1t}\\) and \\(\\boldsymbol{y}_{2t} = (y_{2t}, y_{3t})\\).\nInstantaneous causality can be tested by a Wald type of test of zero restrictions on the corresponding elements of the covariance matrix of the error terms of a VAR model.\n\n \nEmpirical example: Three-variable monetary policy VAR. Continue with the three-variable VAR(4) model selected above. Consider Granger causality and instantaneous causality tests. The goal is to determine if past values of certain variables are useful in predicting others. The null hypothesis for each test is that no Granger causality/instantaneous causality exists.\nThe results, obtained using R, strongly indicate a highly interconnected dynamic system. We find statistically significant Granger causality in all tested directions, meaning the variables have substantial predictive power over each other. A summary of the test results is provided below:\n| (Infl., Unemp.) -&gt; Int. rate  | Granger (F-Test) | 4.86 | &lt; 0.001 | Inst. causality (\\chi^2) | 29.56 | &lt; 0.001 \n| (Unemp., Int. rate) -&gt; Infl.  | Granger (F-Test) | 3.40 | &lt; 0.001 | Inst. causality (\\chi^2) | 5.05  | 0.080\n| (Infl., Int. rate) -&gt;  Unemp. | Granger (F-Test) | 4.82 | &lt; 0.001 | Inst. causality (\\chi^2) | 27.42 | &lt; 0.001\nIn short, the past values of each pair of variables are useful for predicting the third.\n\nThat is we are rejecting the null hypotheses of no Granger causality at the conventional statistical significance levels.\n\nSignificant instantaneous (contemporaneous) relationships are also present in almost all cases, with the only exception being the link between inflation and the other two variables (unemployment rate and interest rate), which is not significant at the 5% level. Overall, these results confirm strong dynamic interlinkages among the interest rate, inflation, and unemployment rate.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Basics of vector autoregression</span>"
    ]
  },
  {
    "objectID": "TSE-ch11.html#r-lab",
    "href": "TSE-ch11.html#r-lab",
    "title": "12  Basics of vector autoregression",
    "section": "12.3 R Lab",
    "text": "12.3 R Lab\nAll the R codes considered in this section are compiled in the following tab\n\nR Lab: VAR modelling\n\n\n\n# VAR modeling\n\n# see details: http://www.jstatsoft.org/v27/i04/paper  % vars package\n\n#-------------------------------------------------------------------------#\n\n# install.packages(\"vars\") ## needed in the estimation VAR models, install ones\nlibrary(vars)\n#library(stats)  # \n\n#-------------------------------------------------------------------------#\n\n# Data\n\nData=read.table(\"US_macrodata.txt\",header=TRUE)\n\nattach(Data) \n\n# Attached data to time series \n# deltat=1/12 -&gt; monthly data, deltat=1/4 quarterly data  \nUS_data=ts(Data[,],start=c(1960,1),deltat=1/4)  # full sample period\n\ninfl=US_data[,\"Inflation\"]   # Inflation (GDP deflator)\nunemp=US_data[,\"Unrate\"]   # unemployment rate \nintr=US_data[,\"FedFunds\"]   # Fed funds rate\n\n# Plot the time series\nplot(intr) # Interest rate\n\n\n\n\n\n\n\n#----------------Stock and Watson (2001) (type of) data-----------------------#\n\n# - Variables: Inflation, unemployment rate and interest rate\nSW_system = cbind(infl,unemp,intr)  # full sample period\n\n# - Sample period 1960:Q1 - 2000:Q4 used below\nSW_data = ts(SW_system,start=c(1960,1),end=c(2000,4),deltat=1/4)\n\n# Plot the time series\nplot.ts(SW_data)\n\n\n\n\n\n\n\n# The three-variable model of SW (2001), for Granger causality testing\ncolnames(SW_data)=c(\"Inflation\",\"Unemployment\",\"Fedfunds\") \n\n\n#--------------------AutoCorrelation structure------------------------------\n# \n\n# freq=1 is needed to get x-axis reasonable \n# autocorrelations and cross-correlations\n\nacf(ts(SW_data,freq=1), lag.max=10, type=\"correlation\") \n\n\n\n\n\n\n\nccf(SW_data[,3], SW_data[,1])\n\n\n\n\n\n\n\n#--------------- VAR modeling, VAR model with Gaussian errors ---------------\n\n# Model selection\n\n# ?VARselect           # properties of the R's model selection routine\n\nmodelselection=VARselect(SW_data, lag.max = 10, type = c(\"const\"), \n                         season = NULL, exogen = NULL)\nmodelselection$selection \n\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     9      3      2      9 \n\n#----------------- Parameter estimation of the VAR model --------------------\n\n# ?VAR\n\n# \"p\" is the order of the model, \"type\" as above \n\n# - In the following, the fourth-order model (p=4) is considered as in SW (2001)\n\nVAR_lag_p=VAR(SW_data, p = 4, type = c(\"const\"),season = NULL, \n              exogen = NULL, lag.max = NULL)\n\nsummary(VAR_lag_p) \n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: Inflation, Unemployment, Fedfunds \nDeterministic variables: const \nSample size: 160 \nLog Likelihood: -396.638 \nRoots of the characteristic polynomial:\n0.9696 0.9696 0.7928 0.7928 0.686 0.686 0.5674 0.5674 0.4609 0.4609 0.2053 0.2053\nCall:\nVAR(y = SW_data, p = 4, type = c(\"const\"), exogen = NULL, lag.max = NULL)\n\n\nEstimation results for equation Inflation: \n========================================== \nInflation = Inflation.l1 + Unemployment.l1 + Fedfunds.l1 + Inflation.l2 + Unemployment.l2 + Fedfunds.l2 + Inflation.l3 + Unemployment.l3 + Fedfunds.l3 + Inflation.l4 + Unemployment.l4 + Fedfunds.l4 + const \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nInflation.l1     0.5886863  0.0821552   7.166 3.47e-11 ***\nUnemployment.l1 -0.8356913  0.4006087  -2.086  0.03870 *  \nFedfunds.l1      0.2370805  0.1082335   2.190  0.03007 *  \nInflation.l2     0.0902554  0.0937902   0.962  0.33747    \nUnemployment.l2  1.3545379  0.6841360   1.980  0.04958 *  \nFedfunds.l2     -0.2168750  0.1453158  -1.492  0.13773    \nInflation.l3     0.1205897  0.0944654   1.277  0.20377    \nUnemployment.l3 -1.0953222  0.6779667  -1.616  0.10833    \nFedfunds.l3     -0.0005773  0.1455216  -0.004  0.99684    \nInflation.l4     0.1866837  0.0846107   2.206  0.02891 *  \nUnemployment.l4  0.4122130  0.3765054   1.095  0.27538    \nFedfunds.l4     -0.0231108  0.1113078  -0.208  0.83581    \nconst            1.0641191  0.3968301   2.682  0.00817 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1.053 on 147 degrees of freedom\nMultiple R-Squared: 0.8509, Adjusted R-squared: 0.8387 \nF-statistic:  69.9 on 12 and 147 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation Unemployment: \n============================================= \nUnemployment = Inflation.l1 + Unemployment.l1 + Fedfunds.l1 + Inflation.l2 + Unemployment.l2 + Fedfunds.l2 + Inflation.l3 + Unemployment.l3 + Fedfunds.l3 + Inflation.l4 + Unemployment.l4 + Fedfunds.l4 + const \n\n                  Estimate Std. Error t value Pr(&gt;|t|)    \nInflation.l1     0.0066525  0.0181442   0.367 0.714410    \nUnemployment.l1  1.4696483  0.0884753  16.611  &lt; 2e-16 ***\nFedfunds.l1      0.0006271  0.0239036   0.026 0.979106    \nInflation.l2    -0.0115173  0.0207138  -0.556 0.579040    \nUnemployment.l2 -0.5155262  0.1510930  -3.412 0.000833 ***\nFedfunds.l2      0.0604859  0.0320933   1.885 0.061446 .  \nInflation.l3     0.0317894  0.0208629   1.524 0.129724    \nUnemployment.l3 -0.0057705  0.1497305  -0.039 0.969310    \nFedfunds.l3     -0.0399995  0.0321388  -1.245 0.215265    \nInflation.l4    -0.0200840  0.0186865  -1.075 0.284230    \nUnemployment.l4 -0.0050194  0.0831521  -0.060 0.951947    \nFedfunds.l4      0.0114985  0.0245826   0.468 0.640657    \nconst            0.0854091  0.0876408   0.975 0.331392    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2326 on 147 degrees of freedom\nMultiple R-Squared: 0.9786, Adjusted R-squared: 0.9768 \nF-statistic: 559.3 on 12 and 147 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation Fedfunds: \n========================================= \nFedfunds = Inflation.l1 + Unemployment.l1 + Fedfunds.l1 + Inflation.l2 + Unemployment.l2 + Fedfunds.l2 + Inflation.l3 + Unemployment.l3 + Fedfunds.l3 + Inflation.l4 + Unemployment.l4 + Fedfunds.l4 + const \n\n                Estimate Std. Error t value Pr(&gt;|t|)    \nInflation.l1     0.07293    0.06945   1.050  0.29536    \nUnemployment.l1 -1.38408    0.33865  -4.087 7.16e-05 ***\nFedfunds.l1      0.95325    0.09149  10.419  &lt; 2e-16 ***\nInflation.l2     0.20379    0.07928   2.570  0.01115 *  \nUnemployment.l2  1.27436    0.57832   2.204  0.02911 *  \nFedfunds.l2     -0.40302    0.12284  -3.281  0.00129 ** \nInflation.l3    -0.07175    0.07985  -0.899  0.37035    \nUnemployment.l3 -0.49829    0.57311  -0.869  0.38601    \nFedfunds.l3      0.34476    0.12301   2.803  0.00575 ** \nInflation.l4    -0.04306    0.07152  -0.602  0.54804    \nUnemployment.l4  0.49476    0.31827   1.555  0.12221    \nFedfunds.l4      0.03129    0.09409   0.333  0.73997    \nconst            0.53914    0.33545   1.607  0.11015    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.8905 on 147 degrees of freedom\nMultiple R-Squared: 0.9273, Adjusted R-squared: 0.9214 \nF-statistic: 156.3 on 12 and 147 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n             Inflation Unemployment Fedfunds\nInflation     1.109681     0.001595   0.1486\nUnemployment  0.001595     0.054126  -0.0928\nFedfunds      0.148558    -0.092796   0.7930\n\nCorrelation matrix of residuals:\n             Inflation Unemployment Fedfunds\nInflation     1.000000     0.006508   0.1584\nUnemployment  0.006508     1.000000  -0.4479\nFedfunds      0.158370    -0.447925   1.0000\n\n# Estimation results are presented equation-by-equation (see lecture notes)\n# Roots of the characteristic polynomial are eigenvalues of the companion\n# form parameter matrix\n\nlogLik(VAR_lag_p) ## The value of the log-likelihood function\n\n'log Lik.' -396.6383 (df=39)\n\n#----- Residual diagnostics, checking the adequacy of the selected model -----\n\nplot(VAR_lag_p)  # residuals (time series) and their autocorrelations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Cross-correlations and the correlations of the squared residuals\n\nres=resid(VAR_lag_p) \ncolnames(res)=c(\"Inflation\",\"Unemployment\",\"FedFunds\") # the SW_system\n\n# Auto and cross-correlations as above \nacf(res,lag.max = 10, type=\"correlation\")\n\n\n\n\n\n\n\n# Squared residuals \ncolnames(res)=c(\"res_Inflation^2\",\"res_Unemployment^2\",\"res_FedFunds^2\")\nacf(res^2,lag.max = 10, type=\"correlation\")  \n\n\n\n\n\n\n\n# Quantile-to-quantile plots. How well the residuals correspond the quantiles \n# of the normal distribution\n# ?qqnorm # see details there\n\nqqnorm(res[,1]) # the component to be considered must be specified. \n\n\n\n\n\n\n\n# Here the first variable (inflation in this case).\n\n\n# The statistical significance of the auto and cross-correlations \n#  --&gt; Portmanteau test\n\n# ?serial.test\n\nserial.test(VAR_lag_p, lags.pt = 12, type = \"PT.adjusted\") #\n\n\n    Portmanteau Test (adjusted)\n\ndata:  Residuals of VAR object VAR_lag_p\nChi-squared = 132.39, df = 72, p-value = 1.904e-05\n\nserial.test(VAR_lag_p, lags.pt = 15, type = \"PT.adjusted\") # \n\n\n    Portmanteau Test (adjusted)\n\ndata:  Residuals of VAR object VAR_lag_p\nChi-squared = 149.51, df = 99, p-value = 0.0007889\n\nserial.test(VAR_lag_p, lags.pt = 6, type = \"PT.adjusted\") # \n\n\n    Portmanteau Test (adjusted)\n\ndata:  Residuals of VAR object VAR_lag_p\nChi-squared = 48.748, df = 18, p-value = 0.0001165\n\n# ----------------------------- Forecasting --------------------------------\n\n# Let us use the sample period of SW (up to 2000:Q4) to estimate \n# the parameters andconstruct forecasts for the next few observations\n\n# - As output, we get forecast, forecast intervals based on the given upper \n# and lower bounds and \"CI\" which is the distance between the interval forecasts.\n\n# - Forecasts are constructed using the iterative approach where, e.g., \n# two-period forecast is based on one-period forecasts, and so on \n# (see the equations in the lecture manuscript)\n\nSW_forecasting=window(SW_system,end=c(2000,4))\nVAR_p_forecasting=VAR(SW_forecasting, p = 4, type = c(\"const\"),season = NULL, \n                      exogen = NULL, lag.max = NULL)\nforecasts=predict(VAR_p_forecasting,n.ahead=4,ci=0.95) # forecasts 4-periods ahead\nforecasts$fcst\n\n$infl\n         fcst     lower    upper       CI\n[1,] 2.902410 0.8377561 4.967063 2.064654\n[2,] 2.936851 0.4167294 5.456973 2.520122\n[3,] 3.053214 0.3107901 5.795638 2.742424\n[4,] 3.156726 0.1845968 6.128856 2.972130\n\n$unemp\n         fcst    lower    upper        CI\n[1,] 3.937531 3.481547 4.393514 0.4559835\n[2,] 4.045763 3.235417 4.856108 0.8103454\n[3,] 4.180429 3.106765 5.254093 1.0736640\n[4,] 4.341953 3.083059 5.600848 1.2588942\n\n$intr\n         fcst    lower     upper       CI\n[1,] 6.507526 4.762214  8.252838 1.745312\n[2,] 6.415671 3.722377  9.108964 2.693293\n[3,] 6.465216 3.238797  9.691635 3.226419\n[4,] 6.417915 2.696100 10.139730 3.721815\n\nplot(forecasts)\n\n\n\n\n\n\n\nfanchart(forecasts) ## see http://www.jstatsoft.org/v27/i04/paper, p. 12.\n\n\n\n\n\n\n\n#------- Granger causality tests  (+ instantaneous causality tests) --------\n\n# Recall that H_0: \"There is no Granger causality\"\n\n# Below, we first test that there is no Granger causality from inflation to \n# unemployment and Federal Funds\n\n# Notice that the program computes the (approximate) p-values based on \n# the F-distribution (which has been found to be a good approximation in \n# some simulation experiments) instead of the chi^2-distribution.\n\n\nGc_inf=causality(VAR_lag_p, cause = \"Inflation\")\nGc_inf\n\n$Granger\n\n    Granger causality H0: Inflation do not Granger-cause Unemployment\n    Fedfunds\n\ndata:  VAR object VAR_lag_p\nF-Test = 4.1184, df1 = 8, df2 = 441, p-value = 9.358e-05\n\n\n$Instant\n\n    H0: No instantaneous causality between: Inflation and Unemployment\n    Fedfunds\n\ndata:  VAR object VAR_lag_p\nChi-squared = 5.0489, df = 2, p-value = 0.0801\n\nGc_unem=causality(VAR_lag_p, cause = \"Unemployment\")\nGc_unem\n\n$Granger\n\n    Granger causality H0: Unemployment do not Granger-cause Inflation\n    Fedfunds\n\ndata:  VAR object VAR_lag_p\nF-Test = 4.0828, df1 = 8, df2 = 441, p-value = 0.0001044\n\n\n$Instant\n\n    H0: No instantaneous causality between: Unemployment and Inflation\n    Fedfunds\n\ndata:  VAR object VAR_lag_p\nChi-squared = 27.417, df = 2, p-value = 1.113e-06\n\nGc_ff=causality(VAR_lag_p, cause = \"Fedfunds\")\nGc_ff\n\n$Granger\n\n    Granger causality H0: Fedfunds do not Granger-cause Inflation\n    Unemployment\n\ndata:  VAR object VAR_lag_p\nF-Test = 2.813, df1 = 8, df2 = 441, p-value = 0.004739\n\n\n$Instant\n\n    H0: No instantaneous causality between: Fedfunds and Inflation\n    Unemployment\n\ndata:  VAR object VAR_lag_p\nChi-squared = 29.563, df = 2, p-value = 3.805e-07\n\nGc_infunem=causality(VAR_lag_p, cause = c(\"Inflation\", \"Unemployment\"))\nGc_infunem\n\n$Granger\n\n    Granger causality H0: Inflation Unemployment do not Granger-cause\n    Fedfunds\n\ndata:  VAR object VAR_lag_p\nF-Test = 4.8627, df1 = 8, df2 = 441, p-value = 9.185e-06\n\n\n$Instant\n\n    H0: No instantaneous causality between: Inflation Unemployment and\n    Fedfunds\n\ndata:  VAR object VAR_lag_p\nChi-squared = 29.563, df = 2, p-value = 3.805e-07\n\nGc_unemff=causality(VAR_lag_p, cause = c(\"Unemployment\", \"Fedfunds\"))\nGc_unemff\n\n$Granger\n\n    Granger causality H0: Unemployment Fedfunds do not Granger-cause\n    Inflation\n\ndata:  VAR object VAR_lag_p\nF-Test = 3.4015, df1 = 8, df2 = 441, p-value = 0.0008338\n\n\n$Instant\n\n    H0: No instantaneous causality between: Unemployment Fedfunds and\n    Inflation\n\ndata:  VAR object VAR_lag_p\nChi-squared = 5.0489, df = 2, p-value = 0.0801\n\nGc_ffinf=causality(VAR_lag_p, cause = c(\"Inflation\", \"Fedfunds\"))\nGc_ffinf\n\n$Granger\n\n    Granger causality H0: Inflation Fedfunds do not Granger-cause\n    Unemployment\n\ndata:  VAR object VAR_lag_p\nF-Test = 4.8236, df1 = 8, df2 = 441, p-value = 1.039e-05\n\n\n$Instant\n\n    H0: No instantaneous causality between: Inflation Fedfunds and\n    Unemployment\n\ndata:  VAR object VAR_lag_p\nChi-squared = 27.417, df = 2, p-value = 1.113e-06",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Basics of vector autoregression</span>"
    ]
  },
  {
    "objectID": "TSE-ch12.html",
    "href": "TSE-ch12.html",
    "title": "13  Nonstationary processes",
    "section": "",
    "text": "13.1 Integrated process\nIn some application fields, such as economics and finance, the existence of trends in the analyzed time series is common.\nEarlier in this material, it was discussed that nonstationary time series containing a trend can be investigated using methods developed for stationary time series if the trend is first removed using a suitable transformation. The most popular of such transformations is taking differences, which amounts to modelling the differenced series \\(\\Delta y_t = y_t-y_{t-1}\\).\nBased on the aforementioned, let us introduce the following definition for univariate time series process:\nIntegrated process. Stochastic process \\(y_t\\) \\(\\{y_t, t=1,2,\\dots\\}\\) is said to be an integrated process of order \\(d\\) or \\(I(d\\)) process if the \\(d\\) times differenced process \\[\\begin{equation*}\n\\Delta^d(y_t-\\mathsf{E}(y_t)), \\quad t=1,2,\\dots\n\\end{equation*}\\] is stationary, but the \\(d-1\\) times differenced process \\[\\begin{equation*}\n\\Delta^{d-1}(y_t-\\mathsf{E}(y_t)), \\quad t=1,2,\\dots\n\\end{equation*}\\] is not. If \\(y_t\\) is an \\(I(d)\\) process, we denote \\[\\begin{equation*}\ny_t\\sim \\mathrm{I}(d) \\,\\,\\, (d\\geq 1).\n\\end{equation*}\\] In this course, we will consider only \\(d=1\\) and \\(d=0\\).\nRandom walk. The simplest example of an \\(I(1)\\) process is random walk, which was briefly covered already in connnection to the AR(1) process. Here the random walk is defined with a drift term \\(\\nu\\): \\[\\begin{equation*}\n    \\Delta y_t = \\nu + u_t, \\,\\, t=1,2,\\ldots\n\\end{equation*}\\] where \\(u_t\\sim \\mathsf{iid}(0,\\sigma^2\\)). By recursive substitutions, we get \\[\\begin{equation*}\n    y_t=y_0+\\nu t + \\sum_{j=1}^{t}u_j, \\,\\,\\, t=1,2,\\dots\n\\end{equation*}\\] Sometimes, depending on the time series and application that we are considering. it is assumed that \\(\\nu=0\\), that is, we have random walk without drift, and hence the term \\(\\nu t\\) above vanishes. Moreover, in that case, differences \\(y_t-y_{t-1}=u_t\\) from the process \\(y_t=y_0+\\sum_{j=0}^{t-1}u_{t-j}\\) are stationary, and this holds even if instead of \\(u_t\\sim\\mathsf{iid}(0,\\sigma^2)\\) we only assume \\(u_t\\) is stationary.\nAssuming the initial (starting) value \\(y_0\\) as constant, for the random walk we get (details are left as an exercise) \\[\\begin{equation*}\n    \\mathsf{E}(y_t)=y_0+\\nu t,\n\\end{equation*}\\] \\[\\begin{equation*}\n    \\mathsf{Var}(y_t)=\\mathsf{Var}\\left(\\sum_{j=1}^{t}u_j\\right)=\\sigma^2t,\n\\end{equation*}\\] \\[\\begin{equation*}\n    \\mathsf{Cor}(y_t,y_{t+k})=\\frac{1}{\\sqrt{1+k/t}}, \\,\\, k\\geq 0.\n\\end{equation*}\\] Here we can observe that random walk is not stationary even if the value of \\(y_0\\) is altered.\nRandom walk is the simplest example of so-called ARIMA processes, as introduced below. The letter \\(I\\) in the middle refers to the term “integrated” as the sum \\(\\sum_{j=0}^{t-1}u_{t-j}\\) is interpreted as an integral. Furthermore, as the term \\(\\sum_{j=1}^{t}u_j\\) is stochastic (random), this trend component is often called a stochastic trend.\nThis brings us to the conclusion: As many economic (and other) time series exhibit a trend, it is important to distinguish whether a trend (persistent long-term movement over time) in a time series is generated by stochastic or deterministic trend.\nStill in a nutshell, an \\(I(0)\\) time series process fluctuates around its mean with a constant and infinite variance that does not depend on time, while \\(I(1)\\) series wanders widely.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonstationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch12.html#integrated-process",
    "href": "TSE-ch12.html#integrated-process",
    "title": "13  Nonstationary processes",
    "section": "",
    "text": "Here the term “trend” is used quite generally.\n\n\n\nHere \\(\\Delta = 1-B\\) is the difference operator.\nThese series might seem stationary and thus be modelled with stationary models reasonably well. If the differences still include some sort of trend, it is natural to extend this procedure to the difference of these differences \\(\\Delta^2 y_{t} = y_t-2y_{t-1}+y_{t-2}\\), and so on. This, however, is very rare in practice, and typically only first differences are considered when differencing data.\n\n\n\n\n\n\n\n\nAs \\(\\Delta y_t\\sim\\mathsf{iid}(\\nu,\\sigma^2)\\) is stationary, we can conclude that the random walk is an \\(I(1)\\) process.\nFrom the calculations above, we can observe that for random walk \\(\\mathsf{Var}(y_t)\\rightarrow\\infty\\) and \\(\\mathsf{Cor}(y_t,y_{t+k})\\rightarrow 1\\) with any \\(k&gt;0\\) as \\(t\\rightarrow\\infty\\). Therefore, random walk processes are strongly autocorrelated, and due to their wandering nature, they exhibit trend-like features.\n\n\n\n\nIf \\(\\nu\\neq0\\), this stochastic trend appears around the deterministic trend \\(y_0+\\nu t\\). A deterministic trend is a non-random function of time, such as a linear function. A time series fluctuates around its deterministic trend component.\nA stochastic trend is random and varies over time. A stochastic trend exhibits a prolonged period of increase followed by a prolonged period of decrease in the time series.\nMoreover, a process, say, \\(y_t = \\alpha + \\beta t + \\psi(B) u_t\\), where \\(\\psi(B)\\) refers to the MA(\\(\\infty\\)) presentation of the ARMA process (including the special case of \\(\\psi(B) =1\\), as discussed in Section 1), is sometimes described as a trend-stationary process, because if one subtracts the trend (\\(\\alpha + \\beta t\\)) the result is a stationary process.\n\n\n\nIt turns out that it is often more appropriate to model economic (macroeconomic and financial) time series assuming that there is a stochastic trend rather than a deterministic trend.\n\n\n\nAt times it is said that an \\(I(0)\\) process is mean reverting, as there is tendency that the series returns to its mean. Furthermore, the process has a limited memory of its past behaviour and sample autocorrelations decay relatively rapidly to zero.\nIn contrast, \\(I(1)\\) process has an infinitely long memory (see, e.g., random walk case above). Each innovation has a permanent effect on the process. Estimated autocorrelations decay to zero very slowly.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonstationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch12.html#arimapdq-process",
    "href": "TSE-ch12.html#arimapdq-process",
    "title": "13  Nonstationary processes",
    "section": "13.2 ARIMA(\\(p\\),\\(d\\),\\(q\\)) process",
    "text": "13.2 ARIMA(\\(p\\),\\(d\\),\\(q\\)) process\nIf the process \\(y_t, \\, t=0,1,\\dots\\), is non-stationary, but the difference \\(\\Delta y_t\\) is stationary and follows an invertible ARMA(\\(p,q\\)) process, \\(y_t\\) is called an ARIMA(\\(p,1,q\\)) process.\n\nThe order in the middle refers to the fact, that stationarity is achieved by differencing the process once.\nIf the process is non-stationary after being differenced once, but the second differences \\(\\Delta^2y_t=(y_t-y_{t-1})-(y_{t-1}-y_{t-2})\\) follow a stationary and invertible ARMA(\\(p,q\\)) process, \\(y_t\\) is called an ARIMA(\\(p,2,q\\)) process.\n\nIn general, \\(y_t\\) is called an ARIMA(\\(p,d,q\\)) process if it becomes a stationary and invertible ARMA(\\(p,q\\)) process after differencing \\(d\\) times.\n\nThat is, \\(\\Delta^d y_t \\sim \\mathrm{ARMA}(p,q)\\).\nIn practice, \\(d=1\\) is by far the most common case, \\(d=2\\) can be encountered quite rarely, and \\(d&gt;2\\) can be safely ignored.\nWith \\(d=0\\), we obtain the special case of an ARMA(\\(p,q\\)) process.\n\n\nThe typical properties of an ARIMA(\\(p\\),1,\\(q\\)) process are generally the same as for random walk. The realizations exhibit a wandering nature explained by the variance growing as a function of \\(t\\) and strong autocorrelation even though the autocorrelation function cannot be defined like with stationary processes.\n \nA warning on overdifferencing. While differencing is a fundamental tool for making a nonstationary time series stationary, applying it too many times, a problem known as overdifferencing, can be counterproductive. It is a common pitfall to assume that if one difference helps, a second one might help more. Even more often, you should not mechanically difference a time series that is already “stationary enough”.\n\nDoing so without a strong theoretical justification (e.g., an economic model suggesting a process is integrated of order two, I(2)), is highly unlikely and can even harm your statistical/econometric analysis.\n\nOverdifferencing does not make a series “more stationary”. Instead, it introduces new artificial patterns into the data that were not present in the original process. This complicates model building and can lead to poor analyses and forecasts.\n\nThe most significant problem is that overdifferencing creates a specific, predictable correlation structure. Specifically, it often introduces a strong negative autocorrelation at lag 1. A model then has to be made more complex (e.g., by adding a moving average term, MA(1)) just to remove the artificial pattern you introduced. This violates the principle of parsimony.\nIncreased Variance: Differencing a stationary series can actually increase its variance, making the series noisier and harder to model accurately.\n\nIllustration: Overdifferencing white noise. Let see what happens when we difference a series that is already stationary: a white noise process, \\(y_t = \\varepsilon_t\\), where \\(\\varepsilon_t \\sim \\mathsf{wn}(0, \\sigma^2)\\). This series has by definition zero autocorrelation at all lags. If we mistakenly difference it, we create a new series, \\(z_t= \\Delta y_t = y_t - y_{t-1} = \\varepsilon_t - \\varepsilon_{t-1}\\). This new series, \\(z_t\\), is no longer white noise. Let’s examine its properties:\n\nVariance: \\(\\mathsf{Var}(z_t) = \\mathsf{Var}(\\varepsilon_t - \\varepsilon_{t-1}) = \\mathsf{Var}(\\varepsilon_t) + \\mathsf{Var}(\\varepsilon_{t-1}) = \\sigma^2 + \\sigma^2 = 2\\sigma^2\\). We have doubled the variance!\nAutocorrelation: The autocovariance at lag 1 is \\(\\mathsf{Cov}(z_t, z_{t-1}) = \\mathsf{Cov}(\\varepsilon_t - \\varepsilon_{t-1}, \\varepsilon_{t-1} - \\varepsilon_{t-2}) = -\\mathsf{Var}(\\varepsilon_{t-1}) = -\\sigma^2\\).\n\nThe autocorrelation at lag 1 is therefore: \\[\\begin{equation*}\n\\rho_1 = \\frac{\\mathsf{Cov}(z_t, z_{t-1})}{\\sqrt{\\mathsf{Var}(z_t)\\mathsf{Var}(z_{t-1})}} = \\frac{-\\sigma^2}{\\sqrt{2\\sigma^2 \\cdot 2\\sigma^2}} = \\frac{-\\sigma^2}{2\\sigma^2} = -0.5\n\\end{equation*}\\] By differencing a series with zero autocorrelation, we created a new series with a perfectly defined negative autocorrelation of -0.5 at lag 1.\n\nAs we will discuss later on, always check for stationarity using tools like the ACF plot and unit root tests (e.g., ADF test, see the coming section) before applying another round of differencing. If the series already appears stationary enough, stop.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonstationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch12.html#unit-root-process",
    "href": "TSE-ch12.html#unit-root-process",
    "title": "13  Nonstationary processes",
    "section": "13.3 Unit root process",
    "text": "13.3 Unit root process\nLet’s consider the (univariate) AR(\\(p\\)) process \\[\\begin{equation*}\na(B)z_t=u_t,\n\\end{equation*}\\] where \\(a(B)=1-a_1 B-\\dots-a_p B^p\\).\n\nHere notation \\(z_t\\) will be used when the unit root process is not assumed to have a constant or other deterministic trends.\n\nIt can be shown (see the Extra tag below) that the polynomial \\(a(\\mathsf{B})\\) can be rewritten as \\[\\begin{equation*}\na\\left(B\\right) =\\Delta -\\phi B - \\phi _{1}\\Delta B -\\cdots -\\phi _{p-1}\\Delta B^{p-1}, \\quad  \\phi =-a\\left( 1\\right),\n\\end{equation*}\\] so the process \\(z_t\\) can be represented as \\[\\begin{equation*}\n\\Delta z_{t}=\\phi z_{t-1}+\\phi _{1}\\Delta z_{t-1}+\\cdots +\\phi _{p-1}\\Delta z_{t-p+1}+ u _{t}, \\quad t=1,2,\\ldots,\n\\end{equation*}\\] where \\(u _{t}\\sim \\mathsf{iid}(0,\\sigma ^{2})\\).\nLet’s assume that for \\(a(z)\\) the following holds: \\[\\begin{equation*}\n    \\text{If} \\,\\, a(z)=0, \\,\\, \\mathrm{then} \\,\\, |z|&gt;1 \\,\\, \\mathrm{or} \\,\\, |z|=1.\n\\end{equation*}\\] In other words, the roots of the polynomial \\(a(z)\\) lie outside or at the unit circle on the complex plane.\n\nIf all the roots lie outside the unit circle, \\(z_t\\) is (at least) asymptotically stationary (as meeting the stationarity condition of the AR(\\(p\\)) process).\n\nLet us now assume there is exactly one unit root. From the known properties of polynomials, it follows that \\[\\begin{equation*}\na(z)=(1-z)b(z),\n\\end{equation*}\\] where \\(b(z)\\) is a polynomial of the degree (at most) \\(p-1\\) with its roots inevitably lying outside the unit circle (see the Extra below).\n\nIt is clear that this is equal to \\(\\phi=0\\) above. When \\(\\phi=0\\), the above-mentioned polynomial becomes \\(b(z) =1-\\phi _{1}z-\\cdots -\\phi_{p-1}z^{p-1}:=\\phi(z)\\).\n\nThus, \\(\\Delta z_t\\) is \\(I(0)\\) and as the roots of the polynomial \\(\\phi(z)\\) lie outside the unit circle, such initial values \\(\\Delta z_{t-1},...,\\Delta z_{t-p+1}\\) can be found that \\(\\Delta z_t\\) is stationary.\n\nTherefore, the process \\(\\Delta z_t\\) can be written as \\(\\Delta z_{t}=\\phi(B)^{-1}u _{t}\\), which is a special case of the random walk we looked at earlier but with the stationary AR(\\(p-1\\)) process \\(\\phi(\\mathsf{B}) ^{-1}u _{t}\\) replacing the innovation \\(u_t\\).\n\n \n\nExtra: Polynomials and power series\n\n\nConsider polynomial function of power \\(m\\) \\[\\begin{equation*}\n    p\\left(z\\right)=\\sum_{k=0}^{m}a_{k}z^{k},\n\\end{equation*}\\] where \\(a_k\\) and \\(z\\) are real or complex numbers and \\(a_m\\neq 0\\). In the case of complex numbers \\(z\\in\\mathbb{C}\\), by the the polynomial \\(p(z)\\) can be given the form \\[\\begin{equation*}\n    p\\left(z\\right)=a_{m}\\left(z-\\zeta_{1}\\right) \\cdots \\left(z-\\zeta_{m}\\right) ,\n\\end{equation*}\\] where \\(\\zeta _{1},...,\\zeta_{m}\\) are the roots of \\(p\\left(z\\right)\\) and thus \\(p\\left(\\zeta _{k}\\right) =0\\). The case \\(\\zeta_{k}=\\zeta _{l}\\) \\(\\left(k\\neq l\\right)\\) is possible and in the case of a real coefficient \\(a_k\\in\\mathbb{R}\\) the roots occur as complex conjugates for all \\(k\\); that is if \\(\\zeta_{k}=x_{k}+iy_{k}\\) \\(\\left(i^{2}=-1\\right)\\), for some \\(l\\neq k\\) it holds \\(\\zeta_{l}=x_{l}-iy_{k}=:\\bar{\\zeta}_{k}\\).\nLet’s return to a normal polynomial \\(p\\left(z\\right)=\\sum_{k=0}^{m}a_{k}z^{k}\\). It is easy to conclude that \\(p(z)\\) can be written as \\[\\begin{equation*}\n    p\\left(z\\right)=p\\left(1\\right)+\\left(1-z\\right)q\\left(z\\right),\n\\end{equation*}\\] where \\[\\begin{equation*}\n    q\\left(z\\right)=\\sum_{k=0}^{m-1}b_{k}z^{k}, \\quad b_{k}=-\\sum_{j=k+1}^{m}a_{j} \\quad k=1,...,m-1.\n\\end{equation*}\\] A corresponding alternative way to present this would be \\[\\begin{equation*}\n    p\\left(z\\right)=p\\left(1\\right) z+\\left(1-z\\right) r\\left(z\\right),\n\\end{equation*}\\] where \\[\\begin{equation*}\n    r\\left(z\\right)=\\sum_{k=0}^{m-1}c_{k}z^{k}, \\quad c_{0}=a_{0} \\quad \\mathrm{and} \\quad c_{k}=-\\sum_{j=k+1}^{m}a_{j}, \\, k=1,...,m-1\n\\end{equation*}\\] These can be generalized to the power series, that is to say to the case \\[\\begin{equation*}\n    p\\left(z\\right)=\\sum_{k=0}^{\\infty }a_{k}z^{k} \\quad \\left(\\left\\vert z\\right\\vert \\leq 1\\right) .\n\\end{equation*}\\] If we assume \\[\\begin{equation*}\n    \\sum_{k=1}^{\\infty}k\\left\\vert a_{k}\\right\\vert&lt;\\infty ,\n\\end{equation*}\\] it can be easily concluded that the above equations hold when \\(\\left\\vert z\\right\\vert \\leq 1\\) and \\(m\\) are replaced with \\(\\infty\\). In addition \\(\\sum_{k=1}^{\\infty}\\left\\vert b_{k}\\right\\vert&lt;\\infty\\) and \\(\\sum_{k=1}^{\\infty}\\left\\vert c_{k}\\right\\vert&lt;\\infty\\).\n\n \nAs discussed above in connection to integrated and ARIMA processes, the presence of a unit root is particularly interesting question from an economic point of view. In models (and processes) with unit roots, shocks (such as policy or technological interventions and disruptions) have persistent effects that last forever, whereas with stationary cases such shocks have only a temporary effect. Therefore, it is of particular interest to test the unit root hypothesis in \\(y_t\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonstationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch12.html#testing-for-a-unit-root",
    "href": "TSE-ch12.html#testing-for-a-unit-root",
    "title": "13  Nonstationary processes",
    "section": "13.4 Testing for a unit root",
    "text": "13.4 Testing for a unit root\nLet us continue with the autoregressive unit root process from the previous section \\[\\begin{equation*}\n    \\Delta z_{t}=\\phi z_{t-1}+u _{t}, \\quad t=1,2,...,\n\\end{equation*}\\] where for the sake simplicity \\(z_0=0\\) and \\(u_t \\sim \\mathsf{iid}(0,\\sigma^2)\\). The question of interest is to test the unit root hypothesis \\[\\begin{equation*}\n    H_{0}:\\phi =0\n\\end{equation*}\\] against the stationary (or stable) alternative of \\(\\phi&lt;1\\).\nIt seems natural to base the test on the least squares estimator (or maximum likelihood estimator, see the forthcoming section on parameter estimation) of \\(\\phi\\): \\[\\begin{equation*}\n\\widehat{\\phi}=\\left(\\sum_{t=1}^{T} z_{t-1}^{2}\\right)^{-1} \\sum_{t=1}^{T} z_{t-1} \\Delta z_{t} \\overset{H_0}{=} \\left(\\sum_{t=1}^{T} z_{t-1}^{2}\\right)^{-1} \\sum_{t=1}^{T} z_{t-1} u_{t},\n\\end{equation*}\\] where \\(T\\) is the number of observations. The unit root test can be based directly on the estimator \\(\\widehat{\\phi}\\), but it is more common to use the “t-ratio” printed by different statistical programs \\[\\begin{equation*}\n    \\tau :=\\frac{\\sqrt{\\sum_{t=1}^{T}z_{t-1}^{2}}}{\\widehat{\\sigma}}\\widehat{\\phi},\n\\end{equation*}\\] which follows, under the null (unit root) hypothesis, nonstandard asymptotic distribution.\n\nThe limiting distribution is independent of the unknown parameters and can be tabulated (tables are often presented in books and computer programs).\nThe percentage points of the limiting distributions usually require numerical methods or simulations. In fact both the estimator \\(\\widehat{\\phi}\\) and the “t-ratio” \\(\\tau\\) have a non-Gaussian and left skewed asymptotic distribution. For example, with the \\(\\tau\\) test statistic the 5%-percentage point is about -1.94 while with a \\(\\mathsf{N}(0,1)\\) it is -1.645, and thus the unit root hypothesis gets rejected too easily.\n\n \nThe above unit root test can be generalized into a case with deterministic constant and trend (if appropriate due to shape of the time series or background information on it). A common model for this is \\[\\begin{equation*}\n    y_{t}=\\mu_{t}+z_{t}, \\quad t=1,2,\\ldots,\n\\end{equation*}\\] where \\(z_t\\) is as determined above (using \\(x_t\\)) and the deterministic trend is \\[\\begin{equation*}\n\\mu_{t}=\\mu  \\quad \\mathrm{or} \\quad \\mu_{t}=\\mu _{1}+\\mu _{2}t.\n\\end{equation*}\\] Indicator variables can also be used to account for seasonal variation if necessary.\n\nThe limiting distribution of the “t-ratio” can again be tabled. It deviates from the one with no constant and is more skewed to the left (5% percentage point is now -2.86).\nThe skewedness of the asymptotic distribution keeps growing when the “t-ratio”-based test is generalized for the linear trend \\(d_{t}=\\mu _{1}+\\mu_{2}t\\) (the details will be skipped now).\n\n \n\nExtra: Details on the above unit root test statistics\n\n\nLet’s consider more details about unit root testing as described above.\nThe asymptotic distribution of the estimator \\(\\widehat{\\phi}\\): Under the null hypothesis, then \\(z_{t}=\\sum_{j=1}^{t}u _{j}\\) is a (Gaussian) random walk, so the usual limit theorems do not hold. Moreover, it can be shown that the “t-ratio” follows the following (non-standard) asymptotic distribution \\[\\begin{equation*}\n    \\tau :=\\frac{\\sqrt{\\sum_{t=1}^{T}z_{t-1}^{2}}}{\\widehat{\\sigma}}\\widehat{\\phi}\n    \\overset{d}{\\rightarrow }\\left( \\int_{0}^{1}W\\left( u\\right) ^{2}du\\right)^{-1/2}\\int_{0}^{1}W\\left( u\\right) dW(u),\n\\end{equation*}\\] where \\(\\widehat{\\sigma}^{2}=(T-1)^{-1}\\sum_{t=1}^{T}(\\Delta z_{t}-\\widehat{\\phi}z_{t-1})^{2}\\) is the usual regression estimator for the error variance \\(\\sigma^2\\) (\\(T\\) can also be used as the denominator) and \\(W(u)\\) is standard Brownian motion.\nIn the latter case with the trend component, by multiplying the equation above on both sides by \\(\\Delta -\\phi \\mathsf{B}\\), reorganizing the terms and setting \\(\\mu_t=\\mu\\) we get \\[\\begin{equation*}\n    \\Delta y_{t}=\\nu +\\phi y_{t-1}+u _{t},\\,\\, t=1,2,...,\n\\end{equation*}\\] where \\(u_{t}\\sim \\mathsf{nid}\\left( 0,\\sigma^{2}\\right)\\) and \\(\\nu =-\\phi \\mu\\). When the unit root hypothesis holds, for the “t-ratio” \\(\\tau_\\mu\\) of the parameter \\(\\phi\\) holds \\[\\begin{equation*}\n    \\tau _{\\mu }\\overset{d}{\\rightarrow }\\left( \\int_{0}^{1}\\bar{W}\\left(\n    u\\right) ^{2}du\\right) ^{-1/2}\\int_{0}^{1}\\bar{W}\\left( u\\right) dW(u),\n\\end{equation*}\\] where \\(\\bar{W}\\) is the centered version of the standard Brownian motion.\n\n \nAugmented Dickey-Fuller (ADF) test. For now we should point out that the unit root test above can be generalized into the AR(\\(p\\)) case.\n\nUnlike additional deterministic components (as we saw above), additional lags (i.e. higher degree \\(p\\)) of the AR process do not impact the asymptotic distributions of the test statistics (assuming that the process is \\(I(1)\\) when the null hypothesis holds).\n\nFor example, given \\(y_t=\\mu+z_t\\) where \\(z_t\\sim\\) AR(\\(p\\)), and hence \\(y_t\\) also follows AR(\\(p\\)), we can use the test regression model \\[\\begin{equation*}\n    \\Delta y_{t}=\\nu +\\phi y_{t-1}+\\sum_{j=1}^{p-1}\\phi _{j}\\Delta y_{t-j}+u _{t}, \\quad t=1,2,...,\n\\end{equation*}\\] where (see the notation and discussion above) \\[\\begin{equation*}\nu _{t}\\sim \\mathsf{nid}\\left( 0,\\sigma ^{2}\\right) \\quad \\mathrm{and} \\quad \\nu= -\\phi \\mu.\n\\end{equation*}\\] and the null hypothesis \\(H_0:\\phi=0\\) implies the unit root process.\n\nThe “t-ratio” \\(\\tau \\mu\\) of the parameter \\(\\phi\\) with the same asymptotic distribution as above can be used here as the test statistic. All of the asymptotic assumptions hold also without Gaussian assumptions (so \\(u _{t}\\sim \\mathsf{iid}\\left( 0,\\sigma ^{2}\\right)\\) is enough).\n\nThe test regression model and the null hypothesis \\(\\phi=0\\) is called the Augmented Dickey-Fuller (ADF) test. If the null hypothesis of unit root cannot be rejected, according to the above equation, an AR(\\(p-1\\)) model can be built on the differences.\n\nIn other words, this implies the need to take first differences to get stationary time series.\nExecuting the ADF test regression requires the lag length \\(p\\) to be specified, which could be done by information criteria or consecutive tests, covered in the next section, or basing it some common fixed choice(s), possibly linked to the data frequency. All in all, it is recommended to consider a few different choices of \\(p\\) and compare the resulting testing results.\nMoreover, as discussed above, we can also extend the ADF test regression by deterministic trend component. That is, we would have then deterministic term \\(\\mu_1 + \\mu_2 t\\).\nDetermining the deterministic terms included in the ADF test is an important and necessary step. It can be based on graphical inspection of the time series and/or theoretical or other experience on the application of interest. That is whether including a trend is reasonable or not, while a constant term is almost always included in the test regression.\n\n \nEmpirical examples. Let us consider unit root testing for a few time series considered above in different parts. As discussed above, the first thing in the ADF test is to determine which deterministic terms to include in the test regression. Basically you should always include a constant term, but whether the deterministic trend component should be included as well should be determined based on visual inspection and background knowledge on the time series at hand.\nIn the following illustrations, we fix the number of lagged differenced lags \\(p\\) in the ADF test to 8 lags.\n\nConsider log of the CPI time series (Introduction): In this case, there is clearly an upward trend present but is it deterministic? Well, it seems reasonable to argue that due to different (economic) reasons we can think that price level is steadily going up and a deterministic trend seems reasonable to be included in the ADF test.\n\n\n\n\n\n\n\n\n\n\n\nFigure: Monthly log(CPI) time series (1990–2025).\n\n \nTest Statistic: -1.1129 \nP-value: 0.924528 \n\nLog of the S&P 500 index. Increasing stock prices point out very well a possible stochastic trend, but it seems difficult to argue that there is deterministic upward trend in stock prices. Therefore, including a constant is surely good choice, but for robustness we will also examine the unit root hypothesis also when including deterministic trend as well (together with the constant) in the ADF regression.\n\n\n\n\n\n\n\n\n\n\n\nFigure: Monthly log(S&P 500) time series (1990-2025).\n\n \nTest Statistic: -0.7689838 \nP-value: 0.8262138 \n\nTaking the log-differences of the CPI and S&P 500 indices results in the time series previously shown in the Introduction. A visual inspection alone suggests that the unit root hypothesis is no longer reasonable for these differenced series. To confirm this, let’s examine the Augmented Dickey-Fuller (ADF) test results below. The test includes a constant, as a deterministic trend is clearly no longer present in the data.\n\nThe ADF tests fail to reject the unit root hypothesis for the original level series of both the CPI and the S&P 500.\n\nFor the CPI levels, even after including a constant and a linear trend in the test regression, we still cannot reject the null hypothesis of a unit root. This strongly suggests that the CPI can be treated as an \\(I(1)\\) process (integrated of order one).\nA similar conclusion holds for the S&P 500 index.\n\nThese conclusions are reinforced by the test results for their log-differences, which appear to be stationary (i.e., \\(I(0)\\) variables). This provides a consistent picture: the original series are non-stationary, but their first differences are stationary.\nlogdiff(CPI):\nTest Statistic: -7.744163 \nP-value: 1.248182e-11 \n\nlogdiff(S&P 500)\nTest Statistic: -6.094829 \nP-value: 1.302771e-07 \n\n \nA critical limitation: The Low Power of the ADF Test. An important limitation of the ADF test is its low statistical power. In practical terms, this means:\nThe ADF test often fails to reject the null hypothesis of a unit root, even when the time series is actually stationary.\nIn other words, the test is not always very good at correctly identifying a stationary process, especially if the process is close to non-stationary (e.g., an AR(1) process where the autoregressive coefficient is close to 1). If we are unable to reject the presence of a unit root, it does not necessarily mean that it is true and the process is necessarily \\(I(1)\\). It could just be (but of course not always!) that there is not sufficient amount of informationin the data to reject the unit root.\nThe practical takeaway is that it’s common for a truly stationary time series to produce a p-value from the ADF test that is too high to be considered statistically significant. You can observe this phenomenon by running the following code, which simulates AR(1) processes, with varying AR(1) coefficients, and ADF test.\nFrom empirical point of view, the choice whether the process is a unit root process (nonstationary) or a “near” unit root process (stationary) is interesting but at times complicated.\n\nParticularly ambiguous are, e.g., interest rates. Interest rates are highly persistent and the unit root hypothesis cannot often be rejected, although nonstationary interest rates do not seem to be very plausible from an economic point of view.\n\n# Simulating AR(1) process with different values of phi_1 \n# - Change T and phi_1\n\n# Load the libraries\nlibrary(urca)\nlibrary(tseries)\n\nT=200         # number of observations\nphi_1=0.98    \n\nepsilon=rnorm(T)  # random draws from nid(0,1)\ny=epsilon\ny[1]=0 # for simplicity and E(y_t) = 0\nfor(i in 2:T) y[i] = phi_1*y[i-1]+epsilon[i]\nplot(y,type=\"l\", main=\"Simulated realization\")\n\ntest_ur_none &lt;- ur.df(y, type = \"none\", lags = adf_lags)\nstat_none &lt;- test_ur_none@teststat[1, 1]\np_val_none &lt;- punitroot(q = stat_none, N = length(test_ur_none@res), trend = \"nc\")\ncat(\"P-value:\", p_val_none, \"\\n\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonstationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch12.html#r-lab",
    "href": "TSE-ch12.html#r-lab",
    "title": "13  Nonstationary processes",
    "section": "13.5 R Lab",
    "text": "13.5 R Lab\nAll the R codes considered in this section are compiled in the following link:\n\nR Lab: Unit root testing (ADF test)\n\n\n\n# Unit root testing (ADF test)\n\n# Install packages if you haven't already\n# install.packages(\"urca\")\n# install.packages(\"tseries\")\n\n# Load the libraries\nlibrary(urca)\nlibrary(tseries)\nlibrary(zoo) # Used for handling year.month date formats in this Shiller data\n\n\n# Create a reproducible non-stationary series (a random walk!)\nset.seed(42)\nnon_stationary_series &lt;- cumsum(rnorm(200))\n\n# It's always a good idea to plot the data first\nplot(non_stationary_series, type = 'l', main = \"Simulated random walk (no drift)\")\n\n\n\n\n\n\n\n# ------------------ Going back to Introduction -------------------------\n\n# --- Define/select data and sample period ---\n\n# Set the sample period. The format (for this monthly data) is \"YYYY-MM\".\nstart_period &lt;- \"1990-01\"\nend_period &lt;- \"2025-06\"\n\n# Define the data file name\ndata_file &lt;- \"Shiller_data_031025.txt\"\n\n\n# --- Load and prepare data ---\n\n# Read the data from the text file. This assumes a tab-separated file with a header.\n# If your separator is different (e.g., a comma), use read.csv() instead.\ntryCatch({\n  shiller_data &lt;- read.delim(data_file, header = TRUE)\n}, error = function(e) {\n  stop(paste(\"Error reading the file:\", \n             data_file, \". Make sure it's in the same folder as the script.\"))\n})\n\n# Convert the 'Date' column to a proper time series date object\nshiller_data$Date_ts &lt;- as.yearmon(as.character(shiller_data$Date), \"%Y.%m\")\n\n# Filter the data to the desired sample period\ndata_subset &lt;- subset(shiller_data, Date_ts &gt;= as.yearmon(start_period) & \n                        Date_ts &lt;= as.yearmon(end_period))\n\n\n# --- Create Time Series Objects for Base R plotting ---\n\n# Get the start year and month from the first data point in our subset\nstart_year &lt;- floor(as.numeric(data_subset$Date_ts[1]))\nstart_month &lt;- cycle(data_subset$Date_ts[1])\n\n# Create 'ts' (time series) objects, here monthly data\ncpi_ts &lt;- ts(data_subset$CPI, start = c(start_year, start_month), frequency = 12)\np_ts &lt;- ts(data_subset$P, start = c(start_year, start_month), frequency = 12)\n\n# (i) Base R Implementation for CPI using plot.ts\nplot.ts(log(cpi_ts),col = \"blue\",lwd = 2,main = \"log(CPI)\",xlab = \"Date\",ylab = \"log(CPI)\")\n\n\n\n\n\n\n\n# (ii) Base R Implementation for S&P 500 (P) using plot.ts\nplot.ts(log(p_ts),col = \"blue\",lwd = 2,main = \"log(S&P 500)\",\n        xlab = \"Date\", ylab = \"S&P 500 Index\")\n\n\n\n\n\n\n\ndata_subset$cpi_log_diff_1m &lt;- c(NA, 1200*diff(log(data_subset$CPI))) # annualized\ndata_subset$p_log_diff &lt;- c(NA, 100*diff(log(data_subset$P)))\n\ncpi_log_diff_1m_ts &lt;- ts(data_subset$cpi_log_diff_1m, start = c(start_year, \n                                                                start_month), frequency = 12)\np_log_diff_ts &lt;- ts(data_subset$p_log_diff, start = c(start_year, \n                                                      start_month), frequency = 12)\n\nplot.ts(cpi_log_diff_1m_ts, col = \"blue\", lwd = 2, \n        main = \"Annualized monthly inflation\", xlab = \"Date\",\n        ylab = \"(Annualized) 1. log-difference of U.S. CPI\")\n\n\n\n\n\n\n\nplot.ts(p_log_diff_ts,col = \"blue\",lwd = 2, main = \"S&P 500 monthly returns\",\n        xlab = \"Date\",\n        ylab = \"log-difference of P\")\n\n\n\n\n\n\n\n# ----------------- Select the time series to be tested --------------------\n\ny &lt;- log(cpi_ts)\ny &lt;- log(p_ts)\ny &lt;- cpi_log_diff_1m_ts[2:length(cpi_log_diff_1m_ts)]\ny &lt;- p_log_diff_ts[2:length(cpi_log_diff_1m_ts)]\n\ny = non_stationary_series\n\n\n# Manually set the number of lags in the ADF test\nadf_lags &lt;- 8\n\n\n# --- 2. ur.df() EXAMPLES ---\n\n# # Case 1: No constant, no trend (not typically used)\n#cat(\"--- ur.df() | type = 'none' ---\\n\")\n#test_ur_none &lt;- ur.df(y, type = \"none\", lags = adf_lags)\n#stat_none &lt;- test_ur_none@teststat[1, 1]\n#p_val_none &lt;- punitroot(q = stat_none, N = length(test_ur_none@res), trend = \"nc\")\n#cat(\"Test Statistic:\", stat_none, \"\\n\")\n#cat(\"P-value:\", p_val_none, \"\\n\")\n\n\n# Case 2: Constant (drift) only\ncat(\"\\n--- ur.df() | type = 'drift' ---\\n\")\n\n\n--- ur.df() | type = 'drift' ---\n\ntest_ur_drift &lt;- ur.df(y, type = \"drift\", lags = adf_lags)\nstat_drift &lt;- test_ur_drift@teststat[1, 1]\np_val_drift &lt;- punitroot(q = stat_drift, N = length(test_ur_drift@res), trend = \"c\")\ncat(\"Test Statistic:\", stat_drift, \"\\n\")\n\nTest Statistic: -1.417984 \n\ncat(\"P-value:\", p_val_drift, \"\\n\")\n\nP-value: 0.5727686 \n\n# Case 3: Constant and trend\ncat(\"\\n--- ur.df() | type = 'trend' ---\\n\")\n\n\n--- ur.df() | type = 'trend' ---\n\ntest_ur_trend &lt;- ur.df(y, type = \"trend\", lags = adf_lags)\nstat_trend &lt;- test_ur_trend@teststat[1, 1]\np_val_trend &lt;- punitroot(q = stat_trend, N = length(test_ur_trend@res), trend = \"ct\")\ncat(\"Test Statistic:\", stat_trend, \"\\n\")\n\nTest Statistic: -2.087435 \n\ncat(\"P-value:\", p_val_trend, \"\\n\")\n\nP-value: 0.5490816 \n\n# --- adf.test() ---\n\n# ?adf.test\n\n# Manually set the number of lags to adf_lags\n# Note: This function has no argument to control for constant/trend.\n# - This procedure contains the constant and trend\n\nk &lt;- adf_lags\n\ncat(\"\\n--- adf.test() | Deterministic terms are implicit ---\\n\")\n\n\n--- adf.test() | Deterministic terms are implicit ---\n\ntest_adf &lt;- adf.test(y, k = adf_lags)\nprint(test_adf)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  y\nDickey-Fuller = -2.0874, Lag order = 8, p-value = 0.5395\nalternative hypothesis: stationary",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonstationary processes</span>"
    ]
  },
  {
    "objectID": "TSE-ch13.html",
    "href": "TSE-ch13.html",
    "title": "14  Linear regressions with I(1) variables",
    "section": "",
    "text": "14.1 Basics of cointegration\nAn important special case of the linear regression between \\(y_t\\) and \\(x_t\\), where \\(y_t\\) and \\(x_t\\) are nonstationary \\(I(1)\\) variables, arises when there is a common stochastic trend in both series.\nIn other words, suppose that there is a linear relationship between variables so that there exists some value \\(\\delta\\) such that \\(y_t - \\delta x_t\\) is stationary (\\(I(0)\\)) although \\(y_t\\) and \\(x_t\\) are nonstationary (\\(I(1)\\)).\nIn principle, an alternative way to construct a linear regression model, when \\(y_t\\) and \\(x_t\\) are \\(I(1)\\), is based on the differences \\(\\Delta y_t\\) and \\(\\Delta x_t\\) which are then stationary (\\(I(0)\\)).\nLet us examine cointegration more detail. If \\(y_t\\) and \\(x_t\\) are \\(I(1)\\) and cointegrated, then \\[\\begin{equation*}\nz_t= y_t - \\delta x_t = [y_t \\quad x_t] [1 \\quad -\\delta]^{\\prime} \\thicksim I(0).\n\\end{equation*}\\] The vector \\([1 \\quad -\\delta]^{\\prime}\\) is called the cointegration vector.\nNotice that we can also use a slightly modified definition of cointegration for the above, especially when there will be more than two variables involved. We will consider this definition, and cointegration in connection to the VAR model more detail in the Advanced Time Series Econometrics course.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linear regressions with I(1) variables</span>"
    ]
  },
  {
    "objectID": "TSE-ch13.html#basics-of-cointegration",
    "href": "TSE-ch13.html#basics-of-cointegration",
    "title": "14  Linear regressions with I(1) variables",
    "section": "",
    "text": "In such case, it is said that \\(y_t\\) and \\(x_t\\) are cointegrated and they share the common trend.\n\n\n\nThis is not an optimal strategy if there is indeed a cointegration relationship between the variables.\nCf. the discussion on overdifferencing in Section 12.\n\n\n\n\nCointegration is often interpreted as a long-run relationship between the variables. Assume that the equilibrium of the variables \\(y_t\\) and \\(x_t\\) is defined by the relationship \\(y_t= \\tilde{\\delta} x_t\\) for some fixed \\(\\tilde{\\delta}\\). Then \\(\\widehat{z}_t= y_t - \\widehat{\\delta} x_t\\) is the “equilibrium error” which measures the extent \\(y_t\\) deviates from its “equilibrium value”.\nAs \\(z_t\\) is \\(I(0)\\), the equilibrium error is stationary and fluctuating around zero. In other words, on average, the system is in equilibrium.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linear regressions with I(1) variables</span>"
    ]
  },
  {
    "objectID": "TSE-ch13.html#testing-cointegration-between-two-variables",
    "href": "TSE-ch13.html#testing-cointegration-between-two-variables",
    "title": "14  Linear regressions with I(1) variables",
    "section": "14.2 Testing cointegration between two variables",
    "text": "14.2 Testing cointegration between two variables\nFrom the discussion above, it is obvious that it is important to distinguish whether there is a cointegration relationship between the variables.\nConsider a “cointegration regression” between two \\(I(1)\\) variables \\(y_t\\) and \\(x_t\\): \\[\\begin{equation*}\ny_t= \\alpha + \\delta x_t + u_t.\n\\end{equation*}\\] If \\(y_t\\) and \\(x_t\\) are cointegrated, then \\(u_t\\) is \\(I(0)\\). If not, \\(u_t\\) will be \\(I(1)\\). Therefore, after the model is estimated by OLS, the presence of a possible cointegration relationship can be evaluated by testing for a unit root in the residuals \\(\\widehat{u}_t\\).\n \nTesting for cointegration with a known coefficient. In some applications, the cointegration coefficient \\(\\delta\\) (and the intercept \\(\\alpha\\)) is known in advance due to economic theory or other application-specific knowledge. When the coefficient is known, the cointegration relationship can be tested as follows. For the sake of simplicity, we will assume \\(\\alpha=0\\) in the procedure described below\n\nConstruct the series \\(z_t = y_{t} - \\delta x_t\\).\nUse the ADF test for the null hypothesis, which implies \\(z_t \\thicksim I(1)\\). If \\(H_0\\) is rejected, then there is a cointegration relationship.\n\n \nTesting for cointegration with unknown coefficients. If \\(\\delta\\) (and \\(\\alpha\\)) are unknown, then we can proceed as follows (Engle-Granger ADF-test):\n\nEstimate the following model by using OLS \\[\\begin{equation*}\ny_t = \\alpha + \\delta x_t + u_t.\n\\end{equation*}\\]\nThe series \\(\\widehat{u}_t= y_t - \\widehat{\\alpha} - \\widehat{\\delta} x_t\\) is the residual of the regression.\nTest the null hypothesis implying \\(\\widehat{u}_t \\thicksim I(1)\\).\nCritical values are now different than in the above case (and ADF test) as unit root testing is based on the residuals.\n\n \nIf \\(y_t\\) and \\(x_t\\) are indeed cointegrated, OLS yields a consistent estimator for the cointegration coefficient \\(\\delta\\). However, the OLS estimator of \\(\\delta\\) has a non-normal asymptotic distribution, and the inferences based on the standard \\(t\\)-test statistic can be misleading.\n \nExample: Consumption and income. Let us consider an empirical question derived from Hall’s (1978) Permanent Income Hypothesis (PIH) in the context of cointegration.\n\nR. E. Hall (1978). Stochastic implications of the life cycle-permanent income hypothesis: Theory and evidence. Journal of Political Economy, 86(6), 971–987.\n\nThe testable hypothesis is that there is a stable long-run relationship exist between real consumption and real income, such that the difference between them (the cointegrating residual) is stationary.\n\nIn other words, the cointegration theory predicts that since consumption is proportional to permanent income (the present value of expected future income), and both are driven by a common stochastic trend (permanent income shocks), they should be cointegrated.\n\n\nSo, let us test the prediction of the PIH that real per capita consumption (log(\\(c_t\\)) and real per capita disposable income (log(\\(y_t\\)) share a single cointegrating relationship.\n\nHypothesis: The two variables (\\(\\log(c_t)\\) and \\(\\log(y_t)\\)) are \\(I(1)\\) and cointegrated.\nLong-run equation: \\(\\log(c_t) = \\beta_0 + \\beta_1 \\log(y_t) + u_t\\)\nPIH implication: The cointegrating vector should be \\((1,-\\beta_1)\\), where \\(\\beta_1\\) is theoretically close to 1 (or exactly 1 in the simplest PIH model) because, in the long run, consumption should grow proportionally with permanent income. The error term \\(u_t\\) and the resulting residuals \\(\\widehat{u}_t\\) must be \\(I(0)\\) (stationary).\n\n\n\n\n\n\n\n\n\n\n\nFigure: Monthly U.S. data (sample period 1959:1–2025:8) on (log) consumption and income.\n\n \nCheck first the unit root hypothesis for the two time series:\nlog(c_t)\nTest Statistic: -2.005634 \nP-value: 0.595\n\nlog(y_t)\nTest Statistic: -2.582764 \nP-value: 0.289 \nEstimation result of the cointegration regression:\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.383122   0.044448    8.62 6.29e-16 ***\nln_Y        1.048293   0.004244  246.99  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.02692 on 264 degrees of freedom\nMultiple R-squared:  0.9957,    Adjusted R-squared:  0.9957 \nF-statistic: 6.1e+04 on 1 and 264 DF,  p-value: &lt; 2.2e-16\n \nBased on the Engle-Granger two-step procedure, the null hypothesis of no cointegration can be rejected at 5% level (see testing result below).\n \n\n\n\n\n\n\n\n\n\n\nFigure: PIH residual time series (from the estimated cointegrated model..\n\n \nTest Statistic: -4.561388 \n&gt; cat(\"P-value:\", p_val_drift, \"\\n\")\nP-value: 0.000197051",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linear regressions with I(1) variables</span>"
    ]
  },
  {
    "objectID": "TSE-ch13.html#linear-regressions-containing-i1-variables",
    "href": "TSE-ch13.html#linear-regressions-containing-i1-variables",
    "title": "14  Linear regressions with I(1) variables",
    "section": "14.3 Linear regressions containing I(1) variables",
    "text": "14.3 Linear regressions containing I(1) variables\nRecall the linear regression \\[\\begin{equation*}\ny_t= \\boldsymbol{x}^{\\prime}_t \\boldsymbol{\\beta} + u_t,\n\\end{equation*}\\] and the two assumptions \\(\\mathrm{(i)}\\)–\\(\\mathrm{(ii)}\\) set in the previous section:\n\n\\(\\mathrm{(i)}\\) The error term \\(u_t\\) is serially uncorrelated and uncorrelated with the regressors included in \\(\\boldsymbol{x}_t\\).\n\\(\\mathrm{(ii)}\\) All the regressors in \\(\\boldsymbol{x}_t\\) are either deterministic or stationary random variables.\n\nAssume now that only \\(\\mathrm{(i)}\\) holds, that is the error term \\(u_t\\) is serially uncorrelated and uncorrelated with the regressors included in \\(\\boldsymbol{x}_t\\).\n\nEven if the model cannot be written in this way, the OLS estimator of the coefficients of the \\(I(1)\\) regressors \\(\\boldsymbol{x}_t\\) is consistent. However, its asymptotic distribution is, in general, nonstandard such that usual inference does not apply.\nIf the model can be written such that all the parameters of interest are coefficients of mean zero stationary variables, their OLS estimator is consistent and asymptotically normal.\n\nTo examine these points more detail, let us consider again the following relatively simple regression model \\[\\begin{equation*}\ny_t= \\beta_0 + \\beta_1 x_{t-1} + \\beta_2 x_{t-2} + u_t,\n\\end{equation*}\\] where \\(x_t \\thicksim I(1)\\). The model can be rewritten as \\[\\begin{equation*}\ny_t= \\beta_0 + (\\beta_1+\\beta_2) x_{t-1} - \\beta_2 (x_{t-1} - x_{t-2}) + u_t,\n\\end{equation*}\\] or \\[\\begin{equation*}\ny_t= \\beta_0 + \\beta_1 (x_{t-1}-x_{t-2}) + (\\beta_1 + \\beta_2) x_{t-2} + u_t.\n\\end{equation*}\\] As \\((x_{t-1} - x_{t-2}) \\thicksim I(0)\\), and hence, standard inference on \\(\\beta_2\\) (or \\(\\beta_1\\)) holds. Therefore, both \\(\\beta_1\\) and \\(\\beta_2\\) cannot simultaneously be written as coefficients of \\(I(0)\\) variables (unless higher lags are included in the model).\n\nThe OLS estimator of \\(\\beta_1\\) and \\(\\beta_2\\) is not, in general, jointly asymptotically normal.\nThe test statistic on a hypothesis concerning both coefficients (for instance \\(H_0: \\beta_1 = \\beta_2\\)), in general, does not have the usual asymptotic \\(\\chi^2\\) distribution.\n\nThese remarks are compiled to the next figure.\n\n\n\n\n\n\n\n\n\n \n\nFigure: Cheat sheet on inference in OLS regression models with \\(I(1)\\) variables (References: Stock and Watson (1988) and lecture notes by Markku Lanne).\n\n \nLet us continue with linear regression where now also the assumption \\(\\mathrm{(i)}\\) does not hold and the dependent variable is \\(I(1)\\).\n\nIf the dependent variable is not cointegrated with any of the regressors, the OLS estimator of the coefficients of the \\(I(1)\\) regressors is inconsistent.\nIf the dependent variable is cointegrated with at least one of the regressors, the OLS estimator of the parameters of interest\n\nthat can be written as coefficients of stationary variables, is inconsistent.\nthat cannot be written as coefficients of stationary variables, is consistent, but not asymptotically normal.\n\n\nFor instance, consider the following regression model: \\[\\begin{equation*}\ny_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{2t} + u_t,\n\\end{equation*}\\] where all variables are \\(I(1)\\), and \\(y_t\\) and \\(x_{2t}\\) are cointegrated and assumption \\(\\mathrm{(i)}\\) does not hold.\n\nIf \\(x_{1t}\\) and \\(x_{2t}\\) are cointegrated such that \\((x_{1t} - \\gamma x_{2t}) \\thicksim ~ I(0)\\), the model can be written as \\[\\begin{equation*}\ny_t = \\beta_0 + \\beta_1 (x_{1t} - \\gamma x_{2t}) + (\\beta_1 \\gamma + \\beta_2) x_{2t} + u_t,\n\\end{equation*}\\] and the OLS estimator of \\(\\beta_1\\) is inconsistent.\nIf \\(x_{1t}\\) is not cointegrated with \\(x_{2t}\\), \\(\\beta_1\\) cannot be written as a coefficient of an \\(I(0)\\) variable, and hence, its OLS estimator is consistent, but not asymptotically normal.\n\n \nThe complete cheat sheet on linear regression with \\(I(0)\\) and \\(I(1)\\) variables is summarized hereby:\n\n\n\n\n\n\n\n\n\n\nFigure: Complete cheat sheet on inference in OLS regression models with \\(I(1)\\) variables.\n\n \nA spurious regression emerges when a statistically significant relationship between two \\(I(1)\\) variables \\(y_t\\) and \\(x_t\\) is found although those are completely unrelated. Assume that a researcher estimates a linear regression \\[\\begin{equation*}\ny_t = \\beta_0 + \\beta_1 x_t + u_t,\n\\end{equation*}\\] but assuming wrongly that the variables \\(y_t\\) and \\(x_t\\) are stationary. Instead, they are generated by two independent random walks (i.e., \\(y_t\\) and \\(x_t\\) are \\(I(1)\\) variables) \\[\\begin{eqnarray*}\ny_t &=& y_{t-1} + u_{1t}, \\quad u_{1t} \\thicksim \\mathsf{iid}(0, \\sigma^2_1) \\\\\nx_t &=& x_{t-1} + u_{2t}, \\quad u_{2t} \\thicksim \\mathsf{iid}(0, \\sigma^2_2).\n\\end{eqnarray*}\\] This typically leads to the estimation result of the above linear regression characterized by a fairly high \\(R^2\\) (the coefficient of determination), highly autocorrelated residuals \\(\\widehat{u}_t\\) and a (highly) statistically significant estimate of \\(\\beta_2\\). This result is clearly spurious given that the variables are completely independent!\n\nEstimation results like this should not be taken seriously.\nThe reason is that with \\(y_t\\) and \\(x_t\\) being \\(I(1)\\) variables, the error term \\(u_t\\) will also be \\(I(1)\\), not stationary (\\(I(0)\\)).\n\nSolution to this problem is to include both the lags of \\(y_t\\) and \\(x_t\\) as predictors in the model. In other words, we end up a model \\[\\begin{equation*}\ny_t = \\beta_0 + \\beta_1 x_t + \\beta_2 y_{t-1} + \\beta_3 x_{t-1} +  u_t.\n\\end{equation*}\\] In this case the OLS estimator is consistent. Thus, in general, including lagged values in the regression is sufficient to solve many of the problems associated with possibly spurious regressions.\nAn example of a spurious regression situation can be replicated with the following program code below in R lab.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linear regressions with I(1) variables</span>"
    ]
  },
  {
    "objectID": "TSE-ch13.html#r-lab",
    "href": "TSE-ch13.html#r-lab",
    "title": "14  Linear regressions with I(1) variables",
    "section": "14.4 R Lab",
    "text": "14.4 R Lab\nAll the R codes considered in this section are compiled in the following link:\n\nR Lab: Cointegration (PIH example)\n\n\n\n# Load required libraries\n# install.packages(c(\"quantmod\", \"tseries\")) # Uncomment and run if not installed\nlibrary(quantmod)\nlibrary(tseries)\nlibrary(urca)\n\n\n# ===============================================\n# Fetch and Prepare Data\n# ===============================================\n# 1. Fetch Data from FRED (Corrected Income Code: DPIC96)\n# PCECC96: Real Personal Consumption Expenditures\n# DPIC96: Real Disposable Personal Income\n# POPTHM: Population (Thousands)\nPCECC96 &lt;- getSymbols(\"PCECC96\", src = \"FRED\", auto.assign = FALSE)\n\nDPIC96 &lt;- getSymbols(\"DPIC96\", src = \"FRED\", auto.assign = FALSE)\n\nPOPTHM &lt;- getSymbols(\"POPTHM\", src = \"FRED\", auto.assign = FALSE)\n\n\n# 2. Align and Transform Data (Log, Per Capita, Quarterly)\n# Merge series and remove any periods with missing data\ndata.raw &lt;- na.omit(merge(PCECC96, DPIC96, POPTHM))\n\n# Convert to 'ts' object with quarterly frequency\nstart.year &lt;- as.numeric(format(index(head(data.raw, 1)), \"%Y\"))\ndata.ts &lt;- ts(data.raw, start = start.year, frequency = 4)\n\n# Calculate real, per capita, log-transformed variables\nPOP_THOUSANDS &lt;- data.ts[, \"POPTHM\"]\nPOP &lt;- POP_THOUSANDS * 1000 # Convert from thousands to single units\n\n# Log Per Capita Consumption (ln_C)\nln_C &lt;- log(data.ts[, \"PCECC96\"] / POP)\n\n# Log Per Capita Income (ln_Y)\nln_Y &lt;- log(data.ts[, \"DPIC96\"] / POP)\n\n\n\n# ===============================================\n# Plotting the Variables\n# ===============================================\n\n# Set up the plotting window to display all three graphs\npar(mfrow=c(3, 1), mar=c(2.5, 4, 2, 2) + 0.1) \n\n# --- Plot 1: Population ---\nplot(POP_THOUSANDS, \n     main = \"U.S. Population (POPTHM)\",\n     ylab = \"Population (Thousands)\",\n     col = \"darkgreen\", \n     lwd = 2)\n# Observation: Population shows a clear deterministic and stochastic trend (non-stationary).\n\n# --- Plot 2: Log Per Capita Real Consumption (ln_C) ---\nplot(ln_C, \n     main = \"Log Real Per Capita Consumption (ln C)\",\n     ylab = \"ln(Consumption)\",\n     col = \"blue\", \n     lwd = 2)\n# Observation: ln_C shows a strong upward trend, typical of an I(1) process.\n\n# --- Plot 3: Log Per Capita Real Disposable Income (ln_Y) ---\nplot(ln_Y, \n     main = \"Log Real Per Capita Disposable Income (ln Y)\",\n     ylab = \"ln(Income)\",\n     col = \"red\", \n     lwd = 2)\n\n\n\n\n\n\n\n# Observation: ln_Y also shows a strong upward trend. Visually, ln_C and ln_Y \n# appear to move together, suggesting a common stochastic trend, a prerequisite for cointegration.\n\n# Reset plot layout\npar(mfrow=c(1, 1))\n\n# Optionally, plot the two log series together to visually confirm they share a trend\ncat(\"\\n--- Visual Cointegration Check (Overlay) ---\\n\")\n\n\n--- Visual Cointegration Check (Overlay) ---\n\nplot(ln_C, \n     main = \"Log Per Capita Consumption (Blue) vs. Income (Red)\", \n     ylab = \"Log Value\", \n     col = \"blue\", \n     lwd = 2,\n     ylim = range(c(ln_C, ln_Y)))\nlines(ln_Y, col = \"red\", lwd = 2)\nlegend(\"topleft\", \n       legend = c(\"ln C (Consumption)\", \"ln Y (Income)\"), \n       col = c(\"blue\", \"red\"), \n       lwd = 2, \n       bty = \"n\")\n\n\n\n\n\n\n\n# ===============================================\n# Unit Root Pre-Checks (Confirm I(1) for both)\n# ===============================================\n\n# cat(\"\\n--- ADF Test on LOG(Consumption) Levels (ln_C) ---\\n\")\n# print(adf.test(ln_C, k = 4)) # k=4 for quarterly data is a common starting point\n\n# ADF-test: Constant and trend\nadf_lags &lt;- 4\ncat(\"\\n--- ur.df() | type = 'trend' ---\\n\")\n\n\n--- ur.df() | type = 'trend' ---\n\ntest_ur_trend &lt;- ur.df(ln_C, type = \"trend\", lags = adf_lags)\nstat_trend &lt;- test_ur_trend@teststat[1, 1]\np_val_trend &lt;- punitroot(q = stat_trend, N = length(test_ur_trend@res), trend = \"ct\")\ncat(\"Test Statistic:\", stat_trend, \"\\n\")\n\nTest Statistic: -2.005634 \n\ncat(\"P-value:\", p_val_trend, \"\\n\")\n\nP-value: 0.5951187 \n\n#cat(\"\\n--- ADF Test on LOG(Income) Levels (ln_Y) ---\\n\")\n#print(adf.test(ln_Y, k = 4))\n# # NOTE: If p-value is &gt; 0.05 for both, the I(1) condition is met to proceed.\n\n# ADF-test: Constant and trend\nadf_lags &lt;- 4\ncat(\"\\n--- ur.df() | type = 'trend' ---\\n\")\n\n\n--- ur.df() | type = 'trend' ---\n\ntest_ur_trend &lt;- ur.df(ln_Y, type = \"trend\", lags = adf_lags)\nstat_trend &lt;- test_ur_trend@teststat[1, 1]\np_val_trend &lt;- punitroot(q = stat_trend, N = length(test_ur_trend@res), trend = \"ct\")\ncat(\"Test Statistic:\", stat_trend, \"\\n\")\n\nTest Statistic: -2.582764 \n\ncat(\"P-value:\", p_val_trend, \"\\n\")\n\nP-value: 0.2886401 \n\n# ===============================================\n# The Engle-Granger Cointegration Test\n# ===============================================\n\n# Estimate the Long-Run Relationship (The Cointegrating Regression)\n# Model: ln_C = alpha + beta * ln_Y + epsilon\ncointegrating_regression &lt;- lm(ln_C ~ ln_Y)\n\ncat(\"\\n--- Cointegrating Regression (ln_C ~ ln_Y) ---\\n\")\n\n\n--- Cointegrating Regression (ln_C ~ ln_Y) ---\n\nprint(summary(cointegrating_regression))\n\n\nCall:\nlm(formula = ln_C ~ ln_Y)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.222565 -0.011066  0.001933  0.013846  0.049964 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.383122   0.044448    8.62 6.29e-16 ***\nln_Y        1.048293   0.004244  246.99  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02692 on 264 degrees of freedom\nMultiple R-squared:  0.9957,    Adjusted R-squared:  0.9957 \nF-statistic: 6.1e+04 on 1 and 264 DF,  p-value: &lt; 2.2e-16\n\n# Extract the residual series (the potential long-run equilibrium error)\nresids &lt;- ts(residuals(cointegrating_regression), start = start.year, frequency = 4)\n\n# Plot the residuals (to visually check for stationarity)\n# The time indexing (year and quarters) is correctly inherited from the 'ln_C' and 'ln_Y' ts objects\nplot(resids, \n     main = \"Residuals from cointegrating regression (Engle-Granger residual)\", \n     ylab = \"Residual\", \n     xlab = \"Time\", \n     type=\"l\",\n     col = \"purple\",\n     lwd = 2)\nabline(h = 0, col = \"gray\", lty = 2)\n\n\n\n\n\n\n\n# Observation: If the series appears mean-reverting around zero, it visually supports stationarity (I(0)).\n\n\n# 3b. Test the Residuals for Stationarity (The Cointegration Test)\n# Null Hypothesis (H0): Residuals are Non-Stationary (No cointegration)\n# Alternative (HA): Residuals are Stationary (Cointegration exists)\n\n\n# Case 2: Constant (drift) only\ncat(\"\\n--- ur.df() | type = 'drift' ---\\n\")\n\n\n--- ur.df() | type = 'drift' ---\n\ntest_ur_drift &lt;- ur.df(resids, type = \"drift\", lags = adf_lags)\nstat_drift &lt;- test_ur_drift@teststat[1, 1]\np_val_drift &lt;- punitroot(q = stat_drift, N = length(test_ur_drift@res), trend = \"c\")\ncat(\"Test Statistic:\", stat_drift, \"\\n\")\n\nTest Statistic: -4.561388 \n\ncat(\"P-value:\", p_val_drift, \"\\n\")\n\nP-value: 0.000197051 \n\n\n\n\nR Lab: Spurious regression simulation\n\n\n\n# Spurious regression\n\n# Setup: Set length (T) and simulate innovations (epsilon)\nT &lt;- 500\nset.seed(42) # For reproducibility\nepsilon_1 &lt;- rnorm(T)\nepsilon_2 &lt;- rnorm(T)\n\n# 1. Simulate two independent Random Walks (I(1) processes)\n# Cumulative sum creates the random walk (y_t = y_{t-1} + epsilon_t)\ny &lt;- cumsum(epsilon_1)\nx &lt;- cumsum(epsilon_2)\n\n# Optional: Plot the random walks (to show non-stationarity)\n# par(mfrow=c(2, 1))\n# plot(y, type=\"l\", main=\"Random Walk y\")\n# plot(x, type=\"l\", main=\"Random Walk x\")\n# par(mfrow=c(1, 1))\n\n# 2. Estimate Spurious Regression (y on x)\nestim_model &lt;- lm(y ~ x)\nprint(\"--- Spurious Regression Summary (y ~ x) ---\")\n\n[1] \"--- Spurious Regression Summary (y ~ x) ---\"\n\nsummary(estim_model)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7504  -2.9024  -0.0052   3.0015  11.1311 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.91838    0.24019  -7.987 9.66e-15 ***\nx            0.28525    0.01399  20.394  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.974 on 498 degrees of freedom\nMultiple R-squared:  0.4551,    Adjusted R-squared:  0.454 \nF-statistic: 415.9 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n# 3. Analyze Spurious Regression Residuals (should look non-stationary/autocorrelated)\nresid_1 &lt;- resid(estim_model)\nprint(\"--- ACF of Spurious Regression Residuals ---\")\n\n[1] \"--- ACF of Spurious Regression Residuals ---\"\n\nacf(resid_1, main=\"ACF of Spurious Regression Residuals\") \n\n\n\n\n\n\n\n# 4. Optional: Augmented Regression (with lags)\n# Prepare lagged variables (removes the first observation)\ny_lag1 &lt;- y[1:(T-1)]\nx_lag1 &lt;- x[1:(T-1)]\ny_t &lt;- y[2:T]\nx_t &lt;- x[2:T]\n\nestim_model_2 &lt;- lm(y_t ~ x_t + x_lag1 + y_lag1)\nprint(\"--- Augmented Regression Summary (y_t ~ x_t + x_{t-1} + y_{t-1}) ---\")\n\n[1] \"--- Augmented Regression Summary (y_t ~ x_t + x_{t-1} + y_{t-1}) ---\"\n\nsummary(estim_model_2)\n\n\nCall:\nlm(formula = y_t ~ x_t + x_lag1 + y_lag1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91745 -0.63821  0.01122  0.66164  2.98589 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.089030   0.062250  -1.430    0.153    \nx_t         -0.007609   0.042138  -0.181    0.857    \nx_lag1       0.013689   0.042090   0.325    0.745    \ny_lag1       0.975694   0.011006  88.648   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9692 on 495 degrees of freedom\nMultiple R-squared:  0.9677,    Adjusted R-squared:  0.9675 \nF-statistic:  4943 on 3 and 495 DF,  p-value: &lt; 2.2e-16\n\nresid_2 &lt;- resid(estim_model_2)\nprint(\"--- ACF of Spurious Regression Residuals With Lags ---\")\n\n[1] \"--- ACF of Spurious Regression Residuals With Lags ---\"\n\nacf(resid_2, main=\"ACF of Spurious Regression Residuals\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Linear regressions with I(1) variables</span>"
    ]
  }
]